<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Bowen&#39;s blog</title>
  
  <subtitle>keep learning - learning notes and blogs</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://bowenli86.github.io/"/>
  <updated>2018-04-10T04:29:25.552Z</updated>
  <id>https://bowenli86.github.io/</id>
  
  <author>
    <name>Bowen Li</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Docker Images, Containers, and Storage Drivers</title>
    <link href="https://bowenli86.github.io/2017/01/10/tools/docker/Docker-Images-Containers-and-Storage-Drivers/"/>
    <id>https://bowenli86.github.io/2017/01/10/tools/docker/Docker-Images-Containers-and-Storage-Drivers/</id>
    <published>2017-01-10T22:07:53.000Z</published>
    <updated>2018-04-10T04:29:25.552Z</updated>
    
    <content type="html"><![CDATA[<p>To use storage drivers effectively, you must understand how Docker builds and stores images. Then, you need an understanding of how these images are used by containers. Finally, you’ll need a short introduction to the technologies that enable both images and container operations.</p><h2 id="Images-and-layers"><a href="#Images-and-layers" class="headerlink" title="Images and layers"></a>Images and layers</h2><p>Each Docker image references <strong>a list of read-only layers</strong> that represent filesystem differences. Layers are stacked on top of each other to form a base for a container’s root filesystem. The diagram below shows the Ubuntu 15.04 image comprising 4 stacked image layers.</p><p><img src="https://docs.docker.com/engine/userguide/storagedriver/images/image-layers.jpg" alt="image"></p><p>The Docker storage driver is responsible for stacking these layers and providing a single unified view.</p><blockquote><p>When you create a new container, you add <strong>a new, thin, writable layer</strong> on top of the underlying stack. This layer is often called the <strong>“container layer”</strong>. All changes made to the running container - such as writing new files, modifying existing files, and deleting files - are written to this thin writable container layer. The diagram below shows a container based on the Ubuntu 15.04 image.</p></blockquote><p><img src="https://docs.docker.com/engine/userguide/storagedriver/images/container-layers.jpg" alt="container"></p><h2 id="Content-Addressable-Storage"><a href="#Content-Addressable-Storage" class="headerlink" title="Content Addressable Storage"></a>Content Addressable Storage</h2><p>Docker 1.10 introduced <strong>a new content addressable storage model</strong>. This is a completely new way to address image and layer data on disk. Previously, image and layer data was referenced and stored using a randomly generated UUID. In the new model this is replaced by a secure content hash.</p><p>The new model improves security, provides a built-in way to avoid ID collisions, and guarantees data integrity after pull, push, load, and save operations. It also enables better sharing of layers by allowing many images to freely share their layers even if they didn’t come from the same build.</p><p>The diagram below shows an updated version of the previous diagram, highlighting the changes implemented by Docker 1.10.</p><p><img src="https://docs.docker.com/engine/userguide/storagedriver/images/container-layers-cas.jpg" alt="container"></p><p>As can be seen, all image layer IDs are cryptographic hashes, whereas the container ID is still a randomly generated UUID.</p><a id="more"></a><h2 id="Container-and-Layers"><a href="#Container-and-Layers" class="headerlink" title="Container and Layers"></a>Container and Layers</h2><p><strong>The major difference between a container and an image is the top writable layer.</strong> All writes to the container that add new or modify existing data are stored in this writable layer. When the container is deleted the writable layer is also deleted. The underlying image remains unchanged.</p><p><strong>Because each container has its own thin writable container layer, and all changes are stored in this container layer, this means that multiple containers can share access to the same underlying image and yet have their own data state.</strong></p><p>The diagram below shows multiple containers sharing the same Ubuntu 15.04 image.</p><p><img src="https://docs.docker.com/engine/userguide/storagedriver/images/sharing-layers.jpg" alt="shared image layer"></p><p>The Docker storage driver is responsible for enabling and managing both the image layers and the writable container layer. How a storage driver accomplishes these can vary between drivers. Two key technologies behind Docker image and container management are <strong>stackable image layers</strong> and <strong>copy-on-write (CoW)</strong>.</p><h2 id="The-copy-on-write-strategy"><a href="#The-copy-on-write-strategy" class="headerlink" title="The copy-on-write strategy"></a>The copy-on-write strategy</h2><p>Sharing is a good way to optimize resources.</p><p>People do this instinctively in daily life. For example, twins Jane and Joseph taking an Algebra class at different times from different teachers can share the same exercise book by passing it between each other. Now, suppose Jane gets an assignment to complete the homework on page 11 in the book. At that point, Jane copies page 11, completes the homework, and hands in her copy. The original exercise book is unchanged and only Jane has a copy of the changed page 11.</p><p><strong>Copy-on-write</strong> is a similar strategy of sharing and copying. In this strategy, system processes that need the same data share the same instance of that data rather than having their own copy. At some point, if one process needs to modify or write to the data, only then does the operating system make a copy of the data for that process to use. Only the process that needs to write has access to the data copy. All the other processes continue to use the original data.</p><p>Docker uses a <strong>copy-on-write</strong> technology with both images and containers. <strong>This CoW strategy optimizes both image disk space usage and the performance of container start times.</strong></p><p>The next sections look at how <strong>copy-on-write</strong> is leveraged with images and containers through sharing and copying.</p><h2 id="Sharing-promotes-smaller-images"><a href="#Sharing-promotes-smaller-images" class="headerlink" title="Sharing promotes smaller images"></a>Sharing promotes smaller images</h2><p>This section looks at <strong>image layers</strong> and <strong>copy-on-write technology</strong>. All image and container layers exist inside the Docker host’s local storage area and are managed by the storage driver. On Linux-based Docker hosts this is usually located under <code>/var/lib/docker/</code>.</p><p>The Docker client reports on image layers when instructed to pull and push images with docker pull and docker push.</p><p>The command below pulls the ubuntu:15.04 Docker image from Docker Hub.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ docker pull ubuntu:15.04</span><br><span class="line"></span><br><span class="line">15.04: Pulling from library/ubuntu</span><br><span class="line">1ba8ac955b97: Pull complete</span><br><span class="line">f157c4e5ede7: Pull complete</span><br><span class="line">0b7e98f84c4c: Pull complete</span><br><span class="line">a3ed95caeb02: Pull complete</span><br><span class="line">Digest: sha256:5e279a9df07990286cce22e1b0f5b0490629ca6d187698746ae5e28e604a640e</span><br><span class="line">Status: Downloaded newer image for ubuntu:15.04</span><br></pre></td></tr></table></figure><blockquote><p>From the output, you’ll see that the command actually pulls 4 image layers. Each of the above lines lists an image layer and its UUID or cryptographic hash. The combination of these four layers makes up the ubuntu:15.04 Docker image.</p></blockquote><p>Each of these layers is stored in its own directory inside the Docker host’s local storage are.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$  docker pull ubuntu:15.04</span><br><span class="line"></span><br><span class="line">15.04: Pulling from library/ubuntu</span><br><span class="line">47984b517ca9: Pull complete</span><br><span class="line">df6e891a3ea9: Pull complete</span><br><span class="line">e65155041eed: Pull complete</span><br><span class="line">c8be1ac8145a: Pull complete</span><br><span class="line">Digest: sha256:5e279a9df07990286cce22e1b0f5b0490629ca6d187698746ae5e28e604a640e</span><br><span class="line">Status: Downloaded newer image for ubuntu:15.04</span><br><span class="line"></span><br><span class="line">$ ls /var/lib/docker/aufs/layers</span><br><span class="line"></span><br><span class="line">47984b517ca9ca0312aced5c9698753ffa964c2015f2a5f18e5efa9848cf30e2</span><br><span class="line">c8be1ac8145a6e59a55667f573883749ad66eaeef92b4df17e5ea1260e2d7356</span><br><span class="line">df6e891a3ea9cdce2a388a2cf1b1711629557454fd120abd5be6d32329a0e0ac</span><br><span class="line">e65155041eed7ec58dea78d90286048055ca75d41ea893c7246e794389ecf203</span><br></pre></td></tr></table></figure><p>Notice how the four directories match up with the layer IDs of the downloaded image. </p><p>All versions of Docker still allow images to share layers. For example, If you pull an image that shares some of the same image layers as an image that has already been pulled, the Docker daemon recognizes this, and only pulls the layers it doesn’t already have stored locally. After the second pull, the two images will share any common image layers.</p><p><img src="https://docs.docker.com/engine/userguide/storagedriver/images/saving-space.jpg" alt="shared image layer"></p><h2 id="Copying-makes-containers-efficient"><a href="#Copying-makes-containers-efficient" class="headerlink" title="Copying makes containers efficient"></a>Copying makes containers efficient</h2><p>You learned earlier that a container is a Docker image with a thin writable, container layer added.</p><p>All writes made to a container are stored in the thin writable container layer. The other layers are read-only (RO) image layers and can’t be changed. This means that multiple containers can safely share a single underlying image. The diagram below shows multiple containers sharing a single copy of the ubuntu:15.04 image. Each container has its own thin RW layer, but they all share a single instance of the ubuntu:15.04 image:</p><p><img src="https://docs.docker.com/engine/userguide/storagedriver/images/sharing-layers.jpg" alt=""></p><p>When an existing file in a container is modified, Docker uses the storage driver to perform a copy-on-write operation. The specifics of operation depends on the storage driver. For the AUFS and OverlayFS storage drivers, the copy-on-write operation is pretty much as follows:</p><ul><li><p>Search through the image layers for the file to update. The process starts at the top, newest layer and works down to the base layer one layer at a time.</p></li><li><p>Perform a “copy-up” operation on the first copy of the file that is found. A “copy up” copies the file up to the container’s own thin writable layer.</p></li><li><p>Modify the copy of the file in container’s thin writable layer.</p></li></ul><p>Containers that write a lot of data will consume more space than containers that do not. This is because most write operations consume new space in the container’s thin writable top layer. If your container needs to write a lot of data, you should consider using a data volume.</p><p>A copy-up operation can incur a noticeable performance overhead. This overhead is different depending on which storage driver is in use. However, large files, lots of layers, and deep directory trees can make the impact more noticeable. Fortunately, the operation only occurs the first time any particular file is modified. Subsequent modifications to the same file do not cause a copy-up operation and can operate directly on the file’s existing copy already present in the container layer.</p><p>Let’s see what happens if we spin up 5 containers based on our changed-ubuntu image we built earlier:</p><ol><li><p>From a terminal on your Docker host, run the following docker run command 5 times.</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -dit changed-ubuntu bash</span><br><span class="line"></span><br><span class="line">75bab0d54f3cf193cfdc3a86483466363f442fba30859f7dcd1b816b6ede82d4</span><br><span class="line"></span><br><span class="line">$ docker run -dit changed-ubuntu bash</span><br><span class="line"></span><br><span class="line">9280e777d109e2eb4b13ab211553516124a3d4d4280a0edfc7abf75c59024d47</span><br><span class="line"></span><br><span class="line">$ docker run -dit changed-ubuntu bash</span><br><span class="line"></span><br><span class="line">a651680bd6c2ef64902e154eeb8a064b85c9abf08ac46f922ad8dfc11bb5cd8a</span><br><span class="line"></span><br><span class="line">$ docker run -dit changed-ubuntu bash</span><br><span class="line"></span><br><span class="line">8eb24b3b2d246f225b24f2fca39625aaad71689c392a7b552b78baf264647373</span><br><span class="line"></span><br><span class="line">$ docker run -dit changed-ubuntu bash</span><br><span class="line"></span><br><span class="line">0ad25d06bdf6fca0dedc38301b2aff7478b3e1ce3d1acd676573bba57cb1cfef</span><br></pre></td></tr></table></figure></li></ol><pre><code>This launches 5 containers based on the changed-ubuntu image. As each container is created, Docker adds a writable layer and assigns it a random UUID. This is the value returned from the docker run command.</code></pre><ol start="2"><li><p>Run the docker ps command to verify the 5 containers are running.</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ docker ps</span><br><span class="line">CONTAINER ID    IMAGE             COMMAND    CREATED              STATUS              PORTS    NAMES</span><br><span class="line">0ad25d06bdf6    changed-ubuntu    &quot;bash&quot;     About a minute ago   Up About a minute            stoic_ptolemy</span><br><span class="line">8eb24b3b2d24    changed-ubuntu    &quot;bash&quot;     About a minute ago   Up About a minute            pensive_bartik</span><br><span class="line">a651680bd6c2    changed-ubuntu    &quot;bash&quot;     2 minutes ago        Up 2 minutes                 hopeful_turing</span><br><span class="line">9280e777d109    changed-ubuntu    &quot;bash&quot;     2 minutes ago        Up 2 minutes                 backstabbing_mahavira</span><br><span class="line">75bab0d54f3c    changed-ubuntu    &quot;bash&quot;     2 minutes ago        Up 2 minutes                 boring_pasteur</span><br></pre></td></tr></table></figure></li></ol><pre><code>The output above shows 5 running containers, all sharing the changed-ubuntu image. Each CONTAINER ID is derived from the UUID when creating each container.</code></pre><ol start="3"><li><p>List the contents of the local storage area.</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ls /var/lib/docker/containers</span><br><span class="line"></span><br><span class="line">0ad25d06bdf6fca0dedc38301b2aff7478b3e1ce3d1acd676573bba57cb1cfef</span><br><span class="line">9280e777d109e2eb4b13ab211553516124a3d4d4280a0edfc7abf75c59024d47</span><br><span class="line">75bab0d54f3cf193cfdc3a86483466363f442fba30859f7dcd1b816b6ede82d4</span><br><span class="line">a651680bd6c2ef64902e154eeb8a064b85c9abf08ac46f922ad8dfc11bb5cd8a</span><br><span class="line">8eb24b3b2d246f225b24f2fca39625aaad71689c392a7b552b78baf264647373</span><br></pre></td></tr></table></figure></li></ol><pre><code>![](https://docs.docker.com/engine/userguide/storagedriver/images/shared-uuid.jpg)</code></pre><p>Docker’s copy-on-write strategy not only reduces the amount of space consumed by containers, it also reduces the time required to start a container. At start time, Docker only has to create the thin writable layer for each container. The diagram below shows these 5 containers sharing a single read-only (RO) copy of the changed-ubuntu image.</p><p>If Docker had to make an entire copy of the underlying image stack each time it started a new container, container start times and disk space used would be significantly increased.</p><h2 id="Data-volumes-and-the-storage-driver"><a href="#Data-volumes-and-the-storage-driver" class="headerlink" title="Data volumes and the storage driver"></a>Data volumes and the storage driver</h2><p><strong>When a container is deleted, any data written to the container that is not stored in a data volume is deleted along with the container.</strong></p><p>A <strong>data volume</strong> is a directory or file in the Docker host’s filesystem that is mounted directly into a container. Data volumes are not controlled by the storage driver. Reads and writes to data volumes bypass the storage driver and operate at native host speeds. You can mount any number of data volumes into a container. Multiple containers can also share one or more data volumes.</p><p>The diagram below shows a single Docker host running two containers. Each container exists inside of its own address space within the Docker host’s local storage area (<code>/var/lib/docker/...</code>). There is also a single shared data volume located at <code>/data</code> on the Docker host. This is mounted directly into both containers.</p><p><img src="https://docs.docker.com/engine/userguide/storagedriver/images/shared-volume.jpg" alt=""></p><p>Data volumes reside outside of the local storage area on the Docker host, further reinforcing their independence from the storage driver’s control. When a container is deleted, any data stored in data volumes persists on the Docker host.</p><hr><h2 id="References"><a href="#References" class="headerlink" title="References:"></a>References:</h2><ul><li><a href="https://docs.docker.com/engine/userguide/storagedriver/imagesandcontainers/" target="_blank" rel="noopener">https://docs.docker.com/engine/userguide/storagedriver/imagesandcontainers/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;To use storage drivers effectively, you must understand how Docker builds and stores images. Then, you need an understanding of how these images are used by containers. Finally, you’ll need a short introduction to the technologies that enable both images and container operations.&lt;/p&gt;
&lt;h2 id=&quot;Images-and-layers&quot;&gt;&lt;a href=&quot;#Images-and-layers&quot; class=&quot;headerlink&quot; title=&quot;Images and layers&quot;&gt;&lt;/a&gt;Images and layers&lt;/h2&gt;&lt;p&gt;Each Docker image references &lt;strong&gt;a list of read-only layers&lt;/strong&gt; that represent filesystem differences. Layers are stacked on top of each other to form a base for a container’s root filesystem. The diagram below shows the Ubuntu 15.04 image comprising 4 stacked image layers.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://docs.docker.com/engine/userguide/storagedriver/images/image-layers.jpg&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;
&lt;p&gt;The Docker storage driver is responsible for stacking these layers and providing a single unified view.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When you create a new container, you add &lt;strong&gt;a new, thin, writable layer&lt;/strong&gt; on top of the underlying stack. This layer is often called the &lt;strong&gt;“container layer”&lt;/strong&gt;. All changes made to the running container - such as writing new files, modifying existing files, and deleting files - are written to this thin writable container layer. The diagram below shows a container based on the Ubuntu 15.04 image.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://docs.docker.com/engine/userguide/storagedriver/images/container-layers.jpg&quot; alt=&quot;container&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Content-Addressable-Storage&quot;&gt;&lt;a href=&quot;#Content-Addressable-Storage&quot; class=&quot;headerlink&quot; title=&quot;Content Addressable Storage&quot;&gt;&lt;/a&gt;Content Addressable Storage&lt;/h2&gt;&lt;p&gt;Docker 1.10 introduced &lt;strong&gt;a new content addressable storage model&lt;/strong&gt;. This is a completely new way to address image and layer data on disk. Previously, image and layer data was referenced and stored using a randomly generated UUID. In the new model this is replaced by a secure content hash.&lt;/p&gt;
&lt;p&gt;The new model improves security, provides a built-in way to avoid ID collisions, and guarantees data integrity after pull, push, load, and save operations. It also enables better sharing of layers by allowing many images to freely share their layers even if they didn’t come from the same build.&lt;/p&gt;
&lt;p&gt;The diagram below shows an updated version of the previous diagram, highlighting the changes implemented by Docker 1.10.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://docs.docker.com/engine/userguide/storagedriver/images/container-layers-cas.jpg&quot; alt=&quot;container&quot;&gt;&lt;/p&gt;
&lt;p&gt;As can be seen, all image layer IDs are cryptographic hashes, whereas the container ID is still a randomly generated UUID.&lt;/p&gt;
    
    </summary>
    
      <category term="tools" scheme="https://bowenli86.github.io/categories/tools/"/>
    
      <category term="docker" scheme="https://bowenli86.github.io/categories/tools/docker/"/>
    
    
      <category term="docker image" scheme="https://bowenli86.github.io/tags/docker-image/"/>
    
      <category term="docker container" scheme="https://bowenli86.github.io/tags/docker-container/"/>
    
      <category term="docker storage drive" scheme="https://bowenli86.github.io/tags/docker-storage-drive/"/>
    
  </entry>
  
  <entry>
    <title>Docker - Concept Intro</title>
    <link href="https://bowenli86.github.io/2016/12/29/tools/docker/Docker-Concept-Intro/"/>
    <id>https://bowenli86.github.io/2016/12/29/tools/docker/Docker-Concept-Intro/</id>
    <published>2016-12-30T06:47:05.000Z</published>
    <updated>2018-04-10T04:29:25.551Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Package-Your-Application-into-a-Standardized-Unit-for-Software-Development"><a href="#Package-Your-Application-into-a-Standardized-Unit-for-Software-Development" class="headerlink" title="Package Your Application into a Standardized Unit for Software Development"></a>Package Your Application into a Standardized Unit for Software Development</h1><p>Docker containers wrap a piece of software in a complete filesystem that contains everything needed to run: code, runtime, system tools, system libraries – anything that can be installed on a server. This guarantees that the software will always run the same, regardless of its environment.</p><h1 id="Comparing-Containers-and-Virtual-Machines"><a href="#Comparing-Containers-and-Virtual-Machines" class="headerlink" title="Comparing Containers and Virtual Machines"></a>Comparing Containers and Virtual Machines</h1><p>Containers and virtual machines have similar resource isolation and allocation benefits – but a different architectural approach allows containers to be more portable and efficient.</p><h2 id="Virtual-Machines"><a href="#Virtual-Machines" class="headerlink" title="Virtual Machines"></a>Virtual Machines</h2><p>Virtual machines include the application, the necessary binaries and libraries, and an entire guest operating system – all of which can amount to tens of GBs.</p><p><img src="https://www.docker.com/sites/default/files/WhatIsDocker_2_VMs_0-2_2.png" alt="VM"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">    App1            App2            App3</span><br><span class="line">-----------     -----------     -----------</span><br><span class="line"> Bins/Libs       Bins/Libs       Bins/Libs</span><br><span class="line">-----------     -----------     -----------</span><br><span class="line"> Guest OS        Guest OS        Guest OS</span><br><span class="line">---------------------------------------------</span><br><span class="line">              Hypervisor</span><br><span class="line">            ---------------</span><br><span class="line">                Host OS</span><br><span class="line">            ---------------</span><br><span class="line">            Infrastructure</span><br></pre></td></tr></table></figure><h2 id="Containers"><a href="#Containers" class="headerlink" title="Containers"></a>Containers</h2><p>Containers include the application and all of its dependencies –but share the kernel with other containers, running as isolated processes in user space on the host operating system. Docker containers are not tied to any specific infrastructure: they run on any computer, on any infrastructure, and in any cloud.</p><p><img src="https://www.docker.com/sites/default/files/WhatIsDocker_3_Containers_2_0.png" alt="container"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">    App1            App2            App3</span><br><span class="line">-----------     -----------     -----------</span><br><span class="line"> Bins/Libs       Bins/Libs       Bins/Libs</span><br><span class="line">---------------------------------------------</span><br><span class="line">              Docker Engine</span><br><span class="line">            ---------------</span><br><span class="line">                Host OS</span><br><span class="line">            ---------------</span><br><span class="line">            Infrastructure</span><br></pre></td></tr></table></figure><h1 id="Terminologies"><a href="#Terminologies" class="headerlink" title="Terminologies"></a>Terminologies</h1><h2 id="1-Images-and-Containers"><a href="#1-Images-and-Containers" class="headerlink" title="1. Images and Containers"></a>1. Images and Containers</h2><p><strong>Docker Engine</strong> provides the core Docker technology that enables images and containers. As the last step in your installation, you ran the <code>docker run hello-world</code> command. The command you ran had three parts.</p><p><img src="https://docs.docker.com/engine/getstarted/tutimg/container_explainer.png" alt="docker command"></p><ul><li>An <strong>image</strong> is <strong>a filesystem and parameters to use at runtime</strong>. It doesn’t have state and never changes.</li><li><p>A <strong>container</strong> is <strong>a running instance of an image</strong>. When you ran the command, Docker Engine:</p><ul><li>checked to see if you had the <code>hello-world</code> software image</li><li>downloaded the image from the <strong>Docker Hub</strong> (more about the hub later)</li><li>loaded the image into the container and <strong>“ran”</strong> it</li></ul></li></ul><p>Depending on how it was built, an image might run a simple, single command and then exit. This is what <strong>hello-world</strong> did.</p><p>A <strong>Docker image</strong>, though, is capable of much more. An image can start software as complex as a database, wait for you to add data, store the data for later use, and then wait for the next person.</p><p>Who built the <code>hello-world</code> software image though? In this case, Docker did but anyone can. <strong>Docker Engine</strong> lets people (or companies) create and share software through <strong>Docker images</strong>. Using <strong>Docker Engine</strong>, you don’t have to worry about whether your computer can run the software in a <strong>Docker image</strong> — a <strong>Docker container</strong> can always run it.</p><a id="more"></a><h2 id="2-Docker-Engine"><a href="#2-Docker-Engine" class="headerlink" title="2. Docker Engine"></a>2. Docker Engine</h2><p>The <strong>Docker Engine</strong> is <strong>a lightweight container runtime and robust tooling that builds and runs your container</strong>. </p><blockquote><p>My own explanation: So what the above statement is saying is: First, <strong>Docker Engine is a tool</strong>, a container runtime tool, and a robust tool. Second, <strong>Docker Engine runs containers</strong></p></blockquote><p>Docker allows you to package up application code and dependencies together in an isolated container that share the OS kernel on the host system. The in-host daemon communicates with the <strong>Docker Client</strong> to execute commands to build, ship and run containers.</p><h2 id="3-Docker-Machine"><a href="#3-Docker-Machine" class="headerlink" title="3. Docker Machine"></a>3. Docker Machine</h2><p>You can use <strong>Docker Machine</strong> to:</p><ul><li>Install and run Docker on Mac or Windows</li><li>Provision and manage multiple remote Docker hosts</li><li>Provision Swarm clusters</li></ul><p><strong>Docker Machine</strong> is a tool that lets you 1) install <strong>Docker Engine</strong> on virtual hosts, and 2) manage the hosts with <strong>docker-machine</strong> commands. </p><p>You can use Machine to create Docker hosts on your local Mac or Windows box, on your company network, in your data center, or on cloud providers like AWS or Digital Ocean.</p><p>Using <strong>docker-machine</strong> commands, you can start, inspect, stop, and restart a managed host, upgrade the Docker client and daemon, and configure a Docker client to talk to your host.</p><p>Point the Machine CLI at a running, managed host, and you can run docker commands directly on that host. For example, run docker-machine env default to point to a host called default, follow on-screen instructions to complete env setup, and run docker ps, docker run <code>hello-world</code>, and so forth.</p><p><strong>Docker Machine</strong> was the only way to run Docker on Mac or Windows previous to Docker v1.12. Starting with the beta program and Docker v1.12, Docker for Mac and Docker for Windows are available as native apps and the better choice for this use case on newer desktops and laptops. We encourage you to try out these new apps. <strong>The installers for Docker for Mac and Docker for Windows include Docker Machine, along with Docker Compose.</strong></p><h2 id="4-Docker-for-Mac"><a href="#4-Docker-for-Mac" class="headerlink" title="4. Docker for Mac"></a>4. Docker for Mac</h2><p><strong>Docker for Mac</strong> is a Mac <strong>native</strong> application, that you install in <code>/Applications</code>. At installation time, it creates symlinks in <code>/usr/local/bin</code> for <strong>docker</strong> and <strong>docker-compose</strong>, to the version of the commands inside the Mac application bundle, in <code>/Applications/Docker.app/Contents/Resources/bin</code>.</p><p>Here are some key points to know about <strong>Docker for Mac</strong> before you get started:</p><ul><li><p><strong>Docker for Mac</strong> does not use VirtualBox, but rather <strong><a href="https://github.com/docker/HyperKit/" target="_blank" rel="noopener">HyperKit</a></strong>, a lightweight macOS virtualization solution built on top of <strong>Hypervisor.framework</strong> in macOS 10.10 Yosemite and higher.</p></li><li><p>Installing <strong>Docker for Mac</strong> does not affect machines you created with <strong>Docker Machine</strong>. The install offers to copy containers and images from your local default machine (if one exists) to the new <strong>Docker for Mac HyperKit VM</strong>. If chosen, content from default is copied to the new <strong>Docker for Mac HyperKit VM</strong>, and your original default machine is kept as is.</p></li><li><p>The <strong>Docker for Mac</strong> application <strong>does not use docker-machine to provision that VM; but rather creates and manages it directly</strong>.</p></li><li><p><strong>At installation time, Docker for Mac provisions an HyperKit VM based on Alpine Linux, running Docker Engine</strong>. It exposes the docker API on a socket in <code>/var/run/docker.sock</code>. Since this is the default location where docker will look if no environment variables are set, you can start using <code>docker</code> and <code>docker-compose</code> without setting any environment variables.</p></li></ul><p>This setup is shown in the following diagram.</p><p><img src="https://docs.docker.com/docker-for-mac/images/docker-for-mac-install.png" alt="Docker for Mac"></p><p>With <strong>Docker for Mac</strong>, you get only one VM, and you don’t manage it. It is managed by the <strong>Docker for Mac</strong> application, which includes autoupdate to update the client and server versions of Docker.</p><p>If you need several VMs and want to manage the version of the Docker client or server you are using, you can continue to use <strong>docker-machine</strong>, on the same machine, as described in Docker Toolbox (<a href="https://docs.docker.com/toolbox/overview/" target="_blank" rel="noopener">Legacy desktop solution</a>) and Docker for Mac coexistence.</p><h2 id="5-Docker-Compose"><a href="#5-Docker-Compose" class="headerlink" title="5. Docker Compose"></a>5. Docker Compose</h2><p><strong>Docker Compose</strong> is <strong>a tool for defining and running multi-container Docker applications</strong>.</p><p>With <strong>Compose</strong>, you use <strong>a Compose file</strong> to configure your application’s services. Then, using a single command, you create and start all the services from your configuration.</p><p><strong>Compose</strong> is great for development, testing, and staging environments, as well as CI workflows.</p><p>Using <strong>Compose</strong> is basically a three-step process.</p><ul><li>Define your app’s environment with a <strong>Dockerfile</strong> so it can be reproduced anywhere.</li><li>Define the services that make up your app in <strong>docker-compose.yml</strong> so they can be run together in an isolated environment.</li><li>Lastly, run <code>docker-compose up</code>, and <strong>Compose</strong> will start and run your entire app.</li></ul><p>A <code>docker-compose.yml</code> looks like this:</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">'2'</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line"><span class="attr">  web:</span></span><br><span class="line"><span class="attr">    build:</span> <span class="string">.</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">"5000:5000"</span></span><br><span class="line"><span class="attr">    volumes:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">.:/code</span></span><br><span class="line"><span class="attr">    - logvolume01:</span><span class="string">/var/log</span></span><br><span class="line"><span class="attr">    links:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">redis</span></span><br><span class="line"><span class="attr">  redis:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">redis</span></span><br><span class="line"><span class="attr">volumes:</span></span><br><span class="line"><span class="attr">  logvolume01:</span> <span class="string">&#123;&#125;</span></span><br></pre></td></tr></table></figure><p>Compose has commands for managing the whole lifecycle of your application:</p><ul><li>Start, stop and rebuild services</li><li>View the status of running services</li><li>Stream the log output of running services</li><li>Run a one-off command on a service</li></ul><h2 id="6-Docker-Hub"><a href="#6-Docker-Hub" class="headerlink" title="6. Docker Hub"></a>6. Docker Hub</h2><p><strong><a href="https://hub.docker.com/" target="_blank" rel="noopener">Docker Hub</a></strong> is <strong>a cloud-based registry service which allows you to link to code repositories, build your images and test them, stores manually pushed images, and links to Docker Cloud so you can deploy images to your hosts</strong>.</p><p>It provides a centralized resource for container image discovery, distribution and change management, user and team collaboration, and workflow automation throughout the development pipeline.</p><p><strong>Docker Hub</strong> provides the following major features:</p><ul><li><strong>Image Repositories</strong>: Find, manage, and push and pull images from community, official, and private image libraries.</li><li><strong>Automated Builds</strong>: Automatically create new images when you make changes to a source code repository.</li><li><strong>Webhooks</strong>: A feature of Automated Builds, Webhooks let you trigger actions after a successful push to a repository.</li><li><strong>Organizations</strong>: Create work groups to manage access to image repositories.</li><li>GitHub and Bitbucket Integration: Add the Hub and your Docker Images to your current workflows.</li></ul><h1 id="Typical-Docker-Platform-Workflow"><a href="#Typical-Docker-Platform-Workflow" class="headerlink" title="Typical Docker Platform Workflow"></a>Typical Docker Platform Workflow</h1><ol><li><p>Get your code and its dependencies into Docker containers:</p><ul><li><p>Write a <code>Dockerfile</code> that specifies the execution environment and pulls in your code.</p></li><li><p>If your app depends on external applications (such as Redis, or MySQL), simply find them on a registry such as <strong>Docker Hub</strong>, and refer to them in a <code>Docker Compose file</code>, along with a reference to your application, so they’ll run simultaneously.</p><ul><li>Software providers also distribute paid software via the Docker Store.</li></ul></li><li><p>Build, then run your containers on a virtual host via <strong>Docker Machine</strong> as you develop.</p></li></ul></li><li><p>Configure networking and storage for your solution, if needed.</p></li><li><p>Upload builds to a registry (ours, yours, or your cloud provider’s), to collaborate with your team.</p></li><li><p>If you’re gonna need to scale your solution across multiple hosts (VMs or physical machines), plan for how you’ll set up your Swarm cluster and scale it to meet demand.<br>Note: Use Universal Control Plane and you can manage your Swarm cluster using a friendly UI!</p></li><li><p>Finally, deploy to your preferred cloud provider (or, for redundancy, multiple cloud providers) with Docker Cloud. Or, use Docker Datacenter, and deploy to your own on-premise hardware.</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Package-Your-Application-into-a-Standardized-Unit-for-Software-Development&quot;&gt;&lt;a href=&quot;#Package-Your-Application-into-a-Standardized-Unit-for-Software-Development&quot; class=&quot;headerlink&quot; title=&quot;Package Your Application into a Standardized Unit for Software Development&quot;&gt;&lt;/a&gt;Package Your Application into a Standardized Unit for Software Development&lt;/h1&gt;&lt;p&gt;Docker containers wrap a piece of software in a complete filesystem that contains everything needed to run: code, runtime, system tools, system libraries – anything that can be installed on a server. This guarantees that the software will always run the same, regardless of its environment.&lt;/p&gt;
&lt;h1 id=&quot;Comparing-Containers-and-Virtual-Machines&quot;&gt;&lt;a href=&quot;#Comparing-Containers-and-Virtual-Machines&quot; class=&quot;headerlink&quot; title=&quot;Comparing Containers and Virtual Machines&quot;&gt;&lt;/a&gt;Comparing Containers and Virtual Machines&lt;/h1&gt;&lt;p&gt;Containers and virtual machines have similar resource isolation and allocation benefits – but a different architectural approach allows containers to be more portable and efficient.&lt;/p&gt;
&lt;h2 id=&quot;Virtual-Machines&quot;&gt;&lt;a href=&quot;#Virtual-Machines&quot; class=&quot;headerlink&quot; title=&quot;Virtual Machines&quot;&gt;&lt;/a&gt;Virtual Machines&lt;/h2&gt;&lt;p&gt;Virtual machines include the application, the necessary binaries and libraries, and an entire guest operating system – all of which can amount to tens of GBs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.docker.com/sites/default/files/WhatIsDocker_2_VMs_0-2_2.png&quot; alt=&quot;VM&quot;&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;    App1            App2            App3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;-----------     -----------     -----------&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt; Bins/Libs       Bins/Libs       Bins/Libs&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;-----------     -----------     -----------&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt; Guest OS        Guest OS        Guest OS&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;---------------------------------------------&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;              Hypervisor&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            ---------------&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                Host OS&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            ---------------&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            Infrastructure&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;Containers&quot;&gt;&lt;a href=&quot;#Containers&quot; class=&quot;headerlink&quot; title=&quot;Containers&quot;&gt;&lt;/a&gt;Containers&lt;/h2&gt;&lt;p&gt;Containers include the application and all of its dependencies –but share the kernel with other containers, running as isolated processes in user space on the host operating system. Docker containers are not tied to any specific infrastructure: they run on any computer, on any infrastructure, and in any cloud.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.docker.com/sites/default/files/WhatIsDocker_3_Containers_2_0.png&quot; alt=&quot;container&quot;&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;    App1            App2            App3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;-----------     -----------     -----------&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt; Bins/Libs       Bins/Libs       Bins/Libs&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;---------------------------------------------&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;              Docker Engine&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            ---------------&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                Host OS&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            ---------------&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            Infrastructure&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h1 id=&quot;Terminologies&quot;&gt;&lt;a href=&quot;#Terminologies&quot; class=&quot;headerlink&quot; title=&quot;Terminologies&quot;&gt;&lt;/a&gt;Terminologies&lt;/h1&gt;&lt;h2 id=&quot;1-Images-and-Containers&quot;&gt;&lt;a href=&quot;#1-Images-and-Containers&quot; class=&quot;headerlink&quot; title=&quot;1. Images and Containers&quot;&gt;&lt;/a&gt;1. Images and Containers&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Docker Engine&lt;/strong&gt; provides the core Docker technology that enables images and containers. As the last step in your installation, you ran the &lt;code&gt;docker run hello-world&lt;/code&gt; command. The command you ran had three parts.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://docs.docker.com/engine/getstarted/tutimg/container_explainer.png&quot; alt=&quot;docker command&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An &lt;strong&gt;image&lt;/strong&gt; is &lt;strong&gt;a filesystem and parameters to use at runtime&lt;/strong&gt;. It doesn’t have state and never changes.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A &lt;strong&gt;container&lt;/strong&gt; is &lt;strong&gt;a running instance of an image&lt;/strong&gt;. When you ran the command, Docker Engine:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;checked to see if you had the &lt;code&gt;hello-world&lt;/code&gt; software image&lt;/li&gt;
&lt;li&gt;downloaded the image from the &lt;strong&gt;Docker Hub&lt;/strong&gt; (more about the hub later)&lt;/li&gt;
&lt;li&gt;loaded the image into the container and &lt;strong&gt;“ran”&lt;/strong&gt; it&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Depending on how it was built, an image might run a simple, single command and then exit. This is what &lt;strong&gt;hello-world&lt;/strong&gt; did.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;Docker image&lt;/strong&gt;, though, is capable of much more. An image can start software as complex as a database, wait for you to add data, store the data for later use, and then wait for the next person.&lt;/p&gt;
&lt;p&gt;Who built the &lt;code&gt;hello-world&lt;/code&gt; software image though? In this case, Docker did but anyone can. &lt;strong&gt;Docker Engine&lt;/strong&gt; lets people (or companies) create and share software through &lt;strong&gt;Docker images&lt;/strong&gt;. Using &lt;strong&gt;Docker Engine&lt;/strong&gt;, you don’t have to worry about whether your computer can run the software in a &lt;strong&gt;Docker image&lt;/strong&gt; — a &lt;strong&gt;Docker container&lt;/strong&gt; can always run it.&lt;/p&gt;
    
    </summary>
    
      <category term="tools" scheme="https://bowenli86.github.io/categories/tools/"/>
    
      <category term="docker" scheme="https://bowenli86.github.io/categories/tools/docker/"/>
    
    
      <category term="docker" scheme="https://bowenli86.github.io/tags/docker/"/>
    
      <category term="docker image" scheme="https://bowenli86.github.io/tags/docker-image/"/>
    
      <category term="docker container" scheme="https://bowenli86.github.io/tags/docker-container/"/>
    
      <category term="docker engine" scheme="https://bowenli86.github.io/tags/docker-engine/"/>
    
      <category term="docker machine" scheme="https://bowenli86.github.io/tags/docker-machine/"/>
    
      <category term="docker hub" scheme="https://bowenli86.github.io/tags/docker-hub/"/>
    
      <category term="docker compose" scheme="https://bowenli86.github.io/tags/docker-compose/"/>
    
  </entry>
  
  <entry>
    <title>How to Conduct Performance Testing - 如何做性能测试</title>
    <link href="https://bowenli86.github.io/2016/12/21/test/How-to-Conduct-Performance-Testing-%E5%A6%82%E4%BD%95%E5%81%9A%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"/>
    <id>https://bowenli86.github.io/2016/12/21/test/How-to-Conduct-Performance-Testing-如何做性能测试/</id>
    <published>2016-12-21T23:16:44.000Z</published>
    <updated>2018-04-10T04:29:25.549Z</updated>
    
    <content type="html"><![CDATA[<p>The original blog is <a href="http://coolshell.cn/articles/17381.html" target="_blank" rel="noopener">http://coolshell.cn/articles/17381.html</a></p><p>Some key take-aways are:</p><ul><li><strong>Average</strong> is not reliable, use <strong>Percentile</strong>. <ul><li>E.g. 99.9% responses should be within 1ms.</li></ul></li><li><strong>Response Latency</strong> should be related to <strong>Throughput</strong><ul><li>E.g. When 99% responses are within 100ms, the max thoughput is 1000qps</li></ul></li><li><strong>Throughput</strong> should be related to <strong>Success Rate</strong><ul><li>E.g. for some critical system, the success rate should be 100%</li></ul></li><li>How to do a strict performance test?<ol><li>Define expected <strong>system response latency</strong> and <strong>success rate</strong></li><li>Find the <strong>max throughput</strong> under those expected limitations</li><li>Conduct <strong>soak test</strong>, that is for example put the <strong>max throughput</strong> on the system for 7 consecutive days</li><li>Define and find the <strong>extreme throughput</strong> of the system. E.g. An extreme value can be defined as - the system’s throughput in 10 mins when success rate is 100%</li><li>Conduct <strong>burst test</strong>, that is for example put <strong>max throughput</strong> on the system for 5 mins, put <strong>extreme throughput</strong> for 1 min, put <strong>max throughput</strong> for another 5 mins, put <strong>extreme throughput</strong> for another 1 min… back and forth for 2 days.</li></ol></li></ul><a id="more"></a><hr><p>偶然间看到了阿里中间件Dubbo的性能测试报告，我觉得这份性能测试报告让人觉得做这性能测试的人根本不懂性能测试，我觉得这份报告会把大众带沟里去，所以，想写下这篇文章，做一点科普。</p><p>首先，这份测试报告里的主要问题如下：</p><p>1）用的全是平均值。老实说，平均值是非常不靠谱的。</p><p>2）响应时间没有和吞吐量 TPS/QPS 挂钩。 而只是测试了低速率的情况，这是完全错误的。</p><p>3）响应时间和吞吐量没有和成功率挂钩。</p><h2 id="为什么平均值不靠谱"><a href="#为什么平均值不靠谱" class="headerlink" title="为什么平均值不靠谱"></a>为什么平均值不靠谱</h2><p>关于平均值为什么不靠谱，我相信大家读新闻的时候经常可以看到，平均工资，平均房价，平均支出，等等这样的字眼，你就知道为什么平均值不靠谱了。（这些都是数学游戏，对于理工科的同学来说，天生应该有免疫力）</p><p>软件的性能测试也一样，平均数也是不靠谱的，这里可以参看这篇详细的文章《<em>Why Averages Suck and Percentiles are Great</em>》，我在这里简单说一下。</p><p>我们知道，性能测试时，测试得到的结果数据不总是一样的，而是有高有低的，如果算平均值就会出现这样的情况，假如，测试了10次，有9次是1ms，而有1次是1s，那么平均数据就是100ms，很明显，这完全不能反应性能测试的情况，也许那1s的请求就是一个不正常的值，是个噪点，应该去掉。所以，我们会在一些评委打分中看到要去掉一个最高分一个最低分，然后再算平均值。</p><p>另外，中位数（Mean）可能会比平均数要稍微靠谱一些，所谓中位数的意就是把将一组数据按大小顺序排列，处在最中间位置的一个数叫做这组数据的中位数 ，这意味着至少有50%的数据低于或高于这个中位数。</p><p>当然，最为正确的统计做法是用百分比分布统计。也就是英文中的TP – Top Percentile，TP50的意思在，50%的请求都小于某个值，TP90表示90%的请求小于某个时间。</p><p>比如：我们有一组数据：[ 10ms,  1s, 200ms, 100ms]，我们把其从小到大排个序：[10ms, 100ms, 200ms, 1s]，于是我们知道，TP50，就是50%的请求ceil(4<em>0.5)=2时间是小于100ms的，TP90就是90%的请求ceil(4</em>0.9)=4时间小于1s。于是：TP50就是100ms，TP90就是1s。</p><p>我以前在路透做的金融系统响应时间的性能测试的要求是这样的，99.9%的请求必须小于1ms，所有的平均时间必须小于1ms。两个条件的限制。</p><p>为什么响应时间（latency）要和吞吐量（Thoughput）挂钩<br>系统的性能如果只看吞吐量，不看响应时间是没有意义的。我的系统可以顶10万请求，但是响应时间已经到了5秒钟，这样的系统已经不可用了，这样的吞吐量也是没有意义的。</p><p>我们知道，当并发量（吞吐量）上涨的时候，系统会变得越来越不稳定，响应时间的波动也会越来越大，响应时间也会变得越来越慢，而吞吐率也越来越上不去（如下图所示），包括CPU的使用率情况也会如此。所以，当系统变得不稳定的时候，吞吐量已经没有意义了。吞吐量有意义的时候仅当系统稳定的时候。</p><h2 id="Benchmark-Optimal-Rate"><a href="#Benchmark-Optimal-Rate" class="headerlink" title="Benchmark Optimal Rate"></a>Benchmark Optimal Rate</h2><p>所以，吞吐量的值必需有响应时间来卡。比如：TP99小于100ms的时候，系统可以承载的最大并发数是1000qps。这意味着，我们要不断的在不同的并发数上测试，以找到软件的最稳定时的最大吞吐量。</p><p>为什么响应时间吞吐量和成功率要挂钩<br>我们这应该不难理解了，如果请求不成功的话，都还做毛的性能测试。比如，我说我的系统并发可以达到10万，但是失败率是</p><p>40%，那么，这10万的并发完全就是一个笑话了。</p><p>性能测试的失败率的容忍应该是非常低的。对于一些关键系统，成功请求数必须在100%，一点都不能含糊。</p><h2 id="如何严谨地做性能测试"><a href="#如何严谨地做性能测试" class="headerlink" title="如何严谨地做性能测试"></a>如何严谨地做性能测试</h2><p>一般来说，性能测试要统一考虑这么几个因素：Thoughput吞吐量，Latency响应时间，资源利用（CPU/MEM/IO/Bandwidth…），成功率，系统稳定性。</p><p>下面的这些性能测试的方式基本上来源自我的老老东家汤森路透，一家做real-time的金融数据系统的公司。</p><p>一，你得定义一个系统的响应时间latency，建议是TP99，以及成功率。比如路透的定义：99.9%的响应时间必需在1ms之内，平均响应时间在1ms以内，100%的请求成功。</p><p>二，在这个响应时间的限制下，找到最高的吞吐量。测试用的数据，需要有大中小各种尺寸的数据，并可以混合。最好使用生产线上的测试数据。</p><p>三，在这个吞吐量做 Soak Test， 比如：使用第二步测试得到的吞吐量连续7天的不间断的压测系统。然后收集CPU，内存，硬盘/网络IO，等指标，查看系统是否稳定，比如，CPU是平稳的，内存使用也是平稳的。那么，这个值就是系统的性能</p><p>四，找到系统的极限值。比如：在成功率100%的情况下（不考虑响应时间的长短），系统能坚持10分钟的吞吐量。</p><p>五，做Burst Test。用第二步得到的吞吐量执行5分钟，然后在第四步得到的极限值执行1分钟，再回到第二步的吞吐量执行5钟，再到第四步的权限值执行1分钟，如此往复个一段时间，比如2天。收集系统数据：CPU、内存、硬盘/网络IO等，观察他们的曲线，以及相应的响应时间，确保系统是稳定的。</p><p>六、低吞吐量和网络小包的测试。有时候，在低吞吐量的时候，可能会导致latency上升，比如TCP_NODELAY的参数没有开启会导致latency上升（详见TCP的那些事），而网络小包会导致带宽用不满也会导致性能上不去，所以，性能测试还需要根据实际情况有选择的测试一下这两咱场景。</p><p>（注：在路透，路透会用第二步得到的吞吐量乘以66.7%来做为系统的软报警线，80%做为系统的硬报警线，而极限值仅仅用来扛突发的peak）</p><p>是不是很繁锁？是的，只因为，这是工程，工程是一门科学，科学是严谨的。</p><p>欢迎大家也分享一下你们性能测试的经验和方法。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The original blog is &lt;a href=&quot;http://coolshell.cn/articles/17381.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://coolshell.cn/articles/17381.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Some key take-aways are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Average&lt;/strong&gt; is not reliable, use &lt;strong&gt;Percentile&lt;/strong&gt;. &lt;ul&gt;
&lt;li&gt;E.g. 99.9% responses should be within 1ms.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Response Latency&lt;/strong&gt; should be related to &lt;strong&gt;Throughput&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;E.g. When 99% responses are within 100ms, the max thoughput is 1000qps&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Throughput&lt;/strong&gt; should be related to &lt;strong&gt;Success Rate&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;E.g. for some critical system, the success rate should be 100%&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How to do a strict performance test?&lt;ol&gt;
&lt;li&gt;Define expected &lt;strong&gt;system response latency&lt;/strong&gt; and &lt;strong&gt;success rate&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Find the &lt;strong&gt;max throughput&lt;/strong&gt; under those expected limitations&lt;/li&gt;
&lt;li&gt;Conduct &lt;strong&gt;soak test&lt;/strong&gt;, that is for example put the &lt;strong&gt;max throughput&lt;/strong&gt; on the system for 7 consecutive days&lt;/li&gt;
&lt;li&gt;Define and find the &lt;strong&gt;extreme throughput&lt;/strong&gt; of the system. E.g. An extreme value can be defined as - the system’s throughput in 10 mins when success rate is 100%&lt;/li&gt;
&lt;li&gt;Conduct &lt;strong&gt;burst test&lt;/strong&gt;, that is for example put &lt;strong&gt;max throughput&lt;/strong&gt; on the system for 5 mins, put &lt;strong&gt;extreme throughput&lt;/strong&gt; for 1 min, put &lt;strong&gt;max throughput&lt;/strong&gt; for another 5 mins, put &lt;strong&gt;extreme throughput&lt;/strong&gt; for another 1 min… back and forth for 2 days.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="test" scheme="https://bowenli86.github.io/categories/test/"/>
    
    
      <category term="performance test" scheme="https://bowenli86.github.io/tags/performance-test/"/>
    
      <category term="sock test" scheme="https://bowenli86.github.io/tags/sock-test/"/>
    
      <category term="burst test" scheme="https://bowenli86.github.io/tags/burst-test/"/>
    
  </entry>
  
  <entry>
    <title>.bash_profile and how to permenantly set environment variables on Mac</title>
    <link href="https://bowenli86.github.io/2016/12/19/tools/mac/bash-profile-and-how-to-permenantly-set-environment-variables-on-Mac/"/>
    <id>https://bowenli86.github.io/2016/12/19/tools/mac/bash-profile-and-how-to-permenantly-set-environment-variables-on-Mac/</id>
    <published>2016-12-19T21:44:55.000Z</published>
    <updated>2018-04-10T04:29:25.552Z</updated>
    
    <content type="html"><![CDATA[<h2 id="bash-profile-file"><a href="#bash-profile-file" class="headerlink" title=".bash_profile file"></a>.bash_profile file</h2><p>If you’re not familiar with a <strong>.bash_profile</strong> file, this is a startup file that is read whenever you open a new Terminal window. It’s a special configuration file, and it needs to be placed in your home directory. For instance, on my MacBook Pro, this file is located as <code>/Users/bowen.li/.bash_profile</code>.</p><h3 id="My-bash-profile-file"><a href="#My-bash-profile-file" class="headerlink" title="My .bash_profile file"></a>My .bash_profile file</h3><p>As you can see from my sample file shown below, the .bash_profile file can contain any legal Unix command, including Unix alias  definitions, Unix export and PATH statements, and other commands to set up your Bash prompt.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># aliases</span><br><span class="line">alias cd..=&quot;cd ..&quot;</span><br><span class="line"></span><br><span class="line"># Setting PATH for Python 3.5</span><br><span class="line"># The original version is saved in .bash_profile.pysave</span><br><span class="line">PATH=&quot;/Library/Frameworks/Python.framework/Versions/3.5/bin:$&#123;PATH&#125;&quot;</span><br><span class="line">export PATH</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">export SNOWFLAKE_USER_ENV=&quot;xxx&quot;</span><br><span class="line">export SNOWFLAKE_PASSWORD_ENV=&quot;xxxx&quot;</span><br></pre></td></tr></table></figure><h2 id="How-to-permenantly-set-environment-variables-on-Mac"><a href="#How-to-permenantly-set-environment-variables-on-Mac" class="headerlink" title="How to permenantly set environment variables on Mac"></a>How to permenantly set environment variables on Mac</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo $YOUR_KEY</span><br></pre></td></tr></table></figure><p>it prints current path value.</p><p>Then do</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bash_profile</span><br></pre></td></tr></table></figure><p>and write</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export YOUR_KEY=YOUR_VALUE</span><br></pre></td></tr></table></figure><p>then do</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bash_profile</span><br></pre></td></tr></table></figure><p>this will execute it and add the path</p><p>then again check with</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo $YOUR_KEY</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;bash-profile-file&quot;&gt;&lt;a href=&quot;#bash-profile-file&quot; class=&quot;headerlink&quot; title=&quot;.bash_profile file&quot;&gt;&lt;/a&gt;.bash_profile file&lt;/h2&gt;&lt;p&gt;If you’r
      
    
    </summary>
    
      <category term="tools" scheme="https://bowenli86.github.io/categories/tools/"/>
    
      <category term="mac" scheme="https://bowenli86.github.io/categories/tools/mac/"/>
    
    
      <category term=".bash_profile" scheme="https://bowenli86.github.io/tags/bash-profile/"/>
    
      <category term="mac environment variable" scheme="https://bowenli86.github.io/tags/mac-environment-variable/"/>
    
  </entry>
  
  <entry>
    <title>OutOfMemoryError and StackOverflowError</title>
    <link href="https://bowenli86.github.io/2016/12/05/java/exception%20and%20error/OutOfMemoryError-and-StackOverflowError/"/>
    <id>https://bowenli86.github.io/2016/12/05/java/exception and error/OutOfMemoryError-and-StackOverflowError/</id>
    <published>2016-12-06T07:42:44.000Z</published>
    <updated>2018-04-10T04:29:25.533Z</updated>
    
    <content type="html"><![CDATA[<p>I have been confused of <strong>OutOfMemoryError</strong> and <strong>StackOverflowError</strong>, not being able to distinguish differences between them. So here comes this post.</p><h2 id="OutOfMemoryError"><a href="#OutOfMemoryError" class="headerlink" title="OutOfMemoryError"></a>OutOfMemoryError</h2><p><a href="https://docs.oracle.com/javase/8/docs/api/java/lang/OutOfMemoryError.html" target="_blank" rel="noopener">https://docs.oracle.com/javase/8/docs/api/java/lang/OutOfMemoryError.html</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">java.lang.Object</span><br><span class="line">    - java.lang.Throwable</span><br><span class="line">        - java.lang.Error</span><br><span class="line">            - java.lang.VirtualMachineError</span><br><span class="line">                - java.lang.OutOfMemoryError</span><br></pre></td></tr></table></figure><blockquote><p>Thrown when the Java Virtual Machine cannot allocate an object because it is out of memory, and no more memory could be made available by the garbage collector. OutOfMemoryError objects may be constructed by the virtual machine as if suppression were disabled and/or the stack trace was not writable.</p></blockquote><h2 id="StackOverflowError"><a href="#StackOverflowError" class="headerlink" title="StackOverflowError"></a>StackOverflowError</h2><p><a href="https://docs.oracle.com/javase/8/docs/api/java/lang/StackOverflowError.html" target="_blank" rel="noopener">https://docs.oracle.com/javase/8/docs/api/java/lang/StackOverflowError.html</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">java.lang.Object</span><br><span class="line">    - java.lang.Throwable</span><br><span class="line">        - java.lang.Error</span><br><span class="line">            - java.lang.VirtualMachineError</span><br><span class="line">                - java.lang.StackOverflowError</span><br></pre></td></tr></table></figure><blockquote><p>Thrown when a stack overflow occurs because an application recurses too deeply.</p></blockquote><h2 id="Error-Exception-and-RuntimeException-The-First-Observation"><a href="#Error-Exception-and-RuntimeException-The-First-Observation" class="headerlink" title="Error, Exception, and RuntimeException - The First Observation"></a>Error, Exception, and RuntimeException - The First Observation</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">- java.lang.Object</span><br><span class="line">    - java.lang.Throwable</span><br><span class="line">        - java.lang.Error</span><br><span class="line">        - java.lang.Exception</span><br><span class="line">        - java.lang.RuntimeException</span><br></pre></td></tr></table></figure><p>The first observation is that these are of <strong>Error</strong>, not <strong>Exception</strong> or <strong>RuntimeException</strong>. As you can see from the above hierarchy, <strong>Error</strong>, <strong>Exception</strong>, and <strong>RuntimeException</strong> are three equative classes. </p><p>According to <a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Error.html" target="_blank" rel="noopener">https://docs.oracle.com/javase/8/docs/api/java/lang/Error.html</a></p><blockquote><p>An <strong>Error</strong> is a subclass of <strong>Throwable</strong> that <strong>indicates serious problems that a reasonable application should not try to catch</strong>. Most such errors are abnormal conditions. The ThreadDeath error, though a “normal” condition, is also a subclass of Error because most applications should not try to catch it.</p></blockquote><blockquote><p>A method is not required to declare in its throws clause any subclasses of Error that might be thrown during the execution of the method but not caught, since these errors are abnormal conditions that should never occur. That is, Error and its subclasses are regarded as unchecked exceptions for the purposes of compile-time checking of exceptions.</p></blockquote><p>So, <strong>Error</strong> should not be explicitly caught and handled by your application.</p><p>Ok, good to get clarified we are not talking about <strong>Exception</strong> or <strong>RuntimeException</strong>. </p><p>Here’s another of my posts talking specifically for Java Exception <a href="https://phoenixjiangnan.github.io/2016/07/04/java/exception/Java%20-%20Exception%20-%20Principles%20to%20Handle%20Exception/" target="_blank" rel="noopener">https://phoenixjiangnan.github.io/2016/07/04/java/exception/Java%20-%20Exception%20-%20Principles%20to%20Handle%20Exception/</a></p><p>Let’s get focused on <strong>Error</strong>.</p><a id="more"></a><h2 id="OutOfMemoryError-v-s-StackOverflowError"><a href="#OutOfMemoryError-v-s-StackOverflowError" class="headerlink" title="OutOfMemoryError v.s. StackOverflowError"></a>OutOfMemoryError v.s. StackOverflowError</h2><p>When you start JVM you define how much <strong>RAM</strong> it can use for processing. JVM divides this into certain memory locations for its processing purpose, two of those are <strong>Stack</strong> and <strong>Heap</strong>.</p><h3 id="Heap-and-OutOfMemoryError"><a href="#Heap-and-OutOfMemoryError" class="headerlink" title="Heap and OutOfMemoryError"></a>Heap and OutOfMemoryError</h3><p><strong>OutOfMemoryError</strong> is related to <strong>Heap</strong>.</p><p>If you have large objects (or) referenced objects in memeory, then you will see <strong>OutofMemoryError</strong>. If you have strong references to objects, then GC can’t clean the memory space allocated for that object. When JVM tries to allocate memory for new object and not enough space available it throws <strong>OutofMemoryError</strong> because it can’t allocate required amount of memory.</p><blockquote><p>How to avoid: Make sure un-necessary objects are available for GC</p></blockquote><h3 id="Stack-and-StackOverflowError"><a href="#Stack-and-StackOverflowError" class="headerlink" title="Stack and StackOverflowError"></a>Stack and StackOverflowError</h3><p><strong>StackOverflowError</strong> is related to <strong>stack</strong>.</p><p>All your local variables and methods calls related data will be on stack. For every method call, one stack frame will be created and local as well as method call related data will be placed inside the stack frame. Once method execution is completed, stack frame will be removed. ONE WAY to reproduce this is, have infinite loop for method call, you will see stackoverflow error, because stack frame will be populated with method data for every call but it won’t be freed (removed).</p><blockquote><p>How to avoid Make sure method calls are ending (not in infinite loop)</p></blockquote><h2 id="Java-Stack-vs-Heap"><a href="#Java-Stack-vs-Heap" class="headerlink" title="Java Stack vs Heap"></a>Java Stack vs Heap</h2><h3 id="Java-Heap-Space"><a href="#Java-Heap-Space" class="headerlink" title="Java Heap Space"></a>Java Heap Space</h3><p>Java Heap space is used by java runtime to allocate memory to <strong>Objects and JRE classes</strong>.</p><p>Whenever we create an object, it’s always created in the Heap space. <strong>Garbage Collection</strong> runs on the heap memory to free the memory used by objects that doesn’t have any reference. Any object created in the heap space has global access and can be referenced from anywhere of the application.</p><h3 id="Java-Stack-Memory"><a href="#Java-Stack-Memory" class="headerlink" title="Java Stack Memory"></a>Java Stack Memory</h3><p>Java Stack memory is used for execution of a thread. They contain <strong>method specific values</strong> that are short-lived and <strong>references</strong> to other objects in the heap that are getting referred from the method.</p><p>Stack memory is always referenced in LIFO (Last-In-First-Out) order. Whenever a method is invoked, a new block is created in the stack memory for the method to hold local primitive values and reference to other objects in the method. As soon as method ends, the block becomes unused and become available for next method.<br>Stack memory size is very less compared to Heap memory.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Did you get the differences between those two errors? I did!</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I have been confused of &lt;strong&gt;OutOfMemoryError&lt;/strong&gt; and &lt;strong&gt;StackOverflowError&lt;/strong&gt;, not being able to distinguish differences between them. So here comes this post.&lt;/p&gt;
&lt;h2 id=&quot;OutOfMemoryError&quot;&gt;&lt;a href=&quot;#OutOfMemoryError&quot; class=&quot;headerlink&quot; title=&quot;OutOfMemoryError&quot;&gt;&lt;/a&gt;OutOfMemoryError&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://docs.oracle.com/javase/8/docs/api/java/lang/OutOfMemoryError.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://docs.oracle.com/javase/8/docs/api/java/lang/OutOfMemoryError.html&lt;/a&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;java.lang.Object&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    - java.lang.Throwable&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        - java.lang.Error&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            - java.lang.VirtualMachineError&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                - java.lang.OutOfMemoryError&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;Thrown when the Java Virtual Machine cannot allocate an object because it is out of memory, and no more memory could be made available by the garbage collector. OutOfMemoryError objects may be constructed by the virtual machine as if suppression were disabled and/or the stack trace was not writable.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;StackOverflowError&quot;&gt;&lt;a href=&quot;#StackOverflowError&quot; class=&quot;headerlink&quot; title=&quot;StackOverflowError&quot;&gt;&lt;/a&gt;StackOverflowError&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://docs.oracle.com/javase/8/docs/api/java/lang/StackOverflowError.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://docs.oracle.com/javase/8/docs/api/java/lang/StackOverflowError.html&lt;/a&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;java.lang.Object&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    - java.lang.Throwable&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        - java.lang.Error&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            - java.lang.VirtualMachineError&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                - java.lang.StackOverflowError&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;Thrown when a stack overflow occurs because an application recurses too deeply.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;Error-Exception-and-RuntimeException-The-First-Observation&quot;&gt;&lt;a href=&quot;#Error-Exception-and-RuntimeException-The-First-Observation&quot; class=&quot;headerlink&quot; title=&quot;Error, Exception, and RuntimeException - The First Observation&quot;&gt;&lt;/a&gt;Error, Exception, and RuntimeException - The First Observation&lt;/h2&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;- java.lang.Object&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    - java.lang.Throwable&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        - java.lang.Error&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        - java.lang.Exception&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        - java.lang.RuntimeException&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;The first observation is that these are of &lt;strong&gt;Error&lt;/strong&gt;, not &lt;strong&gt;Exception&lt;/strong&gt; or &lt;strong&gt;RuntimeException&lt;/strong&gt;. As you can see from the above hierarchy, &lt;strong&gt;Error&lt;/strong&gt;, &lt;strong&gt;Exception&lt;/strong&gt;, and &lt;strong&gt;RuntimeException&lt;/strong&gt; are three equative classes. &lt;/p&gt;
&lt;p&gt;According to &lt;a href=&quot;https://docs.oracle.com/javase/8/docs/api/java/lang/Error.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://docs.oracle.com/javase/8/docs/api/java/lang/Error.html&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;An &lt;strong&gt;Error&lt;/strong&gt; is a subclass of &lt;strong&gt;Throwable&lt;/strong&gt; that &lt;strong&gt;indicates serious problems that a reasonable application should not try to catch&lt;/strong&gt;. Most such errors are abnormal conditions. The ThreadDeath error, though a “normal” condition, is also a subclass of Error because most applications should not try to catch it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;A method is not required to declare in its throws clause any subclasses of Error that might be thrown during the execution of the method but not caught, since these errors are abnormal conditions that should never occur. That is, Error and its subclasses are regarded as unchecked exceptions for the purposes of compile-time checking of exceptions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So, &lt;strong&gt;Error&lt;/strong&gt; should not be explicitly caught and handled by your application.&lt;/p&gt;
&lt;p&gt;Ok, good to get clarified we are not talking about &lt;strong&gt;Exception&lt;/strong&gt; or &lt;strong&gt;RuntimeException&lt;/strong&gt;. &lt;/p&gt;
&lt;p&gt;Here’s another of my posts talking specifically for Java Exception &lt;a href=&quot;https://phoenixjiangnan.github.io/2016/07/04/java/exception/Java%20-%20Exception%20-%20Principles%20to%20Handle%20Exception/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://phoenixjiangnan.github.io/2016/07/04/java/exception/Java%20-%20Exception%20-%20Principles%20to%20Handle%20Exception/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Let’s get focused on &lt;strong&gt;Error&lt;/strong&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="java" scheme="https://bowenli86.github.io/categories/java/"/>
    
      <category term="exception and error" scheme="https://bowenli86.github.io/categories/java/exception-and-error/"/>
    
    
      <category term="OutOfMemoryError" scheme="https://bowenli86.github.io/tags/OutOfMemoryError/"/>
    
      <category term="StackOverflowError" scheme="https://bowenli86.github.io/tags/StackOverflowError/"/>
    
      <category term="error" scheme="https://bowenli86.github.io/tags/error/"/>
    
      <category term="exception" scheme="https://bowenli86.github.io/tags/exception/"/>
    
      <category term="java heap" scheme="https://bowenli86.github.io/tags/java-heap/"/>
    
      <category term="java stack" scheme="https://bowenli86.github.io/tags/java-stack/"/>
    
  </entry>
  
  <entry>
    <title>Big Data and Google&#39;s Three Paper II - BigTable</title>
    <link href="https://bowenli86.github.io/2016/11/29/distributed%20system/data/Big-Data-and-Google-s-Three-Paper-II-BigTable/"/>
    <id>https://bowenli86.github.io/2016/11/29/distributed system/data/Big-Data-and-Google-s-Three-Paper-II-BigTable/</id>
    <published>2016-11-29T08:42:41.000Z</published>
    <updated>2018-04-10T04:29:25.521Z</updated>
    
    <content type="html"><![CDATA[<p>The following ideas are from a ‘blog’ of one of my friends. He posted this ‘blog’ on Wechat, and Wechat doesn’t provide url or allow external users to access it. So I can’t referenece a link here.</p><hr><p>BigTable is most diffult to understand paper because it 1) names lots of its terminologies quite misleadingly 2) barely illustrated any detail.</p><h2 id="1-Misleading-Names-Let’s-rephrase-and-better-understand-them"><a href="#1-Misleading-Names-Let’s-rephrase-and-better-understand-them" class="headerlink" title="1. Misleading Names - Let’s rephrase and better understand them"></a>1. Misleading Names - Let’s rephrase and better understand them</h2><p>I’ve been confused of BigTable, until one day I read a blog called <a href="http://jimbojw.com/wiki/index.php?title=Understanding_Hbase_and_BigTable" target="_blank" rel="noopener">Understanding HBase and BigTable</a>. I strongly recommending everyone find and read that blog.</p><p>Here is the first sentence of the “Data Model” section:</p><pre><code>&gt; A Bigtable is a sparse, distributed, persistent multidimensional sorted map.</code></pre><p>So, BigTable is a <strong>Map</strong>, not a <strong>Table</strong>! Right, you’ve been misleaded to associate BigTable with a database table because of namings. What is a map? A map is key-value data structure! Thus, BigTable is a key-value store. Probably Google should better name it <strong>BigMap</strong> instead of <strong>BigTable</strong>!</p><p>The BigTable paper continues, explaining that:</p><pre><code>&gt; The map is indexed by a row key, column key, and a timestamp; each value in the map is an uninterpreted array of bytes.</code></pre><p>So how’s the map look like? The Map’s value is an array of bytes. Fairly simple. Map’s key as <code>&lt;key1, key2, key3&gt;</code> is a combination of <code>&lt;row, column, timestamp&gt;</code>, <code>row</code> and <code>column</code> as both string, <code>timestamp</code> as a 64 bit number.</p><p>The <code>column</code> key is more complex, it’s formatted as <code>column</code> = <code>prefix:suffix</code>. <code>prefix</code> is called <strong>column family</strong>, which is of limited number in each BigMap. <code>suffix</code> can be of unlimited number.</p><p>Till now, you can tell that <strong>BigTable</strong> has nothing to do with a regular table in transactional database. It’s key-value data model.</p><a id="more"></a><h3 id="Sorted"><a href="#Sorted" class="headerlink" title="Sorted"></a>Sorted</h3><p>BigMap is a SortedMap, not a HashMap. HBase/BigTable the key/value pairs are kept in strict alphabetical order.</p><p>Because these systems tend to be so huge and distributed, this sorting feature is actually very important. The spacial propinquity of rows with like keys ensures that when you must scan the table, the items of greatest interest to you are near each other.</p><p>This is important when choosing a row key convention. For example, consider a table whose keys are domain names. It makes the most sense to list them in reverse notation (so <strong>“com.jimbojw.www”</strong> rather than <strong>“<a href="http://www.jimbojw.com&quot;" target="_blank" rel="noopener">www.jimbojw.com&quot;</a></strong>) so that rows about a subdomain will be near the parent domain row.</p><p>Continuing the domain example, the row for the domain <strong>“mail.jimbojw.com”</strong> would be right next to the row for <strong>“<a href="http://www.jimbojw.com&quot;" target="_blank" rel="noopener">www.jimbojw.com&quot;</a></strong> rather than say <strong>“mail.xyz.com”</strong> which would happen if the keys were regular domain notation.</p><p>It’s important to note that the term “sorted” when applied to HBase/BigTable does not mean that “values” are sorted. There is no automatic indexing of anything other than the keys, just as it would be in a plain-old map implementation.</p><h3 id="Persistent"><a href="#Persistent" class="headerlink" title="Persistent"></a>Persistent</h3><p>Persistence merely means that the data you put in this special map “persists” after the program that created or accessed it is finished. This is no different in concept than any other kind of persistent storage such as a file on a filesystem.</p><h3 id="Sparse"><a href="#Sparse" class="headerlink" title="Sparse"></a>Sparse</h3><p>As already mentioned, a given row can have any number of columns in each column family, or none at all. The other type of sparseness is row-based gaps, which merely means that there may be gaps between keys.</p><p>This, of course, makes perfect sense if you’ve been thinking about HBase/BigTable in the map-based terms of this article rather than perceived similar concepts in RDBMS’s.</p><p>BigTable provides several ways of searching</p><ul><li>Given <code>row</code>, <code>column</code>, and <code>timestamp</code>, return the largest element that is less than <code>timestamp</code></li><li>Given <code>row</code> and <code>column</code>, return the element with largest <code>timestamp</code></li><li>Given <code>prefix</code> of <code>column</code>, return all elements with the same <code>prefix</code></li></ul><h2 id="Implementation-Details-LSM-Tree"><a href="#Implementation-Details-LSM-Tree" class="headerlink" title="Implementation Details - LSM Tree"></a>Implementation Details - LSM Tree</h2><p>Though there’s not much details in BigTable paper. Luckily, Google open-sourced <strong>LevelDB</strong>, a key-value store which is well recognized as the implementation of BigTable on a single node. Thus, <strong>LevelDB</strong> can be a pretty authentic source for us to learn implementation details of BigTable.</p><p>BigTable is composed of a <strong>client library</strong>, a <strong>Master Server</strong>, and lots of <strong>Tablet Servers</strong>. A concrete BigTable would be broken into tablets of size from 100MB to 200MB, and those tablets will be distributed to some <strong>Tablet Servers</strong> to <strong>server client</strong> requests.</p><p>When system runs, the number of <strong>Tablet Servers</strong> is not fixed. The number will scale up and down according to the actual workload, and is controlled by the <strong>Master Server</strong>. <strong>Tablet Server</strong> doesn’t store any actual files - they act as a service and proxy to visit actual files in <strong>Google File System</strong> (See the foundamental impact of GFS/HDFS?).</p><p>Unlike <strong>Tablet Server</strong>, <strong>Master Server</strong> always exists. <strong>Master Server</strong> stores the metadata of which tablet is distributed to which <strong>Tablet Server</strong>, whether scale up or down <strong>Tablet Servers</strong>, and how to load balance among all <strong>Tablet Servers</strong>. Adding new <strong>Tables</strong> or modifying existing <strong>Tables</strong>, for instance adding a new <strong>column family</strong>, is all handled by <strong>Master Server</strong> through <strong>Tablet Server</strong>. While <strong>Master Server</strong> is not responsible for managing any Tablet.</p><p>Well, unlike what most people imagne, clients doesn’t need to talk to <strong>Master Server</strong> when it tries to visit a BigTable. This design greatly mitigates the load of <strong>Master Server</strong>. So, how does client visit BigTable? It uses Chubby!</p><p><strong>Chubby</strong> is a highly-available distributed lock service. The open-source ZooKeeper project is a copycat of it. <strong>Chubby</strong> implements a file-system-like structure. Clients can visit those files to get lock of the visited object. According to the BigTable paper, <strong>Chubby</strong> is used by BigTable clients to locate <strong>Tablet</strong> and by <strong>Master Server</strong> to monitor <strong>Tablet Servers</strong>.</p><h3 id="How-do-clients-manipulate-BigTable-data"><a href="#How-do-clients-manipulate-BigTable-data" class="headerlink" title="How do clients manipulate BigTable data?"></a>How do clients manipulate BigTable data?</h3><p>The most important for us to know is how clients manipulate BigTable data?</p><p>The first step is to visit metadata in three layers. First layer is a <strong>Chubby file</strong>. Through that <strong>Chubby file</strong>, clients can locate <strong>Root Tablet</strong>, which is the second layer. What’s special about <strong>Root Tablet</strong> is that is’s never partitioned. The last layer is all <strong>Metadata Tablets</strong> hold by <strong>Root Tablet</strong>. Thus, generally speaking, clients can locate data by visiting a <strong>Chubby file</strong>, a <strong>Root Tablet</strong>, and a <strong>Metadata Tablet</strong>.</p><p>Clients will cache the metadata it has found for future reference, but caching is not required. Besides, because of <strong>Chubby</strong>, <strong>Master Server</strong> doesn’t take any responsibilities in data locating, thus it’s workload is quite small.</p><p>The paper doesn’t talk about what’s the format of BigTable’s metadata. The most detailed description is this - “The METADATA table stores the location of a tablet under a row key that is an encoding of the tablet’s table identifier and its end row.” Well, it’s actually not important. Engineers can always come up with their own formats.</p><h3 id="LSM-Tree"><a href="#LSM-Tree" class="headerlink" title="LSM-Tree"></a>LSM-Tree</h3><p>In BigTable, <strong>SSTable (Sorted Strings Table)</strong> is a basic unit. Each <strong>Tablet</strong> has several <strong>SSTables</strong>. Paper doesn’t reveal how SSTable is implemented. But, we can deduce that based on our understanding of the open-sourced LevelDB. The implementation brings back to life a data structure called LSM-Tree developed by Patrick O’Neil, a retired professor from UMass Boston. Here’s the paper <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.44.2782&amp;rep=rep1&amp;type=ps" target="_blank" rel="noopener">The Log-Structured Merge-Tree (LSM-Tree)</a>.</p><p><strong>SSTable</strong> in LevelDB is composed of 1) <strong>memTable</strong> and 2) <strong>SSTable</strong> on disk. In memory, it uses <a href="https://en.wikipedia.org/wiki/Skip_list" target="_blank" rel="noopener">skip list</a>. Thus, write is only towards memory, and is very very fast. When memory is full, a <strong>memTable</strong> is turned into an <strong>immutable memTable</strong>. Then LevelDB opens a new writable <strong>memTable</strong>, and starts a new process to write the <strong>immutable memTable</strong> to disk as a <strong>SSTable</strong>. When this step finishes, space of the <strong>immutable memTable</strong> in memory is released. Over and over, disk accumulates lots of <strong>SSTables</strong>, and here comes the compaction. <strong>SSTables</strong> have different levels, like level1, level2, etc. <strong>SSTables</strong> that are of level1 will go through compaction called <strong>minor compact</strong>. There’s major compact in further levels. From level2, no more two <strong>SSTable</strong> will have overlapped keys in their trees, which is not guaranteed in level1.</p><p>Therefore, a read request needs to access memTable, immutable memTable, tree in level1, and one tree for any further level after and including level2. This indicates that a read transaction is more expensive than a write transaction. </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">    memTable</span><br><span class="line">        |</span><br><span class="line">immutable memTable</span><br><span class="line">        |</span><br><span class="line">    tree in level1</span><br><span class="line">    / |  |  | \</span><br><span class="line">      |                    trees in level2</span><br><span class="line">    tree in level2</span><br><span class="line">    / |  |  | \</span><br><span class="line">         |</span><br><span class="line">        tree in level3</span><br><span class="line">        / | | | \</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure><p>Well, another noticeable thing is that, if clients try to access latest data, that data might be in memory. Thus, LevelDB’s design optimizes read for data of new version by sacrificing read for cold data.</p><p>BTW, one of biggest differences between LevelDB and RocksDB, a Facebook’s copycat of LevelDB, is that RocksDB introduced something called <strong>universal compact</strong>. I’m not familiar with what it is.</p><p>Of course, like lots of other similar system, BigTable’s recovery strategy is based on logs. All write transactions are persisted in logs before going into memory. LevelDB’s log format is of the classic <strong>append only</strong> strategy.</p><p>What’s funny yet sad is that LSM-Tree is such a gorgeous invention but its author hasn’t earned the matching reputation.</p><hr><p>References:</p><ul><li><p><a href="http://jimbojw.com/wiki/index.php?title=Understanding_Hbase_and_BigTable" target="_blank" rel="noopener">http://jimbojw.com/wiki/index.php?title=Understanding_Hbase_and_BigTable</a></p></li><li><p><a href="http://rocksdb.org/" target="_blank" rel="noopener">http://rocksdb.org/</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The following ideas are from a ‘blog’ of one of my friends. He posted this ‘blog’ on Wechat, and Wechat doesn’t provide url or allow external users to access it. So I can’t referenece a link here.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;BigTable is most diffult to understand paper because it 1) names lots of its terminologies quite misleadingly 2) barely illustrated any detail.&lt;/p&gt;
&lt;h2 id=&quot;1-Misleading-Names-Let’s-rephrase-and-better-understand-them&quot;&gt;&lt;a href=&quot;#1-Misleading-Names-Let’s-rephrase-and-better-understand-them&quot; class=&quot;headerlink&quot; title=&quot;1. Misleading Names - Let’s rephrase and better understand them&quot;&gt;&lt;/a&gt;1. Misleading Names - Let’s rephrase and better understand them&lt;/h2&gt;&lt;p&gt;I’ve been confused of BigTable, until one day I read a blog called &lt;a href=&quot;http://jimbojw.com/wiki/index.php?title=Understanding_Hbase_and_BigTable&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Understanding HBase and BigTable&lt;/a&gt;. I strongly recommending everyone find and read that blog.&lt;/p&gt;
&lt;p&gt;Here is the first sentence of the “Data Model” section:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; A Bigtable is a sparse, distributed, persistent multidimensional sorted map.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;So, BigTable is a &lt;strong&gt;Map&lt;/strong&gt;, not a &lt;strong&gt;Table&lt;/strong&gt;! Right, you’ve been misleaded to associate BigTable with a database table because of namings. What is a map? A map is key-value data structure! Thus, BigTable is a key-value store. Probably Google should better name it &lt;strong&gt;BigMap&lt;/strong&gt; instead of &lt;strong&gt;BigTable&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;The BigTable paper continues, explaining that:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; The map is indexed by a row key, column key, and a timestamp; each value in the map is an uninterpreted array of bytes.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;So how’s the map look like? The Map’s value is an array of bytes. Fairly simple. Map’s key as &lt;code&gt;&amp;lt;key1, key2, key3&amp;gt;&lt;/code&gt; is a combination of &lt;code&gt;&amp;lt;row, column, timestamp&amp;gt;&lt;/code&gt;, &lt;code&gt;row&lt;/code&gt; and &lt;code&gt;column&lt;/code&gt; as both string, &lt;code&gt;timestamp&lt;/code&gt; as a 64 bit number.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;column&lt;/code&gt; key is more complex, it’s formatted as &lt;code&gt;column&lt;/code&gt; = &lt;code&gt;prefix:suffix&lt;/code&gt;. &lt;code&gt;prefix&lt;/code&gt; is called &lt;strong&gt;column family&lt;/strong&gt;, which is of limited number in each BigMap. &lt;code&gt;suffix&lt;/code&gt; can be of unlimited number.&lt;/p&gt;
&lt;p&gt;Till now, you can tell that &lt;strong&gt;BigTable&lt;/strong&gt; has nothing to do with a regular table in transactional database. It’s key-value data model.&lt;/p&gt;
    
    </summary>
    
      <category term="distributed syste" scheme="https://bowenli86.github.io/categories/distributed-syste/"/>
    
      <category term="data" scheme="https://bowenli86.github.io/categories/distributed-syste/data/"/>
    
    
      <category term="bigtable" scheme="https://bowenli86.github.io/tags/bigtable/"/>
    
      <category term="leveldb" scheme="https://bowenli86.github.io/tags/leveldb/"/>
    
      <category term="rocksdb" scheme="https://bowenli86.github.io/tags/rocksdb/"/>
    
      <category term="lsm-tree" scheme="https://bowenli86.github.io/tags/lsm-tree/"/>
    
  </entry>
  
  <entry>
    <title>CAP and Evetual Consistency</title>
    <link href="https://bowenli86.github.io/2016/11/16/distributed%20system/CAP-and-Evetual-Consistency/"/>
    <id>https://bowenli86.github.io/2016/11/16/distributed system/CAP-and-Evetual-Consistency/</id>
    <published>2016-11-17T05:04:01.000Z</published>
    <updated>2018-04-10T04:29:25.519Z</updated>
    
    <content type="html"><![CDATA[<p>This is a post from AWS CTO Werner Vogels at 2008, talking about the CAP theory and eventual consistency. I think it’s a really good reading for distributed system engineers</p><hr><h2 id="Eventually-Consistent-Building-reliable-distributed-systems-at-a-worldwide-scale-demands-trade-offs-between-consistency-and-availability"><a href="#Eventually-Consistent-Building-reliable-distributed-systems-at-a-worldwide-scale-demands-trade-offs-between-consistency-and-availability" class="headerlink" title="Eventually Consistent - Building reliable distributed systems at a worldwide scale demands trade-offs between consistency and availability."></a>Eventually Consistent - Building reliable distributed systems at a worldwide scale demands trade-offs between consistency and availability.</h2><p>At the foundation of Amazon’s cloud computing are infrastructure services such as Amazon’s S3 (Simple Storage Service), SimpleDB, and EC2 (Elastic Compute Cloud) that provide the resources for constructing Internet-scale computing platforms and a great variety of applications. The requirements placed on these infrastructure services are very strict; they need to score high marks in the areas of security, scalability, availability, performance, and cost effectiveness, and they need to meet these requirements while serving millions of customers around the globe, continuously.</p><p>Under the covers these services are massive distributed systems that operate on a worldwide scale. <strong>This scale creates additional challenges, because when a system processes trillions and trillions of requests, events that normally have a low probability of occurrence are now guaranteed to happen and need to be accounted for up front in the design and architecture of the system</strong>. <strong>Given the worldwide scope of these systems, we use replication techniques ubiquitously to guarantee consistent performance and high availability</strong>. Although replication brings us closer to our goals, it cannot achieve them in a perfectly transparent manner; under a number of conditions the customers of these services will be confronted with the consequences of using replication techniques inside the services.</p><p>One of the ways in which this manifests itself is in the type of <strong>data consistency</strong> that is provided, particularly when the underlying distributed system provides an eventual consistency model for data replication. When designing these large-scale systems at Amazon, we use a set of guiding principles and abstractions related to large-scale data replication and focus on the trade-offs between <strong>high availability</strong> and <strong>data consistency</strong>. In this article I present some of the relevant background that has informed our approach to delivering reliable distributed systems that need to operate on a global scale. An earlier version of this text appeared as a posting on the <em>All Things Distributed weblog</em> in December 2007 and was greatly improved with the help of its readers.</p><h2 id="Historical-Perspective"><a href="#Historical-Perspective" class="headerlink" title="Historical Perspective"></a>Historical Perspective</h2><p>In an ideal world there would be only one consistency model: <strong>when an update is made all observers would see that update</strong>. The first time this surfaced as difficult to achieve was in the database systems of the late ‘70s. The best “period piece” on this topic is “Notes on Distributed Databases” by Bruce Lindsay et al. 5 It lays out the fundamental principles for database replication and discusses a number of techniques that deal with achieving consistency. Many of these techniques try to achieve distribution transparency—that is, to the user of the system it appears as if there is only one system instead of a number of collaborating systems. Many systems during this time took the approach that it was better to fail the complete system than to break this transparency.</p><p>In the mid-‘90s, with the rise of larger Internet systems, these practices were revisited. At that time people began to consider the idea that availability was perhaps the most important property of these systems, but they were struggling with what it should be traded off against. Eric Brewer, systems professor at the University of California, Berkeley, and at that time head of Inktomi, brought the different trade-offs together in a keynote address to the PODC (Principles of Distributed Computing) conference in 2000.1 He presented the <strong>CAP theorem</strong>, which states that of three properties of <strong>shared-data systems—data consistency</strong>, <strong>system availability</strong>, and <strong>tolerance to network partition</strong>—only two can be achieved at any given time. A more formal confirmation can be found in a 2002 paper by Seth Gilbert and Nancy Lynch.</p><p>A system that is not tolerant to network partitions can achieve data consistency and availability, and often does so by using transaction protocols. To make this work, client and storage systems must be part of the same environment; they fail as a whole under certain scenarios, and as such, clients cannot observe partitions. <strong>An important observation is that in larger distributed-scale systems, network partitions are a given; therefore, consistency and availability cannot be achieved at the same time.</strong> This means that there are two choices on what to drop: <strong>relaxing consistency will allow the system to remain highly available under the partitionable conditions</strong>, whereas <strong>making consistency a priority means that under certain conditions the system will not be available.</strong></p><p>Both options require the client developer to be aware of what the system is offering. If the system emphasizes consistency, the developer has to deal with the fact that the system may not be available to take, for example, a write. If this write fails because of system unavailability, then the developer will have to deal with what to do with the data to be written. If the system emphasizes availability, it may always accept the write, but under certain conditions a read will not reflect the result of a recently completed write. The developer then has to decide whether the client requires access to the absolute latest update all the time. There is a range of applications that can handle slightly stale data, and they are served well under this model.</p><p>In principle the consistency property of transaction systems as defined in the <strong>ACID properties (atomicity, consistency, isolation, durability)</strong> is a different kind of consistency guarantee. In ACID, consistency relates to the guarantee that when a transaction is finished the database is in a consistent state; for example, when transferring money from one account to another the total amount held in both accounts should not change. In ACID-based systems, this kind of consistency is often the responsibility of the developer writing the transaction but can be assisted by the database managing integrity constraints.</p><a id="more"></a><h2 id="Consistency—Client-and-Server"><a href="#Consistency—Client-and-Server" class="headerlink" title="Consistency—Client and Server"></a>Consistency—Client and Server</h2><p>There are two ways of looking at consistency. One is from the developer/client point of view: <strong>how they observe data updates</strong>. The second way is from the server side: <strong>how updates flow through the system and what guarantees systems can give with respect to updates</strong>.</p><h2 id="Client-side-Consistency"><a href="#Client-side-Consistency" class="headerlink" title="Client-side Consistency"></a>Client-side Consistency</h2><p>The client side has these components:</p><ul><li><p><strong>A storage system</strong>. For the moment we’ll treat it as a black box, but one should assume that under the covers it is something of large scale and highly distributed, and that it is built to guarantee durability and availability.</p></li><li><p><strong>Process A</strong>. This is a process that writes to and reads from the storage system.</p></li><li><p><strong>Processes B and C</strong>. These two processes are independent of process A and write to and read from the storage system. It is irrelevant whether these are really processes or threads within the same process; what is important is that they are independent and need to communicate to share information.</p></li></ul><p>Client-side consistency has to do with how and when observers (in this case the processes A, B, or C) see updates made to a data object in the storage systems. In the following examples illustrating the different types of consistency, process A has made an update to a data object:</p><ul><li><p><strong>Strong consistency</strong>. After the update completes, any subsequent access (by A, B, or C) will return the updated value.</p></li><li><p><strong>Weak consistency</strong>. The system does not guarantee that subsequent accesses will return the updated value. A number of conditions need to be met before the value will be returned. The period between the update and the moment when it is guaranteed that any observer will always see the updated value is dubbed the inconsistency window.</p></li><li><p><strong>Eventual consistency</strong>. This is a specific form of weak consistency; the storage system guarantees that if no new updates are made to the object, eventually all accesses will return the last updated value. If no failures occur, the maximum size of the inconsistency window can be determined based on factors such as communication delays, the load on the system, and the number of replicas involved in the replication scheme. The most popular system that implements eventual consistency is <strong>DNS (Domain Name System)</strong>. Updates to a name are distributed according to a configured pattern and in combination with time-controlled caches; eventually, all clients will see the update.</p></li></ul><p>The eventual consistency model has a number of variations that are important to consider:</p><ul><li><p><strong>Causal consistency</strong>. If process A has communicated to process B that it has updated a data item, a subsequent access by process B will return the updated value, and a write is guaranteed to supersede the earlier write. Access by process C that has no causal relationship to process A is subject to the normal eventual consistency rules.</p></li><li><p><strong>Read-your-writes consistency</strong>. This is an important model where process A, after it has updated a data item, always accesses the updated value and will never see an older value. This is a special case of the causal consistency model.</p></li><li><p><strong>Session consistency</strong>. This is a practical version of the previous model, where a process accesses the storage system in the context of a session. As long as the session exists, the system guarantees read-your-writes consistency. If the session terminates because of a certain failure scenario, a new session needs to be created and the guarantees do not overlap the sessions.</p></li><li><p><strong>Monotonic read consistency</strong>. If a process has seen a particular value for the object, any subsequent accesses will never return any previous values.</p></li><li><p><strong>Monotonic write consistency</strong>. In this case the system guarantees to serialize the writes by the same process. Systems that do not guarantee this level of consistency are notoriously hard to program.</p></li></ul><p>A number of these properties can be combined. For example, one can get monotonic reads combined with session-level consistency. From a practical point of view these two properties (monotonic reads and read-your-writes) are most desirable in an eventual consistency system, but not always required. These two properties make it simpler for developers to build applications, while allowing the storage system to relax consistency and provide high availability.</p><p>As you can see from these variations, quite a few different scenarios are possible. It depends on the particular applications whether or not one can deal with the consequences.</p><p>Eventual consistency is not some esoteric property of extreme distributed systems. Many modern RDBMSs (relational database management systems) that provide primary-backup reliability implement their replication techniques in both synchronous and asynchronous modes. In synchronous mode the replica update is part of the transaction. In asynchronous mode the updates arrive at the backup in a delayed manner, often through log shipping. In the latter mode if the primary fails before the logs are shipped, reading from the promoted backup will produce old, inconsistent values. Also to support better scalable read performance, RDBMSs have started to provide the ability to read from the backup, which is a classical case of providing eventual consistency guarantees in which the inconsistency windows depend on the periodicity of the log shipping.</p><h2 id="Server-side-Consistency"><a href="#Server-side-Consistency" class="headerlink" title="Server-side Consistency"></a>Server-side Consistency</h2><p>On the server side we need to take a deeper look at how updates flow through the system to understand what drives the different modes that the developer who uses the system can experience. Let’s establish a few definitions before getting started:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">N = the number of nodes that store replicas of the data</span><br><span class="line"></span><br><span class="line">W = the number of replicas that need to acknowledge the receipt of the update before the update completes</span><br><span class="line"></span><br><span class="line">R = the number of replicas that are contacted when a data object is accessed through a read operation</span><br></pre></td></tr></table></figure><p>If <strong>W+R &gt; N</strong>, then the write set and the read set always overlap and one can guarantee strong consistency. In the primary-backup RDBMS scenario, which implements synchronous replication, <strong>N=2, W=2, and R=1</strong>. No matter from which replica the client reads, it will always get a consistent answer. In asynchronous replication with reading from the backup enabled, <strong>N=2, W=1, and R=1</strong>. In this case R+W=N, and consistency cannot be guaranteed.</p><p>The problems with these configurations, which are basic quorum protocols, is that when the system cannot write to W nodes because of failures, the write operation has to fail, marking the unavailability of the system. With N=3 and W=3 and only two nodes available, the system will have to fail the write.</p><p>In distributed-storage systems that need to provide high performance and high availability, the number of replicas is in general higher than two. Systems that focus solely on fault tolerance often use N=3 (with W=2 and R=2 configurations). Systems that need to serve very high read loads often replicate their data beyond what is required for fault tolerance; N can be tens or even hundreds of nodes, with R configured to 1 such that a single read will return a result. Systems that are concerned with consistency are set to W=N for updates, which may decrease the probability of the write succeeding. A common configuration for these systems that are concerned about fault tolerance but not consistency is to run with W=1 to get minimal durability of the update and then rely on a lazy (epidemic) technique to update the other replicas.</p><p>How to configure N, W, and R depends on what the common case is and which performance path needs to be optimized. In R=1 and N=W we optimize for the read case, and in W=1 and R=N we optimize for a very fast write. Of course in the latter case, durability is not guaranteed in the presence of failures, and if <strong>W &lt; (N+1)/2</strong>, there is the possibility of conflicting writes when the write sets do not overlap.</p><p><strong>Weak/eventual consistency arises when <code>W+R &lt;= N</code>, meaning that there is a possibility that the read and write set will not overlap.</strong> If this is a deliberate configuration and not based on a failure case, then it hardly makes sense to set R to anything but 1. This happens in two very common cases: the first is the massive replication for read scaling mentioned earlier; the second is where data access is more complicated. In a simple key-value model it is easy to compare versions to determine the latest value written to the system, but in systems that return sets of objects it is more difficult to determine what the correct latest set should be. In most of these systems where the write set is smaller than the replica set, a mechanism is in place that applies the updates in a lazy manner to the remaining nodes in the replica’s set. The period until all replicas have been updated is the inconsistency window discussed before. If <strong>W+R &lt;= N</strong>, then the system is vulnerable to reading from nodes that have not yet received the updates.</p><p>Whether or not read-your-writes, session, and monotonic consistency can be achieved depends in general on the “stickiness” of clients to the server that executes the distributed protocol for them. If this is the same server every time, then it is relatively easy to guarantee read-your-writes and monotonic reads. This makes it slightly harder to manage load balancing and fault tolerance, but it is a simple solution. Using sessions, which are sticky, makes this explicit and provides an exposure level that clients can reason about.</p><p>Sometimes the client implements read-your-writes and monotonic reads. By adding versions on writes, the client discards reads of values with versions that precede the last-seen version.</p><p>Partitions happen when some nodes in the system cannot reach other nodes, but both sets are reachable by groups of clients. If you use a classical majority quorum approach, then the partition that has W nodes of the replica set can continue to take updates while the other partition becomes unavailable. The same is true for the read set. Given that these two sets overlap, by definition the minority set becomes unavailable. Partitions don’t happen frequently, but they do occur between data centers, as well as inside data centers.</p><p>In some applications the unavailability of any of the partitions is unacceptable, and it is important that the clients that can reach that partition make progress. In that case both sides assign a new set of storage nodes to receive the data, and a merge operation is executed when the partition heals. For example, within Amazon the shopping cart uses such a write-always system; in the case of partition, a customer can continue to put items in the cart even if the original cart lives on the other partitions. The cart application assists the storage system with merging the carts once the partition has healed.</p><h2 id="Amazon’s-Dynamo"><a href="#Amazon’s-Dynamo" class="headerlink" title="Amazon’s Dynamo"></a>Amazon’s Dynamo</h2><p>A system that has brought all of these properties under explicit control of the application architecture is Amazon’s Dynamo, a key-value storage system that is used internally in many services that make up the Amazon e-commerce platform, as well as Amazon’s Web Services. One of the design goals of Dynamo is to allow the application service owner who creates an instance of the Dynamo storage system—which commonly spans multiple data centers—to make the trade-offs between consistency, durability, availability, and performance at a certain cost point.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Data inconsistency in large-scale reliable distributed systems has to be tolerated for two reasons: improving read and write performance under highly concurrent conditions; and handling partition cases where a majority model would render part of the system unavailable even though the nodes are up and running.</p><p>Whether or not inconsistencies are acceptable depends on the client application. In all cases the developer needs to be aware that consistency guarantees are provided by the storage systems and need to be taken into account when developing applications. There are a number of practical improvements to the eventual consistency model, such as session-level consistency and monotonic reads, which provide better tools for the developer. Many times the application is capable of handling the eventual consistency guarantees of the storage system without any problem. A specific popular case is a Web site in which we can have the notion of user-perceived consistency. In this scenario the inconsistency window needs to be smaller than the time expected for the customer to return for the next page load. This allows for updates to propagate through the system before the next read is expected.</p><p>The goal of this article is to raise awareness about the complexity of engineering systems that need to operate at a global scale and that require careful tuning to ensure that they can deliver the durability, availability, and performance that their applications require. One of the tools the system designer has is the length of the consistency window, during which the clients of the systems are possibly exposed to the realities of large-scale systems engineering.</p><hr><p>References</p><ul><li><a href="http://www.allthingsdistributed.com/2008/12/eventually_consistent.html" target="_blank" rel="noopener">http://www.allthingsdistributed.com/2008/12/eventually_consistent.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This is a post from AWS CTO Werner Vogels at 2008, talking about the CAP theory and eventual consistency. I think it’s a really good reading for distributed system engineers&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;Eventually-Consistent-Building-reliable-distributed-systems-at-a-worldwide-scale-demands-trade-offs-between-consistency-and-availability&quot;&gt;&lt;a href=&quot;#Eventually-Consistent-Building-reliable-distributed-systems-at-a-worldwide-scale-demands-trade-offs-between-consistency-and-availability&quot; class=&quot;headerlink&quot; title=&quot;Eventually Consistent - Building reliable distributed systems at a worldwide scale demands trade-offs between consistency and availability.&quot;&gt;&lt;/a&gt;Eventually Consistent - Building reliable distributed systems at a worldwide scale demands trade-offs between consistency and availability.&lt;/h2&gt;&lt;p&gt;At the foundation of Amazon’s cloud computing are infrastructure services such as Amazon’s S3 (Simple Storage Service), SimpleDB, and EC2 (Elastic Compute Cloud) that provide the resources for constructing Internet-scale computing platforms and a great variety of applications. The requirements placed on these infrastructure services are very strict; they need to score high marks in the areas of security, scalability, availability, performance, and cost effectiveness, and they need to meet these requirements while serving millions of customers around the globe, continuously.&lt;/p&gt;
&lt;p&gt;Under the covers these services are massive distributed systems that operate on a worldwide scale. &lt;strong&gt;This scale creates additional challenges, because when a system processes trillions and trillions of requests, events that normally have a low probability of occurrence are now guaranteed to happen and need to be accounted for up front in the design and architecture of the system&lt;/strong&gt;. &lt;strong&gt;Given the worldwide scope of these systems, we use replication techniques ubiquitously to guarantee consistent performance and high availability&lt;/strong&gt;. Although replication brings us closer to our goals, it cannot achieve them in a perfectly transparent manner; under a number of conditions the customers of these services will be confronted with the consequences of using replication techniques inside the services.&lt;/p&gt;
&lt;p&gt;One of the ways in which this manifests itself is in the type of &lt;strong&gt;data consistency&lt;/strong&gt; that is provided, particularly when the underlying distributed system provides an eventual consistency model for data replication. When designing these large-scale systems at Amazon, we use a set of guiding principles and abstractions related to large-scale data replication and focus on the trade-offs between &lt;strong&gt;high availability&lt;/strong&gt; and &lt;strong&gt;data consistency&lt;/strong&gt;. In this article I present some of the relevant background that has informed our approach to delivering reliable distributed systems that need to operate on a global scale. An earlier version of this text appeared as a posting on the &lt;em&gt;All Things Distributed weblog&lt;/em&gt; in December 2007 and was greatly improved with the help of its readers.&lt;/p&gt;
&lt;h2 id=&quot;Historical-Perspective&quot;&gt;&lt;a href=&quot;#Historical-Perspective&quot; class=&quot;headerlink&quot; title=&quot;Historical Perspective&quot;&gt;&lt;/a&gt;Historical Perspective&lt;/h2&gt;&lt;p&gt;In an ideal world there would be only one consistency model: &lt;strong&gt;when an update is made all observers would see that update&lt;/strong&gt;. The first time this surfaced as difficult to achieve was in the database systems of the late ‘70s. The best “period piece” on this topic is “Notes on Distributed Databases” by Bruce Lindsay et al. 5 It lays out the fundamental principles for database replication and discusses a number of techniques that deal with achieving consistency. Many of these techniques try to achieve distribution transparency—that is, to the user of the system it appears as if there is only one system instead of a number of collaborating systems. Many systems during this time took the approach that it was better to fail the complete system than to break this transparency.&lt;/p&gt;
&lt;p&gt;In the mid-‘90s, with the rise of larger Internet systems, these practices were revisited. At that time people began to consider the idea that availability was perhaps the most important property of these systems, but they were struggling with what it should be traded off against. Eric Brewer, systems professor at the University of California, Berkeley, and at that time head of Inktomi, brought the different trade-offs together in a keynote address to the PODC (Principles of Distributed Computing) conference in 2000.1 He presented the &lt;strong&gt;CAP theorem&lt;/strong&gt;, which states that of three properties of &lt;strong&gt;shared-data systems—data consistency&lt;/strong&gt;, &lt;strong&gt;system availability&lt;/strong&gt;, and &lt;strong&gt;tolerance to network partition&lt;/strong&gt;—only two can be achieved at any given time. A more formal confirmation can be found in a 2002 paper by Seth Gilbert and Nancy Lynch.&lt;/p&gt;
&lt;p&gt;A system that is not tolerant to network partitions can achieve data consistency and availability, and often does so by using transaction protocols. To make this work, client and storage systems must be part of the same environment; they fail as a whole under certain scenarios, and as such, clients cannot observe partitions. &lt;strong&gt;An important observation is that in larger distributed-scale systems, network partitions are a given; therefore, consistency and availability cannot be achieved at the same time.&lt;/strong&gt; This means that there are two choices on what to drop: &lt;strong&gt;relaxing consistency will allow the system to remain highly available under the partitionable conditions&lt;/strong&gt;, whereas &lt;strong&gt;making consistency a priority means that under certain conditions the system will not be available.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Both options require the client developer to be aware of what the system is offering. If the system emphasizes consistency, the developer has to deal with the fact that the system may not be available to take, for example, a write. If this write fails because of system unavailability, then the developer will have to deal with what to do with the data to be written. If the system emphasizes availability, it may always accept the write, but under certain conditions a read will not reflect the result of a recently completed write. The developer then has to decide whether the client requires access to the absolute latest update all the time. There is a range of applications that can handle slightly stale data, and they are served well under this model.&lt;/p&gt;
&lt;p&gt;In principle the consistency property of transaction systems as defined in the &lt;strong&gt;ACID properties (atomicity, consistency, isolation, durability)&lt;/strong&gt; is a different kind of consistency guarantee. In ACID, consistency relates to the guarantee that when a transaction is finished the database is in a consistent state; for example, when transferring money from one account to another the total amount held in both accounts should not change. In ACID-based systems, this kind of consistency is often the responsibility of the developer writing the transaction but can be assisted by the database managing integrity constraints.&lt;/p&gt;
    
    </summary>
    
      <category term="distributed system" scheme="https://bowenli86.github.io/categories/distributed-system/"/>
    
    
      <category term="cap" scheme="https://bowenli86.github.io/tags/cap/"/>
    
      <category term="cap theory" scheme="https://bowenli86.github.io/tags/cap-theory/"/>
    
      <category term="eventual consistency" scheme="https://bowenli86.github.io/tags/eventual-consistency/"/>
    
  </entry>
  
  <entry>
    <title>Circular Buffer</title>
    <link href="https://bowenli86.github.io/2016/10/26/data%20structures%20and%20algorithms/Circular-Buffer/"/>
    <id>https://bowenli86.github.io/2016/10/26/data structures and algorithms/Circular-Buffer/</id>
    <published>2016-10-27T03:24:27.000Z</published>
    <updated>2018-04-10T04:29:25.517Z</updated>
    
    <content type="html"><![CDATA[<p>When preparing for interviews, I was inspired by this <a href="http://tutorials.jenkov.com/java-performance/ring-buffer.html" target="_blank" rel="noopener">post</a> and wrote my own version of circular buffer.</p><p>Here’s some essense of circular buffer this data structure.</p><blockquote><p>How many elements are in the buffer is calculated based on the write and read position.</p></blockquote><blockquote><p>How the calculation looks depends on whether the write position has flipped (wrapped around) or not.</p></blockquote><blockquote><p>If the write position has not wrapped around you can simply subtract the read position from the write position to know how many elements are in the buffer. If the write position has wrapped around (flipped) then the available space is equal to the capacity minus the read position plus the write position.</p></blockquote><blockquote><p>To keep track of whether the write position has flipped or not a special “flip marker” is used. That is where the name of the implementation comes from. Actually, in most cases you could just check if the write position is larger or smaller than the read position to detect if the write position has wrapped around. But, that doesn’t work when write position and read position are equal (the ring buffer is either completely full or completely empty).</p></blockquote><h2 id="Circular-Buffer"><a href="#Circular-Buffer" class="headerlink" title="Circular Buffer"></a>Circular Buffer</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CircularBuffer</span> </span>&#123;</span><br><span class="line">    Integer[] elements;</span><br><span class="line">    <span class="keyword">int</span> capacity;</span><br><span class="line">    <span class="keyword">int</span> writePtr = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> readPtr = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">CircularBuffer</span><span class="params">(<span class="keyword">int</span> capacity)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.capacity = capacity;</span><br><span class="line">        elements = <span class="keyword">new</span> Integer[capacity];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">put</span><span class="params">(Integer num)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// If writePtr is pointing at a filled position, meaning all positions having been taken</span></span><br><span class="line">        <span class="keyword">if</span> (elements[writePtr] != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException();</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// Put the element at the pointed position</span></span><br><span class="line">        elements[writePtr] = num;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Move the pointer</span></span><br><span class="line">        <span class="keyword">if</span> (writePtr == capacity - <span class="number">1</span>) &#123;</span><br><span class="line">            writePtr = <span class="number">0</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            writePtr ++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> Integer <span class="title">take</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// If readPtr is pointing at a null position, meaning no element is in this circular buffer</span></span><br><span class="line">        <span class="keyword">if</span> (elements[readPtr] == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Record the element, and remove it</span></span><br><span class="line">        Integer result = elements[readPtr];</span><br><span class="line">        elements[readPtr] = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Move the pointer</span></span><br><span class="line">        <span class="keyword">if</span>(readPtr == capacity - <span class="number">1</span>) &#123;</span><br><span class="line">            readPtr = <span class="number">0</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            readPtr ++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Return element</span></span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">int</span> <span class="title">occupiedSize</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// If writePtr is ahead of readPtr</span></span><br><span class="line">        <span class="keyword">if</span>(writePtr &gt; readPtr) &#123;</span><br><span class="line">            <span class="keyword">return</span> writePtr - readPtr;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span>(writePtr &lt; readPtr) &#123;</span><br><span class="line">            <span class="comment">// If readPtr is ahead of writePtr, meaning the writePtr has turned over</span></span><br><span class="line">            <span class="keyword">return</span> capacity - readPtr + writePtr;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// If writePtr == readPtr</span></span><br><span class="line">            <span class="comment">// If readPtr is pointing at an element</span></span><br><span class="line">            <span class="keyword">if</span>(elements[readPtr] != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> capacity;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// If readPtr is pointing no element</span></span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">int</span> <span class="title">remainingCapacity</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> capacity - occupiedSize();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">reset</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        writePtr = <span class="number">0</span>;</span><br><span class="line">        readPtr = <span class="number">0</span>;</span><br><span class="line">        elements = <span class="keyword">new</span> Integer[capacity];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        CircularBuffer cb = <span class="keyword">new</span> CircularBuffer(<span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        System.out.println(cb.occupiedSize());</span><br><span class="line"></span><br><span class="line">        cb.put(<span class="number">10</span>);</span><br><span class="line">        System.out.println(cb.occupiedSize());</span><br><span class="line"></span><br><span class="line">        cb.put(<span class="number">20</span>);</span><br><span class="line">        System.out.println(cb.occupiedSize());</span><br><span class="line"></span><br><span class="line">        System.out.println(cb.take2());</span><br><span class="line">        System.out.println(cb.occupiedSize());</span><br><span class="line"></span><br><span class="line">        cb.put(<span class="number">30</span>);</span><br><span class="line">        System.out.println(cb.occupiedSize());</span><br><span class="line"></span><br><span class="line">        cb.put(<span class="number">40</span>);</span><br><span class="line">        System.out.println(cb.occupiedSize());</span><br><span class="line"></span><br><span class="line">        System.out.println(cb.take2());</span><br><span class="line">        System.out.println(cb.take2());</span><br><span class="line">        System.out.println(cb.take2());</span><br><span class="line">        System.out.println(cb.take2());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Advanced-Circular-Buffer"><a href="#Advanced-Circular-Buffer" class="headerlink" title="Advanced Circular Buffer"></a>Advanced Circular Buffer</h2><p>Write a circular buffer that can override elements when it’s full</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CircularBuffer2</span> </span>&#123;</span><br><span class="line">    Integer[] elements;</span><br><span class="line">    <span class="keyword">int</span> rPtr;</span><br><span class="line">    <span class="keyword">int</span> wPtr;</span><br><span class="line">    <span class="keyword">int</span> capacity;</span><br><span class="line"></span><br><span class="line">    CircularBuffer2(<span class="keyword">int</span> capacity) &#123;</span><br><span class="line">        <span class="keyword">this</span>.capacity = capacity;</span><br><span class="line">        elements = <span class="keyword">new</span> Integer[capacity];</span><br><span class="line"></span><br><span class="line">        rPtr = <span class="number">0</span>;</span><br><span class="line">        wPtr = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">put</span><span class="params">(Integer i)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// If there's alreay an element</span></span><br><span class="line">        <span class="keyword">if</span>(elements[wPtr] != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="comment">// Override it</span></span><br><span class="line">            elements[wPtr] = i;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Move rPtr and wPtr together</span></span><br><span class="line">            <span class="keyword">if</span>(wPtr == capacity - <span class="number">1</span>) &#123;</span><br><span class="line">                wPtr = <span class="number">0</span>;</span><br><span class="line">                rPtr = <span class="number">0</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                wPtr ++;</span><br><span class="line">                rPtr ++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// If buffer is not full</span></span><br><span class="line">            elements[wPtr] = i;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Only move wPtr</span></span><br><span class="line">            <span class="keyword">if</span>(wPtr == capacity - <span class="number">1</span>) &#123;</span><br><span class="line">                wPtr = <span class="number">0</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                wPtr ++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> Integer <span class="title">get</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// If there's no element</span></span><br><span class="line">        <span class="keyword">if</span>(elements[rPtr] == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Get the element</span></span><br><span class="line">        Integer result = elements[rPtr];</span><br><span class="line">        <span class="comment">// Release resource</span></span><br><span class="line">        elements[rPtr] = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Move rPtr</span></span><br><span class="line">        <span class="keyword">if</span>(rPtr == capacity - <span class="number">1</span>) &#123;</span><br><span class="line">            rPtr = <span class="number">0</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            rPtr++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        CircularBuffer2 cb = <span class="keyword">new</span> CircularBuffer2(<span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        cb.put(<span class="number">1</span>);</span><br><span class="line">        cb.put(<span class="number">2</span>);</span><br><span class="line">        cb.put(<span class="number">3</span>);</span><br><span class="line">        cb.put(<span class="number">4</span>);</span><br><span class="line">        System.out.println(cb.get());</span><br><span class="line">        System.out.println(cb.get());</span><br><span class="line">        System.out.println(cb.get());</span><br><span class="line"></span><br><span class="line">        cb = <span class="keyword">new</span> CircularBuffer2(<span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        cb.put(<span class="number">1</span>);</span><br><span class="line">        cb.put(<span class="number">2</span>);</span><br><span class="line">        cb.put(<span class="number">3</span>);</span><br><span class="line">        cb.put(<span class="number">4</span>);</span><br><span class="line">        cb.put(<span class="number">5</span>);</span><br><span class="line">        cb.put(<span class="number">6</span>);</span><br><span class="line">        System.out.println(cb.get());</span><br><span class="line">        System.out.println(cb.get());</span><br><span class="line">        System.out.println(cb.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;When preparing for interviews, I was inspired by this &lt;a href=&quot;http://tutorials.jenkov.com/java-performance/ring-buffer.html&quot; target=&quot;_bl
      
    
    </summary>
    
      <category term="data structures and algorithms" scheme="https://bowenli86.github.io/categories/data-structures-and-algorithms/"/>
    
    
      <category term="java circular buffer" scheme="https://bowenli86.github.io/tags/java-circular-buffer/"/>
    
      <category term="circular buffer" scheme="https://bowenli86.github.io/tags/circular-buffer/"/>
    
      <category term="circular buffer that overrides elements" scheme="https://bowenli86.github.io/tags/circular-buffer-that-overrides-elements/"/>
    
  </entry>
  
  <entry>
    <title>Big Data and Google&#39;s Three Papers I - GFS and MapReduce</title>
    <link href="https://bowenli86.github.io/2016/10/23/distributed%20system/data/Big-Data-and-Google-s-Three-Papers-I-GFS-and-MapReduce/"/>
    <id>https://bowenli86.github.io/2016/10/23/distributed system/data/Big-Data-and-Google-s-Three-Papers-I-GFS-and-MapReduce/</id>
    <published>2016-10-24T05:25:35.000Z</published>
    <updated>2018-04-10T04:29:25.521Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Big data</strong> is a pretty new concept that came up only serveral years ago. It emerged along with three papers from Google, <a href="http://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf" target="_blank" rel="noopener">Google File System</a>(2003), <a href="http://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf" target="_blank" rel="noopener">MapReduce</a>(2004), and <a href="http://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf" target="_blank" rel="noopener">BigTable</a>(2006).</p><p>Today I want to talk about some of my observation and understanding of the three papers, their impacts on open source big data community, particularly Hadoop ecosystem, and their positions in big data area according to the evolvement of Hadoop ecosystem.</p><h2 id="Google-File-System-GFS"><a href="#Google-File-System-GFS" class="headerlink" title="Google File System (GFS)"></a>Google File System (GFS)</h2><p><strong>Hadoop Distributed File System (HDFS)</strong> is an open sourced version of GFS, and the foundation of Hadoop ecosystem. Its fundamental role is not only documented clearly in Hadoop’s official website, but also reflected during the past ten years as big data tools evolve.</p><p>One example is that there have been so many alternatives to Hadoop MapReduce and BigTable-like NoSQL data stores coming up. For MapReduce, you have Hadoop Pig, Hadoop Hive, Spark, Kafka + Samza, Storm, and other batch/streaming processing frameworks. For NoSQL, you have HBase, AWS Dynamo, Cassandra, MongoDB, and other document, graph, key-value data stores. You can find out this trend even inside Google, e.g. 1) Google released DataFlow as official replacement of MapReduce, I bet there must be more alternatives to MapReduce within Google that haven’t been annouced 2) Google is actually emphasizing more on Spanner currently than BigTable.</p><blockquote><p>But I havn’t heard any replacement or planned replacement of GFS/HDFS.</p></blockquote><p>HDFS makes three essential assumptions among all others:</p><blockquote><ol><li>it runs on a large number of commodity hardwards, and is able to replicate files among machines to tolerate and recover from failures</li></ol></blockquote><blockquote><ol start="2"><li>it only handles extremely large files, usually at GB, or even TB and PB</li></ol></blockquote><blockquote><ol start="3"><li>it only support file append, but not update</li></ol></blockquote><p>These properties, plus some other ones, indicate two important characteristics that big data cares about:</p><blockquote><ol><li>it is able to persist files or other states with high reliability, availability, and scalability. It minimizes the possibility of losing anything; files or states are always available; the file system can scale horizontally as the size of files it stores increase</li></ol></blockquote><blockquote><ol start="2"><li>it handles <strong>big</strong> data! And as a tradeoff, it prefers throughput than low latency</li></ol></blockquote><p>In short, GFS/HDFS have proven to be the most influential component to support big data. Long live GFS/HDFS!</p><h2 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h2><p>Frankly, I’m not a big fan of MapReduce.</p><p>Google’s MapReduce paper is actually composed of two things: 1) A data processing model named MapReduce 2) A distributed, large scale data processing paradigm. The first is just one implementation of the second, and to be honest, I don’t think that implementation is a good one.</p><h3 id="1-A-data-processing-model-named-MapReduce"><a href="#1-A-data-processing-model-named-MapReduce" class="headerlink" title="1. A data processing model named MapReduce"></a>1. A data processing model named MapReduce</h3><p>I first learned map and reduce from <strong>Hadoop MapReduce</strong>. It has been an old idea, and is orginiated from functional programming, though Google carried it forward and made it well-known.</p><p>MapReduce can be strictly broken into three phases:</p><blockquote><ol><li>Map</li><li>Sort/Shuffle/Merge</li><li>Reduce</li></ol></blockquote><p><code>Map</code> and <code>Reduce</code> is programmable and provided by developers, and <code>Shuffle</code> is built-in. <code>Map</code> takes some inputs (usually a GFS/HDFS file), and breaks them into <strong>key-value</strong> pairs. <code>Sort/Shuffle/Merge</code> sorts outputs from all <code>Map</code> by <strong>key</strong>, and transport all records with the same key to the same place, guaranteed. <code>Reduce</code> does some other computations to records with the same key, and generates the final outcome by storing it in a new GFS/HDFS file.</p><p>From a data processing point of view, this design is quite rough with lots of really obvious practical defects or limitations. For example, it’s a batching processing model, thus not suitable for stream/real time data processing; it’s not good at iterating data, chaining up MapReduce jobs are costly, slow, and painful; it’s terrible at handling complex business logic; etc.</p><p>From a database stand pint of view, MapReduce is basically a <code>SELECT + GROUP BY</code> from a database point.</p><p>There’s no need for Google to preach such outdated tricks as panacea.</p><h3 id="2-A-distributed-large-scale-data-processing-paradigm"><a href="#2-A-distributed-large-scale-data-processing-paradigm" class="headerlink" title="2. A distributed, large scale data processing paradigm"></a>2. A distributed, large scale data processing paradigm</h3><p>This part in Google’s paper seems much more meaningful to me. It describes <strong>an distribued system paradigm that realizes large scale parallel computation on top of huge amount of commodity hardware</strong>.<br>Though MapReduce looks less valuable as Google tends to claim, this paradigm enpowers MapReduce with a breakingthough capability to process large amount of data unprecedentedly.</p><p>There are three noticing units in this paradigm.</p><blockquote><ol><li>Move computation to data, rather than transport data to where computation happens. This significantly reduces the network I/O patterns and keeps most of the I/O on the local disk or within the same rack.</li></ol></blockquote><blockquote><ol start="2"><li>Put all input, intermediate output, and final output to a large scale, highly reliable, highly available, and highly scalable file system, a.k.a. GFS/HDFS, to have the file system take cares lots of concerns.</li></ol></blockquote><blockquote><ol start="3"><li>Take advantage of an advanced resource management system. That system is able to automatically manage and monitor all work machines, assign resources to applications and jobs, recover from failure, and retry tasks.</li></ol></blockquote><p>The first point is actually the only innovative and practical idea Google gave in MapReduce paper. As data is extremely large, moving it will also be costly. So, instead of moving data around cluster to feed different computations, it’s much cheaper to move computations to where the data is located.</p><p>The secondly thing is, as you have guessed, GFS/HDFS.</p><p>Lastly, there’s a resource management system called <a href="http://research.google.com/pubs/pub43438.html" target="_blank" rel="noopener">Borg</a> inside Google. Google has been using it for decades, but not revealed it until 2015. Even with that, it’s not because Google is generous to give it to the world, but because Docker emerged and stripped away Borg’s competitive advantages. Google didn’t even mention Borg, such a profound piece in its data processing system, in its MapReduce paper - shame on Google!</p><p>Now you can see that the MapReduce promoted by Google is nothing significant. It’s an old programming pattern, and its implementation takes huge advantage of other systems.</p><p>That’s also why Yahoo! developed <a href="http://hortonworks.com/blog/apache-hadoop-yarn-background-and-an-overview/" target="_blank" rel="noopener">Apache Hadoop YARN</a>, a general-purpose, distributed, application management framework that supersedes the classic Apache Hadoop MapReduce framework for processing data in Hadoop clusters.</p><hr><p>I will talk about BigTable and its open sourced version in another post</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Big data&lt;/strong&gt; is a pretty new concept that came up only serveral years ago. It emerged along with three papers from Google, &lt;
      
    
    </summary>
    
      <category term="distributed system" scheme="https://bowenli86.github.io/categories/distributed-system/"/>
    
      <category term="data" scheme="https://bowenli86.github.io/categories/distributed-system/data/"/>
    
    
      <category term="Google File System" scheme="https://bowenli86.github.io/tags/Google-File-System/"/>
    
      <category term="GFS" scheme="https://bowenli86.github.io/tags/GFS/"/>
    
      <category term="HDFS" scheme="https://bowenli86.github.io/tags/HDFS/"/>
    
      <category term="Hadoop" scheme="https://bowenli86.github.io/tags/Hadoop/"/>
    
      <category term="Hadoop File System" scheme="https://bowenli86.github.io/tags/Hadoop-File-System/"/>
    
      <category term="MapReduce" scheme="https://bowenli86.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>Hexo SEO | Hexo SEO Tutorial</title>
    <link href="https://bowenli86.github.io/2016/10/19/hexo/Hexo-SEO-Hexo-SEO-Tutorial/"/>
    <id>https://bowenli86.github.io/2016/10/19/hexo/Hexo-SEO-Hexo-SEO-Tutorial/</id>
    <published>2016-10-19T23:09:54.000Z</published>
    <updated>2018-04-10T04:29:25.528Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://hunao.info/2016/06/01/Hexo-Seo%E4%BC%98%E5%8C%96%E8%AE%A9%E4%BD%A0%E7%9A%84%E5%8D%9A%E5%AE%A2%E5%9C%A8google%E6%90%9C%E7%B4%A2%E6%8E%92%E5%90%8D%E7%AC%AC%E4%B8%80/" target="_blank" rel="noopener">This blog</a> is very helpful and practical tutorial of how to do SEO on Hexo and Next theme, as well as how to use Google Search Console as a general tool for SEO.</p><p>I’ve followed the tutorial to tune up my site configs. Try to search my blogs on google!</p><hr><h2 id="Hexo-SEO-优化让你的博客在google搜索排名第一"><a href="#Hexo-SEO-优化让你的博客在google搜索排名第一" class="headerlink" title="Hexo SEO 优化让你的博客在google搜索排名第一"></a>Hexo SEO 优化让你的博客在google搜索排名第一</h2><p>刚刚建买了域名建了博客，发现在google，百度毛都搜不到，真是悲伤，后来才知道原来是要seo的，所以看了一些文章，然后自己也摸索了一下，终于在让自己的博客在google搜索排名第一了!!!上图！</p><p><img src="http://i.imgur.com/eA6Jesb.png" alt="google更新了"></p><p>哈哈，顿时觉得自己好厉害耶!<br>下面给大家分享一些我的经验。</p><h3 id="本教程在NexT主题上操作，其他主题请自行测试"><a href="#本教程在NexT主题上操作，其他主题请自行测试" class="headerlink" title="本教程在NexT主题上操作，其他主题请自行测试"></a>本教程在NexT主题上操作，其他主题请自行测试</h3><h3 id="首页title优化"><a href="#首页title优化" class="headerlink" title="首页title优化"></a>首页title优化</h3><p>更改<strong>index.swig</strong>文件<strong>(your-hexo-site\themes\next\layout)</strong>;</p><p>将下面这段代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% block title %&#125; &#123;&#123; config.title &#125;&#125; &#123;% endblock %&#125;</span><br></pre></td></tr></table></figure><p>改成</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% block title %&#125; &#123;&#123; config.title &#125;&#125; - &#123;&#123; theme.description &#125;&#125; &#123;% endblock %&#125;</span><br></pre></td></tr></table></figure><p>这时候你的首页会更符合<strong>网站名称 - 网站描述</strong>这习惯。<br>进阶，做了 SEO 优化，把关键词也显示在title标题里，可改成</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% block title %&#125; &#123;&#123; theme.keywords &#125;&#125; - &#123;&#123; config.title &#125;&#125;&#123;&#123; theme.description &#125;&#125; &#123;% endblock %&#125;</span><br></pre></td></tr></table></figure><p>注意：别堆砌关键字，整个标题一般不超过80个字符，可以通过chinaz的seo综合查询检查。</p><h3 id="给你的博客添加sitemap站点地图"><a href="#给你的博客添加sitemap站点地图" class="headerlink" title="给你的博客添加sitemap站点地图"></a>给你的博客添加sitemap站点地图</h3><p>安装sitemap站点地图自动生成插件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-generator-sitemap --save</span><br><span class="line">npm install hexo-generator-baidu-sitemap --save</span><br></pre></td></tr></table></figure><p>在主题配置文件中添加一下配置,这里有的文章说的是在站点配置文章中添加，这个应该问题不大。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sitemap: </span><br><span class="line">  path: sitemap.xml</span><br><span class="line">baidusitemap:</span><br><span class="line">  path: baidusitemap.xml</span><br></pre></td></tr></table></figure><p>注意：上面的格式一定要正确，一定要有缩进，不然会出错，我想信很多小伙伴因为没有缩进而不能编译的。</p><p>然后在<strong>主题配置文件</strong>中修改<strong>url</strong>为你的域名，例如我的</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">url: http://hunao.info</span><br></pre></td></tr></table></figure><p>NexT主题默认的是<code>http://yoursite.com</code></p><p>配置好后，<code>hexo g</code> 就能在<strong>your-hexo-site\public</strong> 中生成<strong>sitemap.xml</strong> 和 <strong>baidusitemap.xml</strong>了;其中第一个是一会要提交给google的，后面那个看名字当然就是提交给Baidu的了；</p><p>在<code>your-hexo-site\source</code>中新建文件<strong>robots.txt</strong>,内容可以参照我的</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">User-agent: *</span><br><span class="line">Allow: /</span><br><span class="line">Allow: /archives/</span><br><span class="line">Allow: /categories/</span><br><span class="line">Allow: /tags/ </span><br><span class="line">Disallow: /vendors/</span><br><span class="line">Disallow: /js/</span><br><span class="line">Disallow: /css/</span><br><span class="line">Disallow: /fonts/</span><br><span class="line">Disallow: /vendors/</span><br><span class="line">Disallow: /fancybox/</span><br></pre></td></tr></table></figure><p><strong>其中Allow后面的就是你的menu；</strong></p><p>在<strong>robots.txt</strong>中添加下面的代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Sitemap: http://www.hunao.info/sitemap.xml</span><br><span class="line">Sitemap: http://www.hunao.info/baidusitemap.xml</span><br></pre></td></tr></table></figure><p>请自行将<strong><a href="http://www.hunao.info" target="_blank" rel="noopener">www.hunao.info</a></strong>改成自己的域名, 然后<code>hexo d -g</code>提交一下</p><a id="more"></a><h3 id="给非友情链接的出站链接添加-“nofollow”-标签"><a href="#给非友情链接的出站链接添加-“nofollow”-标签" class="headerlink" title="给非友情链接的出站链接添加 “nofollow” 标签"></a>给非友情链接的出站链接添加 “nofollow” 标签</h3><p>这个可以参考文章末尾提供的参考链接，写的很详细，我就不贴出来了。</p><h3 id="注册Google-Search-Console"><a href="#注册Google-Search-Console" class="headerlink" title="注册Google Search Console"></a>注册Google Search Console</h3><p>链接：<a href="https://www.google.com/webmasters/" target="_blank" rel="noopener">https://www.google.com/webmasters/</a></p><p>根据提示注册好之后，添加你的博客域名。</p><p><img src="http://i.imgur.com/LkWvicZ.png" alt="google console"></p><p>如图，我添加了两个，你可以视情况而定。然后点击你的域名进入:</p><p><img src="http://i.imgur.com/7ehsLCS.png" alt="管理界面"></p><h3 id="测试robots-txt"><a href="#测试robots-txt" class="headerlink" title="测试robots.txt"></a>测试robots.txt</h3><p>点击左侧的<strong>robots.txt</strong>测试工具，根据提示提交你的<strong>robots.txt</strong>，其实刚才我们已经提交了。</p><p><img src="http://i.imgur.com/zcCe2ep.png" alt="robots"></p><p>注意要0错误才可以，如果有错误的话，会有提示，改正确就可以了。</p><h3 id="提交站点地图"><a href="#提交站点地图" class="headerlink" title="提交站点地图"></a>提交站点地图</h3><p>还记得我们刚才创建创建sitemap.xml文件吧,现在它要派上用场了。点击左侧工具栏的站点地图</p><p><img src="http://i.imgur.com/jMkaqRv.png" alt="站点地图"></p><p>这里我已经添加了，所以你看到的和我看到界面应该不一样，然后点右上角的添加/测试站点地图。输入sitemap先点测试，如果没问题的话，再提交。</p><p><img src="http://i.imgur.com/gIURT3t.png" alt="添加sitemap"></p><h3 id="Google-抓取方式"><a href="#Google-抓取方式" class="headerlink" title="Google 抓取方式"></a>Google 抓取方式</h3><p>提交站点地图之后，点击左侧的Google 抓取方式</p><p><img src="http://i.imgur.com/VWnQNUs.png" alt="google抓取方式"></p><p>这一步很重要！这一不很重要！这一步很重要！</p><p><img src="http://i.imgur.com/NdZ6zFx.png" alt="抓取url"></p><p>在这里我们填上我们需要抓取的url,不填这表示抓取首页，抓取方式可以选择桌面，智能手机等等，自行根据需要选择。填好url之后，点击抓取.</p><p>然后可能会出现几种情况，如:完成、部分完成、重定向等，自由这三种情况是可以提交的。<br>提交完成后，提交至索引，根据提示操作就可以了，我的提交：</p><p><img src="http://i.imgur.com/etmgeUL.png" alt="提交"></p><p>至此，你的博客在google搜索上排名想不靠前都难了，马上上google搜索一下你的关键词和博客title测试一下吧；</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="http://www.arao.me/2015/hexo-next-theme-optimize-seo/" target="_blank" rel="noopener">动动手指，不限于NexT主题的Hexo优化（SEO篇）</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;http://hunao.info/2016/06/01/Hexo-Seo%E4%BC%98%E5%8C%96%E8%AE%A9%E4%BD%A0%E7%9A%84%E5%8D%9A%E5%AE%A2%E5%9C%A8google%E6%90%9C%E7%B4%A2%E6%8E%92%E5%90%8D%E7%AC%AC%E4%B8%80/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;This blog&lt;/a&gt; is very helpful and practical tutorial of how to do SEO on Hexo and Next theme, as well as how to use Google Search Console as a general tool for SEO.&lt;/p&gt;
&lt;p&gt;I’ve followed the tutorial to tune up my site configs. Try to search my blogs on google!&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;Hexo-SEO-优化让你的博客在google搜索排名第一&quot;&gt;&lt;a href=&quot;#Hexo-SEO-优化让你的博客在google搜索排名第一&quot; class=&quot;headerlink&quot; title=&quot;Hexo SEO 优化让你的博客在google搜索排名第一&quot;&gt;&lt;/a&gt;Hexo SEO 优化让你的博客在google搜索排名第一&lt;/h2&gt;&lt;p&gt;刚刚建买了域名建了博客，发现在google，百度毛都搜不到，真是悲伤，后来才知道原来是要seo的，所以看了一些文章，然后自己也摸索了一下，终于在让自己的博客在google搜索排名第一了!!!上图！&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/eA6Jesb.png&quot; alt=&quot;google更新了&quot;&gt;&lt;/p&gt;
&lt;p&gt;哈哈，顿时觉得自己好厉害耶!&lt;br&gt;下面给大家分享一些我的经验。&lt;/p&gt;
&lt;h3 id=&quot;本教程在NexT主题上操作，其他主题请自行测试&quot;&gt;&lt;a href=&quot;#本教程在NexT主题上操作，其他主题请自行测试&quot; class=&quot;headerlink&quot; title=&quot;本教程在NexT主题上操作，其他主题请自行测试&quot;&gt;&lt;/a&gt;本教程在NexT主题上操作，其他主题请自行测试&lt;/h3&gt;&lt;h3 id=&quot;首页title优化&quot;&gt;&lt;a href=&quot;#首页title优化&quot; class=&quot;headerlink&quot; title=&quot;首页title优化&quot;&gt;&lt;/a&gt;首页title优化&lt;/h3&gt;&lt;p&gt;更改&lt;strong&gt;index.swig&lt;/strong&gt;文件&lt;strong&gt;(your-hexo-site\themes\next\layout)&lt;/strong&gt;;&lt;/p&gt;
&lt;p&gt;将下面这段代码&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;% block title %&amp;#125; &amp;#123;&amp;#123; config.title &amp;#125;&amp;#125; &amp;#123;% endblock %&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;改成&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;% block title %&amp;#125; &amp;#123;&amp;#123; config.title &amp;#125;&amp;#125; - &amp;#123;&amp;#123; theme.description &amp;#125;&amp;#125; &amp;#123;% endblock %&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;这时候你的首页会更符合&lt;strong&gt;网站名称 - 网站描述&lt;/strong&gt;这习惯。&lt;br&gt;进阶，做了 SEO 优化，把关键词也显示在title标题里，可改成&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;% block title %&amp;#125; &amp;#123;&amp;#123; theme.keywords &amp;#125;&amp;#125; - &amp;#123;&amp;#123; config.title &amp;#125;&amp;#125;&amp;#123;&amp;#123; theme.description &amp;#125;&amp;#125; &amp;#123;% endblock %&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;注意：别堆砌关键字，整个标题一般不超过80个字符，可以通过chinaz的seo综合查询检查。&lt;/p&gt;
&lt;h3 id=&quot;给你的博客添加sitemap站点地图&quot;&gt;&lt;a href=&quot;#给你的博客添加sitemap站点地图&quot; class=&quot;headerlink&quot; title=&quot;给你的博客添加sitemap站点地图&quot;&gt;&lt;/a&gt;给你的博客添加sitemap站点地图&lt;/h3&gt;&lt;p&gt;安装sitemap站点地图自动生成插件&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;npm install hexo-generator-sitemap --save&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;npm install hexo-generator-baidu-sitemap --save&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;在主题配置文件中添加一下配置,这里有的文章说的是在站点配置文章中添加，这个应该问题不大。&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;sitemap: &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  path: sitemap.xml&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;baidusitemap:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  path: baidusitemap.xml&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;注意：上面的格式一定要正确，一定要有缩进，不然会出错，我想信很多小伙伴因为没有缩进而不能编译的。&lt;/p&gt;
&lt;p&gt;然后在&lt;strong&gt;主题配置文件&lt;/strong&gt;中修改&lt;strong&gt;url&lt;/strong&gt;为你的域名，例如我的&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;url: http://hunao.info&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;NexT主题默认的是&lt;code&gt;http://yoursite.com&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;配置好后，&lt;code&gt;hexo g&lt;/code&gt; 就能在&lt;strong&gt;your-hexo-site\public&lt;/strong&gt; 中生成&lt;strong&gt;sitemap.xml&lt;/strong&gt; 和 &lt;strong&gt;baidusitemap.xml&lt;/strong&gt;了;其中第一个是一会要提交给google的，后面那个看名字当然就是提交给Baidu的了；&lt;/p&gt;
&lt;p&gt;在&lt;code&gt;your-hexo-site\source&lt;/code&gt;中新建文件&lt;strong&gt;robots.txt&lt;/strong&gt;,内容可以参照我的&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;User-agent: *&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Allow: /&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Allow: /archives/&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Allow: /categories/&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Allow: /tags/ &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Disallow: /vendors/&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Disallow: /js/&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Disallow: /css/&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Disallow: /fonts/&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Disallow: /vendors/&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Disallow: /fancybox/&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;其中Allow后面的就是你的menu；&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在&lt;strong&gt;robots.txt&lt;/strong&gt;中添加下面的代码&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;Sitemap: http://www.hunao.info/sitemap.xml&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Sitemap: http://www.hunao.info/baidusitemap.xml&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;请自行将&lt;strong&gt;&lt;a href=&quot;http://www.hunao.info&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;www.hunao.info&lt;/a&gt;&lt;/strong&gt;改成自己的域名, 然后&lt;code&gt;hexo d -g&lt;/code&gt;提交一下&lt;/p&gt;
    
    </summary>
    
      <category term="hexo" scheme="https://bowenli86.github.io/categories/hexo/"/>
    
    
      <category term="Hexo SEO" scheme="https://bowenli86.github.io/tags/Hexo-SEO/"/>
    
      <category term="Hexo SEO tutorial" scheme="https://bowenli86.github.io/tags/Hexo-SEO-tutorial/"/>
    
  </entry>
  
  <entry>
    <title>The Always On Architecture - Moving Beyond Legacy Disaster Recovery</title>
    <link href="https://bowenli86.github.io/2016/10/11/distributed%20system/The-Always-On-Architecture-Moving-Beyond-Legacy-Disaster-Recovery/"/>
    <id>https://bowenli86.github.io/2016/10/11/distributed system/The-Always-On-Architecture-Moving-Beyond-Legacy-Disaster-Recovery/</id>
    <published>2016-10-11T07:49:59.000Z</published>
    <updated>2018-04-10T04:29:25.520Z</updated>
    
    <content type="html"><![CDATA[<p>This post caught my eyes immediatelly when I browsed my <a href="http://feedly.com/" target="_blank" rel="noopener">Feedly</a>.</p><p>I’ve been working on Tableau Server, an on-premise data analytics and visualization platform for enterprise, for about two years. We used a Postgres failover approach in Tableau Server, as described in <a href="https://phoenixjiangnan.github.io/2016/07/26/system%20design/project%20experience/How-did-I-reduce-Tableau-Server-s-downtime-in-a-Postgres-failover-from-3-min-to-1-sec/" target="_blank" rel="noopener">this post</a>, and I’ve been fixing bugs for the failover mechanism for a while. The experience and pain from fixing those bugs have made me fully realized how outdated the failover approach is, while we don’t have a good solution because we are building an expensive on-premise software with so many limitations around it. We cannot assume all customers can use AWS RDS (although Tableau is making it possible so that you can if you want to), we cannot assume customers can deploy Tableau Server in more than one data center, we cannot assume a ton of things….. Those limitations have made me quite tired of my work, because they restrict my imaginations and innovations, distort the optimal way to fix problems, impede our efforts of debugging, and prevent us from adopting best practices in the industry.</p><p>Yeah, if you are developing an open-source project, those problems might not be big things because your project is FREE and how can customers complain about you! </p><p>After some careful thoughts, I believe I need a change in my career. This post is one of the triggers.</p><hr><p><a href="http://highscalability.com/blog/2016/8/23/the-always-on-architecture-moving-beyond-legacy-disaster-rec.html" target="_blank" rel="noopener">The original post</a></p><hr><blockquote><p>Failover does not cut it anymore. You need an <code>ALWAYS ON</code> architecture with multiple data centers.<br>– Martin Van Ryswyk, VP of Engineering at DataStax</p></blockquote><p>Failover, switching to a redundant or standby system when a component fails, has a long and checkered history as a way of dealing with failure. The reason is your failover mechanism becomes a single point of failure that often fails just when it’s needed most. Having worked on a few telecom systems that used a failover strategy I know exactly how stressful failover events can be and how stupid you feel when your failover fails. If you have a double or triple fault in your system, failover is exactly the time when it will happen. </p><p>For a long time the only real trick we had for achieving fault tolerance was to have a hot, warm, or cold standby (disk, interface, card, server, router, generator, datacenter, etc.) and failover to it when there’s a problem. This old style of Disaster Recovery planning is no longer adequate or necessary.</p><p>Now, thanks to cloud infrastructures, at least at a software system level, we have an alternative: <strong>an always on architecture</strong>. Google calls this <strong>a natively multihomed architecture</strong>. You can distribute data across multiple datacenters in such away that all your datacenters are always active. Each datacenter can automatically scale capacity up and down depending on what happens to other datacenters. You know, the usual sort of cloud propaganda. Robin Schumacher makes a good case here: Long live Dear CXO – When Will What Happened to Delta Happen to You?</p><a id="more"></a><h2 id="Recent-Problems-With-Disaster-Recovery"><a href="#Recent-Problems-With-Disaster-Recovery" class="headerlink" title="Recent Problems With Disaster !Recovery"></a>Recent Problems With Disaster !Recovery</h2><p>Southwest had a service disruption a year ago and again was recently bit by a “once in a thousand-year event” that caused a service outage (doesn’t it seem like once in 1000 year events happen a lot more lately?). The first incident was blamed on “legacy systems” that could no longer handle the increased load of a now much larger airline. The most recent problem was caused by a router partially failed so the failover mechanism didn’t kick in. 2,300 flights were canceled over four days at cost of perhaps $10 million. When you do your system’s engineering review do you consider partial failures? Probably not. Yet they happen and are notoriously hard to detect and deal with.</p><p>Sprint has also experienced bad backup problems:</p><pre><code>Sprint said a fire in D.C. caused problems at Sprint&apos;s data center in Reston, VA. How a fire across the street from Sprint&apos;s switch in D.C. caused issues 20 miles away wasn&apos;t quite clear, but apparently, emergency Sprint generators in D.C. didn&apos;t kick in as they were supposed to and, as so often happens, one thing led to another.</code></pre><p>And unless you were on Mars, you will have heard Delta recently experienced their own failover problems:</p><pre><code>According to the flight captain of JFK-SLC this morning, a routine scheduled switch to the backup generator this morning at 2:30am caused a fire that destroyed both the backup and the primary. Firefighters took a while to extinguish the fire. Power is now back up and 400 out of the 500 servers rebooted, still waiting for the last 100 to have the whole system fully functional</code></pre><p>Delta has come under a lot of criticism. Why was the backup generator so close to the primary that a fire could destroy both? Why is the entire worldwide system running in a single datacenter? Why don’t they test more? Why don’t they have full failover to a backup datacenter? Why are they more interested in cutting costs the building a reliable system? Why do they still use those old mainframes? Why does a company that earns $42 billion a year have such crappy systems? It’s only 500 servers, why not convert it to a cluster already? Why does management only care about their bonuses and cutting IT costs? Isn’t that what you get from years out of outsourcing IT?</p><p>There’s a lot of venom, as you might expect. If you want a little background on Delta’s systems then here’s a good article: <a href="http://www.wsj.com/articles/SB10001424052702303480304579575891541812918" target="_blank" rel="noopener">Delta Air Lines to Take Control of Its Data Systems</a>. It appears that as of 2014 Delta spun in all its passenger-service and flight operations systems. They had 180 proprietary Delta technology applications controlling their ticketing, website, flight check-in, crew scheduling and more. And they spent about $250 million a year on IT.</p><h2 id="Does-The-Whole-System-Need-A-Refactoring"><a href="#Does-The-Whole-System-Need-A-Refactoring" class="headerlink" title="Does The Whole System Need A Refactoring?"></a>Does The Whole System Need A Refactoring?</h2><p>Interesting comment on the technology involved in these systems from FormerRTCoverageAA: </p><pre><code>My advice is for ALL the major airlines to each put in about 10 million dollars (20-30 airlines would put a fund together about 200-300 million) to modernize and work on the Interfaces between them, and the hotel and car rental systems, tours, and other functions that SABRE/Amadeus/Apollo/etc. interface to. This would fund a research consortium to look at the current technology, and DEFINE THE INTERFACES for the next generation system. Maybe HP and IBM and Microsoft and whoever else wants to play could put in some money too. The key for this consortium is to have the INTERFACES defined. Give the specifications to the vendors (HP, IBM, Microsoft, Google, Priceline, Hilton, Hertz, whoever) that want to build the next generation reservations system. Then let them have 1 year and all have to work to inter-operate on the specification (just like they do on the “old” specs today for things like teletype, and last seat availability).This has worked well in the healthcare space in getting payers and providers to work together. Each potential vendor needs to plan to spend 10-50 million dollars on their proposed solution. Then, we have the inter-operability technology fair (I would make it 2 weeks to 1 month) and each vendor can pitch to each airline, car rental, hotel, tour company, Uber, etc. Let each vendor do what he wants as long as the requirements for the specifications are met. Let the best tech vendor win.It’s far past time to update these systems. Otherwise, more heartache pain and probably government bailouts to come. Possibly even larger travel and freight interruptions. A longer term blow up could put an airline out of business. Remember Eastern? I do….</code></pre><p>This all sounds like a great idea, but what could Delta do with its own architecture?</p><h2 id="The-Always-On-Architecture"><a href="#The-Always-On-Architecture" class="headerlink" title="The Always On Architecture"></a>The Always On Architecture</h2><p>Earlier this year I wrote on article on a paper from Google: <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/44686.pdf" target="_blank" rel="noopener">High-Availability at Massive Scale: Building Google’s Data Infrastructure for Ads</a> that explains their history with Always On. It seems appropriate. Here’s the article:</p><p><img src="https://c2.staticflickr.com/2/1510/25190447426_c478660902_o.png" alt=""></p><p>The main idea of the paper is that <a href="https://en.wikipedia.org/wiki/Failover" target="_blank" rel="noopener">the typical failover</a> architecture used when moving from a single datacenter to multiple datacenters doesn’t work well in practice. What does work, where work means using fewer resources while providing high availability and consistency, is a <strong>natively multihomed architecture</strong>:</p><blockquote><p>Our current approach is to build natively multihomed systems. Such systems <strong>run hot in multiple datacenters all the time, and adaptively move load between datacenters, with the ability to handle outages of any scale completely transparently</strong>. Additionally, planned datacenter outages and maintenance events are completely transparent, causing minimal disruption to the operational systems. In the past, such events required labor-intensive efforts to move operational systems from one datacenter to another</p></blockquote><p>The use of “multihoming” in this context may be confusing because <a href="https://en.wikipedia.org/wiki/Multihoming" target="_blank" rel="noopener">multihoming</a> usually refers to a computer connected to more than one network. At Google scale perhaps it’s just as natural to talk about connecting to multiple datacenters.</p><p>Google has built several multi-homed systems to guarantee high availability (4 to 5 nines) and consistency in the presence of datacenter level outages: <a href="http://research.google.com/pubs/pub38125.html" target="_blank" rel="noopener">F1 / Spanner: Relational Database</a>; <a href="http://research.google.com/pubs/pub41318.html" target="_blank" rel="noopener">Photon: Joining Continuous Data Streams</a>; <a href="http://research.google.com/pubs/pub42851.html" target="_blank" rel="noopener">Mesa: Data Warehousing</a>. The approach taken by each of these systems is discussed in the paper, as are the many challenges is building a multi-homed system: Synchronous Global State; What to Checkpoint; Repeatable Input; Exactly Once Output.</p><p>The huge constraint here is <strong>having availability and consistency</strong>. This highlights the refreshing and continued emphasis Google puts on making even these complex systems <strong>easy for programmers to use</strong>:</p><blockquote><p>The simplicity of a multi-homed system is particularly valuable for users. Without multi-homing, failover, recovery, and dealing with inconsistency are all application problems. With multi-homing, these hard problems are solved by the infrastructure, so the application developer gets high availability and consistency for free and can focus instead on building their application.</p></blockquote><p>The biggest surprise in the paper was the idea that a <strong>multihomed system can actually take far fewer resources than a failover system</strong>:</p><blockquote><p>In a multi-homed system deployed in three datacenters with 20% total catchup capacity, the total resource footprint is 170% of steady state. This is dramatically less than the 300% required in the failover design above</p></blockquote><h2 id="What’s-Wrong-With-Failover"><a href="#What’s-Wrong-With-Failover" class="headerlink" title="What’s Wrong With Failover?"></a>What’s Wrong With Failover?</h2><blockquote><p>Failover-based approaches, however, do not truly achieve high availability, and can have excessive cost due to the deployment of standby resources.</p></blockquote><blockquote><p>Our teams have had several bad experiences dealing with failover-based systems in the past. Since unplanned outages are rare, failover procedures were often added as an afterthought, not automated and not well tested. On multiple occasions, teams spent days recovering from an outage, bringing systems back online component by component, recovering state with ad hoc tools like custom MapReduces, and gradually tuning the system as it tried to catch up processing the backlog starting from the initial outage. These situations not only cause extended unavailability, but are also extremely stressful for the teams running complex mission-critical systems.</p></blockquote><h2 id="How-Do-Multihomed-Systems-Work"><a href="#How-Do-Multihomed-Systems-Work" class="headerlink" title="How Do Multihomed Systems Work?"></a>How Do Multihomed Systems Work?</h2><blockquote><p>In contrast, multi-homed systems are designed to run in multiple datacenters as a core design property, so there is no on-the-side failover concept. A multi-homed system runs live in multiple datacenters all the time. Each datacenter processes work all the time, and work is dynamically shared between datacenters to balance load. When one datacenter is slow, some fraction of work automatically moves to faster datacenters. When a datacenter is completely unavailable, all its work is automatically distributed to other datacenters.</p></blockquote><blockquote><p>There is no failover process other than the continuous dynamic load balancing. Multi-homed systems coordinate work across datacenters using shared global state that must be updated synchronously. All critical system state is replicated so that any work can be restarted in an alternate datacenter at any point, while still guaranteeing exactly once semantics. Multi-homed systems are uniquely able to provide high availability and full consistency in the presence of datacenter level failures.</p></blockquote><blockquote><p>In any of our typical streaming system, the events being processed are based on user interactions, and logged by systems serving user traffic in many datacenters around the world. A log collection service gathers these logs globally and copies them to two or more specific logs datacenters. Each logs datacenter gets a complete copy of the logs, with the guarantee that all events copied to any one datacenter will (eventually) be copied to all logs datacenters. The stream processing systems run in one or more of the logs datacenters and processes all events. Output from the stream processing system is usually stored into some globally replicated system so that the output can be consumed reliably from anywhere.</p></blockquote><blockquote><p>In a multi-homed system, all datacenters are live and processing all the time. Deploying three datacenters is typical. In steady state, each of the three datacenters process 33% of the traffic. After a failure where one datacenter is lost, the two remaining datacenters each process 50% of the traffic. </p></blockquote><p>Obviously Delta and other companies with extensive legacy systems are in a difficult position for this kind of approach. But if you consider IT something other than a cost center, and you plan to stay around for the long haul, and whole nations rely on the quality of your infrastructure, it’s probably something you should consider. We have the technology.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This post caught my eyes immediatelly when I browsed my &lt;a href=&quot;http://feedly.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Feedly&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I’ve been working on Tableau Server, an on-premise data analytics and visualization platform for enterprise, for about two years. We used a Postgres failover approach in Tableau Server, as described in &lt;a href=&quot;https://phoenixjiangnan.github.io/2016/07/26/system%20design/project%20experience/How-did-I-reduce-Tableau-Server-s-downtime-in-a-Postgres-failover-from-3-min-to-1-sec/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;this post&lt;/a&gt;, and I’ve been fixing bugs for the failover mechanism for a while. The experience and pain from fixing those bugs have made me fully realized how outdated the failover approach is, while we don’t have a good solution because we are building an expensive on-premise software with so many limitations around it. We cannot assume all customers can use AWS RDS (although Tableau is making it possible so that you can if you want to), we cannot assume customers can deploy Tableau Server in more than one data center, we cannot assume a ton of things….. Those limitations have made me quite tired of my work, because they restrict my imaginations and innovations, distort the optimal way to fix problems, impede our efforts of debugging, and prevent us from adopting best practices in the industry.&lt;/p&gt;
&lt;p&gt;Yeah, if you are developing an open-source project, those problems might not be big things because your project is FREE and how can customers complain about you! &lt;/p&gt;
&lt;p&gt;After some careful thoughts, I believe I need a change in my career. This post is one of the triggers.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href=&quot;http://highscalability.com/blog/2016/8/23/the-always-on-architecture-moving-beyond-legacy-disaster-rec.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;The original post&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Failover does not cut it anymore. You need an &lt;code&gt;ALWAYS ON&lt;/code&gt; architecture with multiple data centers.&lt;br&gt;– Martin Van Ryswyk, VP of Engineering at DataStax&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Failover, switching to a redundant or standby system when a component fails, has a long and checkered history as a way of dealing with failure. The reason is your failover mechanism becomes a single point of failure that often fails just when it’s needed most. Having worked on a few telecom systems that used a failover strategy I know exactly how stressful failover events can be and how stupid you feel when your failover fails. If you have a double or triple fault in your system, failover is exactly the time when it will happen. &lt;/p&gt;
&lt;p&gt;For a long time the only real trick we had for achieving fault tolerance was to have a hot, warm, or cold standby (disk, interface, card, server, router, generator, datacenter, etc.) and failover to it when there’s a problem. This old style of Disaster Recovery planning is no longer adequate or necessary.&lt;/p&gt;
&lt;p&gt;Now, thanks to cloud infrastructures, at least at a software system level, we have an alternative: &lt;strong&gt;an always on architecture&lt;/strong&gt;. Google calls this &lt;strong&gt;a natively multihomed architecture&lt;/strong&gt;. You can distribute data across multiple datacenters in such away that all your datacenters are always active. Each datacenter can automatically scale capacity up and down depending on what happens to other datacenters. You know, the usual sort of cloud propaganda. Robin Schumacher makes a good case here: Long live Dear CXO – When Will What Happened to Delta Happen to You?&lt;/p&gt;
    
    </summary>
    
      <category term="distributed system" scheme="https://bowenli86.github.io/categories/distributed-system/"/>
    
    
      <category term="always on architecture" scheme="https://bowenli86.github.io/tags/always-on-architecture/"/>
    
      <category term="failover" scheme="https://bowenli86.github.io/tags/failover/"/>
    
  </entry>
  
  <entry>
    <title>Repost - Job-Hopping and Salary Raise 跳槽加薪</title>
    <link href="https://bowenli86.github.io/2016/10/10/career%20and%20leadership/Repost-Job-Hopping-and-Salary-Raise-%E8%B7%B3%E6%A7%BD%E5%8A%A0%E8%96%AA/"/>
    <id>https://bowenli86.github.io/2016/10/10/career and leadership/Repost-Job-Hopping-and-Salary-Raise-跳槽加薪/</id>
    <published>2016-10-10T07:13:13.000Z</published>
    <updated>2018-04-10T04:29:25.516Z</updated>
    
    <content type="html"><![CDATA[<hr><p>From <a href="http://weibo.com/1134424202/E50IpFN45?from=page_1005051134424202_profile&amp;wvr=6&amp;mod=weibotime&amp;type=comment" target="_blank" rel="noopener">http://weibo.com/1134424202/E50IpFN45?from=page_1005051134424202_profile&amp;wvr=6&amp;mod=weibotime&amp;type=comment</a></p><hr><p>互联网行业“跳槽加薪”这件事，除了常见解释，还有一个隐性的理由。</p><p>对于公司内部，加薪体系通常是线性的，类似于每年+10-20%这样。但是在人才市场上存在非线性的涨幅。</p><p>举例来说，一位1年经验，资历尚浅的普通程序员，薪水12-15K是可以接受的。进入公司工作了两年，每年+20%，薪水18-22K。两年后，他已经从普通程序员成长为能独当一面的优秀程序员，而这个位置的市场价格是25-35K。</p><p>然后他跳个槽，30K，比公司给出的加薪幅度高10K。</p><p>公司只好招聘新人来弥补这个位置，顶替缺口的是另一位独当一面的优秀程序员，薪水同样是30K。这时围观群众纷纷嘲笑HR是傻逼，老板是臭傻逼。</p><p>之所以出现这种情况，是因为管理者不容易精准地动态判断，一个人什么时候该线性加薪，什么时候该非线性加薪，哪里是员工市场价出现跳跃式增长的时间点。如果不考虑这个时间点的话，每年+10%-20%已经是很不错的待遇了。而每个人每年来一次非线性加薪也是不合理的，公司分分钟破产。</p><p>大家都知道，跳个槽加薪多，原地不动加薪少。老板个个都是臭傻逼。但“老板个个都是臭傻逼”这件事其实不符合常识，毕竟很多“老板”其实也只是高级经理人，掏钱发工资的又不是他，得力下属都跑掉了，并不符合他的利益。</p><p>从结果上来说，当管理者不能判断非线性加薪的时间点时，跳个槽，由市场决定自己的新身价，是最合理的解决方案。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;p&gt;From &lt;a href=&quot;http://weibo.com/1134424202/E50IpFN45?from=page_1005051134424202_profile&amp;amp;wvr=6&amp;amp;mod=weibotime&amp;amp;type=comment&quot;
      
    
    </summary>
    
      <category term="career and leadership" scheme="https://bowenli86.github.io/categories/career-and-leadership/"/>
    
    
      <category term="job hopping" scheme="https://bowenli86.github.io/tags/job-hopping/"/>
    
      <category term="跳槽加薪" scheme="https://bowenli86.github.io/tags/%E8%B7%B3%E6%A7%BD%E5%8A%A0%E8%96%AA/"/>
    
  </entry>
  
  <entry>
    <title>Cache - Cache Design Patterns</title>
    <link href="https://bowenli86.github.io/2016/10/08/system%20design/cache/Cache-Cache-Design-Patterns/"/>
    <id>https://bowenli86.github.io/2016/10/08/system design/cache/Cache-Cache-Design-Patterns/</id>
    <published>2016-10-08T17:54:44.000Z</published>
    <updated>2018-04-10T04:29:25.546Z</updated>
    
    <content type="html"><![CDATA[<p>I read a blog a couple months ago talking about four most common cache design patterns, and want to summarize that post and add my understandings here.</p><h2 id="Naive-Approach"><a href="#Naive-Approach" class="headerlink" title="Naive Approach"></a>Naive Approach</h2><p>The naive approach is, when update data in cache service, first remove cached data, update data source, and then reload data from data source to cache.</p><p>However, the logic is wrong!</p><p>For example, there are two concurrent operation, one being update operation and the other being query operation. After update operation removed cached data, query operation doesn’t hit the target and triggers a reload of outdated data from data store, and then the update operation modifies data store. Thus, the cache now contains dirty/expired data, and will continue to hold such dirty data.</p><p>I’m gonna discuss four caching strategies/design patterns today.</p><ul><li>Read Through</li><li>Write Through</li><li>Cache Aside</li><li>Write Behind Caching</li></ul><p>Note that they’re design patterns, neither mechanisms of databases like MySQL or Postgres, nor how Memcache or Redis works.</p><h2 id="Read-Write-Through"><a href="#Read-Write-Through" class="headerlink" title="Read / Write Through"></a>Read / Write Through</h2><p>This is where the application treats cache as the main data store and reads data from it and writes data to it. The cache is responsible for reading and writing this data to the database, thereby relieving the application of this responsibility.</p><a id="more"></a><h2 id="1-Read-Through"><a href="#1-Read-Through" class="headerlink" title="1. Read Through"></a>1. Read Through</h2><p><code>Read Through</code> is used in querying operations to update cached data.</p><p>When cached data is evicted (due to expiration or LRU eviction), it’s the application’s responsibility to load new data into cache in <code>Cache Aside</code> strategy , while it’s cache service’s responsibility in <code>Read Through</code> strategy. This function brings transparency of caching to application. </p><h2 id="2-Write-Through"><a href="#2-Write-Through" class="headerlink" title="2. Write Through"></a>2. Write Through</h2><p><code>Write Through</code> is used in data updating operations to update cached data.</p><p>Whenever application updates a piece of data</p><ul><li>if data is not in cache, cache service directly update data store</li><li>if data is in cache, cache service will first update itself, and then update database. A write is done synchronously both to the cache and to the backing store, conforming a single transaction.</li></ul><h2 id="3-Cache-Aside"><a href="#3-Cache-Aside" class="headerlink" title="3. Cache Aside"></a>3. Cache Aside</h2><p><a href="https://msdn.microsoft.com/en-us/library/dn589799.aspx" target="_blank" rel="noopener">This article</a> explains Cache Aside strategy very well. I’ll grab some content from there.</p><p>For caches that do not provide read/write-through functionality, it is the responsibility of the applications that use the cache to maintain the data in the cache. An application can emulate the functionality of read-through caching by implementing the cache-aside strategy. This strategy effectively loads data into the cache on demand. The following figure summarizes the steps in this process.</p><p><img src="https://i-msdn.sec.s-msft.com/dynimg/IC709568.png" alt=""></p><p>If an application updates information, it can emulate the write-through strategy as follows:</p><ul><li>Make the modification to the data store</li><li>Invalidate the corresponding item in the cache.</li></ul><p>When the item is next required, using the cache-aside strategy will cause the updated data to be retrieved from the data store and added back into the cache.</p><h3 id="Issues-and-Considerations"><a href="#Issues-and-Considerations" class="headerlink" title="Issues and Considerations"></a>Issues and Considerations</h3><p>Consider the following points when deciding how to implement this pattern:</p><ul><li><p><strong>Lifetime of Cached Data</strong>. Many caches implement an expiration policy that causes data to be invalidated and removed from the cache if it is not accessed for a specified period. For cache-aside to be effective, ensure that the expiration policy matches the pattern of access for applications that use the data. Do not make the expiration period too short because this can cause applications to continually retrieve data from the data store and add it to the cache. Similarly, do not make the expiration period so long that the cached data is likely to become stale. Remember that caching is most effective for relatively static data, or data that is read frequently.</p></li><li><p><strong>Evicting Data</strong>. Most caches have only a limited size compared to the data store from where the data originates, and they will evict data if necessary. Most caches adopt a least-recently-used policy for selecting items to evict, but this may be customizable. Configure the global expiration property and other properties of the cache, and the expiration property of each cached item, to help ensure that the cache is cost effective. It may not always be appropriate to apply a global eviction policy to every item in the cache. For example, if a cached item is very expensive to retrieve from the data store, it may be beneficial to retain this item in cache at the expense of more frequently accessed but less costly items.</p></li><li><p><strong>Priming the Cache</strong>. Many solutions prepopulate the cache with the data that an application is likely to need as part of the startup processing. The Cache-Aside pattern may still be useful if some of this data expires or is evicted.</p></li><li><p><strong>Consistency</strong>. Implementing the Cache-Aside pattern does not guarantee consistency between the data store and the cache. An item in the data store may be changed at any time by an external process, and this change might not be reflected in the cache until the next time the item is loaded into the cache. In a system that replicates data across data stores, this problem may become especially acute if synchronization occurs very frequently.</p></li><li><p><strong>Local (In-Memory) Caching</strong>. A cache could be local to an application instance and stored in-memory. Cache-aside can be useful in this environment if an application repeatedly accesses the same data. However, a local cache is private and so different application instances could each have a copy of the same cached data. This data could quickly become inconsistent between caches, so it may be necessary to expire data held in a private cache and refresh it more frequently. In these scenarios it may be appropriate to investigate the use of a shared or a distributed caching mechanism.</p></li></ul><h3 id="When-to-Use-this-Pattern"><a href="#When-to-Use-this-Pattern" class="headerlink" title="When to Use this Pattern"></a>When to Use this Pattern</h3><p>Use this pattern when:</p><ul><li><p>A cache does not provide native read-through and write-through operations.</p></li><li><p>Resource demand is unpredictable. This pattern enables applications to load data on demand. It makes no assumptions about which data an application will require in advance.</p></li></ul><p>This pattern might not be suitable:</p><ul><li>When the cached data set is static. If the data will fit into the available cache space, prime the cache with the data on startup and apply a policy that prevents the data from expiring.</li><li>For caching session state information in a web application hosted in a web farm. In this environment, you should avoid introducing dependencies based on client-server affinity.</li></ul><h2 id="4-Write-Behind"><a href="#4-Write-Behind" class="headerlink" title="4. Write Behind"></a>4. Write Behind</h2><p>In the Write-Behind scenario, modified cache entries are written to cache, not data source. Updates in cache are asynchronously written to the data source after a configured delay, whether after 10 seconds, 20 minutes, a day, a week or even longer. Note that this only applies to cache inserts and updates - cache entries are removed synchronously from the data source.</p><p>For Write-Behind caching, cache service maintains a write-behind queue of the data that must be updated in the data source. When the application updates <code>X</code> in the cache, <code>X</code> is added to the write-behind queue (if it isn’t there already; otherwise, it is replaced), and after the specified write-behind delay, cache service will update the underlying data source with the latest state of <code>X</code>. Note that the write-behind delay is relative to the first of a series of modifications—in other words, the data in the data source will never lag behind the cache by more than the write-behind delay.</p><p>The result is a <code>read-once and write at a configured interval</code> (that is, much less often) scenario. There are four main benefits to this type of architecture:</p><ul><li><p>The application improves in performance, because the user does not have to wait for data to be written to the underlying data source. (The data is written later, and by a different execution thread.)</p></li><li><p>The application experiences drastically reduced database load: Since the amount of both read and write operations is reduced, so is the database load. The reads are reduced by caching, as with any other caching approach. The writes, which are typically much more expensive operations, are often reduced because multiple changes to the same object within the write-behind interval are <code>coalesced</code> and only written once to the underlying data source (<code>write-coalescing</code>). Additionally, writes to multiple cache entries may be combined into a single database transaction (<code>write-combining</code>) .</p></li><li><p>The application is somewhat insulated from database failures: the Write-Behind feature can be configured in such a way that a write failure will result in the object being re-queued for write. If the data that the application is using is in the cache, the application can continue operation without the database being up.</p></li><li><p>Linear Scalability: For an application to handle more concurrent users you need only increase the number of nodes in the cluster; the effect on the database in terms of load can be tuned by increasing the write-behind interval.</p></li></ul><blockquote><p>Write-behind strategy is actually Linux file system’s Page Cache. Because all updates are written asynchronously with a delay, Linux will lose some data updates when exit unexpectedly, e.g. machine being shut down.</p></blockquote><h2 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h2><p>Developers can leverage all the strategies demonstrated above, and combine some of them together in when designing caching strategies. For example, you can have a Read-Through Write-Behind cache layer.</p><hr><p>References:</p><ul><li><p><a href="http://docs.oracle.com/cd/E13924_01/coh.340/e13819/readthrough.htm" target="_blank" rel="noopener">http://docs.oracle.com/cd/E13924_01/coh.340/e13819/readthrough.htm</a></p></li><li><p><a href="https://msdn.microsoft.com/en-us/library/dn589799.aspx" target="_blank" rel="noopener">https://msdn.microsoft.com/en-us/library/dn589799.aspx</a></p></li></ul><hr><p>Here’s the <a href="http://coolshell.cn/articles/17416.html" target="_blank" rel="noopener">original post</a></p><hr><p>看来，有很多人是不知道缓存同步的几个Design Pattern的，如：<code>Cache aside</code>, <code>Read through</code>, <code>Write through</code>, <code>Write behind caching</code>…</p><blockquote><p>看到好些人在写更新缓存数据代码时，先删除缓存，然后再更新数据库，而后续的操作会把数据再装载的缓存中。然而，这个是逻辑是错误的。试想，两个并发操作，一个是更新操作，另一个是查询操作，更新操作删除缓存后，查询操作没有命中缓存，先把老数据读出来后放到缓存中，然后更新操作更新了数据库。于是，在缓存中的数据还是老的数据，导致缓存中的数据是脏的，而且还一直这样脏下去了。</p></blockquote><p>我不知道为什么这么多人用的都是这个逻辑，当我在微博上发了这个贴以后，我发现好些人给了好多非常复杂和诡异的方案，所以，我想写这篇文章说一下几个缓存更新的Design Pattern（让我们多一些套路吧）。</p><p>这里，我们先不讨论更新缓存和更新数据这两个事是一个事务的事，或是会有失败的可能，我们先假设<code>更新数据库</code>和<code>更新缓存</code>都可以成功的情况（我们先把成功的代码逻辑先写对）。</p><p>更新缓存的的Design Pattern有四种：<code>Cache aside</code>, <code>Read through</code>, <code>Write through</code>, <code>Write behind caching</code>，我们下面一一来看一下这四种Pattern。</p><h2 id="1-Cache-Aside-Pattern"><a href="#1-Cache-Aside-Pattern" class="headerlink" title="1. Cache Aside Pattern"></a>1. Cache Aside Pattern</h2><p>这是最常用最常用的pattern了。其具体逻辑如下：</p><ul><li>失效：应用程序先从cache取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。</li><li>命中：应用程序从cache中取数据，取到后返回。</li><li>更新：先把数据存到数据库中，成功后，再让缓存失效。</li></ul><p><code>注意，我们的更新是先更新数据库，成功后，让缓存失效。</code> </p><p>那么，这种方式是否可以没有文章前面提到过的那个问题呢？我们可以脑补一下。</p><p>一个是查询操作，一个是更新操作的并发，首先，没有了删除cache数据的操作了，而是先更新了数据库中的数据，此时，缓存依然有效，所以，并发的查询操作拿的是没有更新的数据，但是，更新操作马上让缓存的失效了，后续的查询操作再把数据从数据库中拉出来。而不会像文章开头的那个逻辑产生的问题，后续的查询操作一直都在取老的数据。</p><h2 id="2-Read-through-3-Write-Through-Pattern"><a href="#2-Read-through-3-Write-Through-Pattern" class="headerlink" title="2. Read through / 3. Write Through Pattern"></a>2. Read through / 3. Write Through Pattern</h2><p>我们可以看到，在上面的 Cache Aside 套路中，我们的应用代码需要维护两个数据存储，一个是缓存（Cache），一个是数据库（Repository）。所以，应用程序比较啰嗦。而 Read/Write Through 套路是把更新数据库（Repository）的操作由缓存自己代理了，所以，对于应用层来说，就简单很多了。可以理解为，应用认为后端就是一个单一的存储，而存储自己维护自己的Cache。</p><h3 id="Read-Through"><a href="#Read-Through" class="headerlink" title="Read Through"></a>Read Through</h3><p>Read Through 套路就是在查询操作中更新缓存，也就是说，当缓存失效的时候（过期或LRU换出），Cache Aside 是由调用方负责把数据加载入缓存，而 Read Through 则用缓存服务自己来加载，从而对应用方是透明的。</p><h3 id="Write-Through"><a href="#Write-Through" class="headerlink" title="Write Through"></a>Write Through</h3><p>Write Through 套路和 Read Through 相仿，不过是在更新数据时发生。当有数据更新的时候，如果没有命中缓存，直接更新数据库，然后返回。如果命中了缓存，则更新缓存，然后再由 Cache 自己更新数据库（这是一个同步操作）</p><p>下图自来Wikipedia的Cache词条。其中的Memory你可以理解为就是我们例子里的数据库。</p><h2 id="4-Write-Behind-Caching-Pattern"><a href="#4-Write-Behind-Caching-Pattern" class="headerlink" title="4. Write Behind Caching Pattern"></a>4. Write Behind Caching Pattern</h2><p>Write Behind 又叫 Write Back。一些了解 Linux 操作系统内核的同学对 write back 应该非常熟悉，这不就是 Linux 文件系统的 Page Cache 的算法吗？是的，你看基础这玩意全都是相通的。所以，基础很重要，我已经不是一次说过基础很重要这事了。</p><p>Write Back 套路，一句说就是，在更新数据的时候，只更新缓存，不更新数据库，而我们的缓存会异步地批量更新数据库。这个设计的好处就是让数据的 I/O 操作飞快无比（因为直接操作内存嘛）. 因为异步，write back 还可以合并对同一个数据的多次操作，所以性能的提高是相当可观的。</p><p>但是，其带来的问题是，数据不是强一致性的，而且可能会丢失（我们知道 Unix/Linux 非正常关机会导致数据丢失，就是因为这个事）。在软件设计上，我们基本上不可能做出一个没有缺陷的设计，就像算法设计中的时间换空间，空间换时间一个道理，有时候，强一致性和高性能，高可用和高性性是有冲突的。软件设计从来都是取舍 Trade-Off。</p><p>另外，Write Back 实现逻辑比较复杂, 因为他需要 track 有哪数据是被更新了的，需要刷到持久层上。操作系统的 write back 会在仅当这个cache需要失效的时候，才会被真正持久起来，比如，内存不够了，或是进程退出了等情况，这又叫 lazy write。</p><p>在 wikipedia 上有一张 write back 的流程图，基本逻辑如下：</p><p>Write-back_with_write-allocation</p><h2 id="再多唠叨一些"><a href="#再多唠叨一些" class="headerlink" title="再多唠叨一些"></a>再多唠叨一些</h2><ol><li><p>上面讲的这些 Design Pattern，其实并不是软件架构里的 mysql 数据库和 memcache/redis 的更新策略，这些东西都是计算机体系结构里的设计，比如CPU的缓存，硬盘文件系统中的缓存，硬盘上的缓存，数据库中的缓存。基本上来说，这些缓存更新的设计模式都是非常老古董的，而且历经长时间考验的策略，所以这也就是，工程学上所谓的Best Practice，遵从就好了。</p></li><li><p>有时候，我们觉得能做宏观的系统架构的人一定是很有经验的，其实，宏观系统架构中的很多设计都来源于这些微观的东西。比如，云计算中的很多虚拟化技术的原理，和传统的虚拟内存不是很像么？Unix 下的那些 I/O 模型，也放大到了架构里的同步异步的模型，还有 Unix 发明的管道不就是数据流式计算架构吗？TCP 的好些设计也用在不同系统间的通讯中，仔细看看这些微观层面，你会发现有很多设计都非常精妙……所以，请允许我在这里放句观点鲜明的话——如果你要做好架构，首先你得把计算机体系结构以及很多老古董的基础技术吃透了。</p></li><li><p>在软件开发或设计中，我非常建议在之前先去参考一下已有的设计和思路，看看相应的 guideline，best practice 或 design pattern，吃透了已有的这些东西，再决定是否要重新发明轮子。千万不要似是而非地，想当然的做软件设计。</p></li><li><p>上面，我们没有考虑缓存（Cache）和持久层（Repository）的整体事务的问题。比如，更新 Cache 成功，更新数据库失败了怎么吗？或是反过来。关于这个事，如果你需要强一致性，你需要使用“两阶段提交协议”——prepare, commit/rollback，比如 Java 7 的 XAResource，还有 MySQL 5.7 的 XA Transaction，有些cache也支持XA，比如EhCache。当然，XA这样的强一致性的玩法会导致性能下降，关于分布式的事务的相关话题，你可以看看《分布式系统的事务处理》一文。</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I read a blog a couple months ago talking about four most common cache design patterns, and want to summarize that post and add my understandings here.&lt;/p&gt;
&lt;h2 id=&quot;Naive-Approach&quot;&gt;&lt;a href=&quot;#Naive-Approach&quot; class=&quot;headerlink&quot; title=&quot;Naive Approach&quot;&gt;&lt;/a&gt;Naive Approach&lt;/h2&gt;&lt;p&gt;The naive approach is, when update data in cache service, first remove cached data, update data source, and then reload data from data source to cache.&lt;/p&gt;
&lt;p&gt;However, the logic is wrong!&lt;/p&gt;
&lt;p&gt;For example, there are two concurrent operation, one being update operation and the other being query operation. After update operation removed cached data, query operation doesn’t hit the target and triggers a reload of outdated data from data store, and then the update operation modifies data store. Thus, the cache now contains dirty/expired data, and will continue to hold such dirty data.&lt;/p&gt;
&lt;p&gt;I’m gonna discuss four caching strategies/design patterns today.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Read Through&lt;/li&gt;
&lt;li&gt;Write Through&lt;/li&gt;
&lt;li&gt;Cache Aside&lt;/li&gt;
&lt;li&gt;Write Behind Caching&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that they’re design patterns, neither mechanisms of databases like MySQL or Postgres, nor how Memcache or Redis works.&lt;/p&gt;
&lt;h2 id=&quot;Read-Write-Through&quot;&gt;&lt;a href=&quot;#Read-Write-Through&quot; class=&quot;headerlink&quot; title=&quot;Read / Write Through&quot;&gt;&lt;/a&gt;Read / Write Through&lt;/h2&gt;&lt;p&gt;This is where the application treats cache as the main data store and reads data from it and writes data to it. The cache is responsible for reading and writing this data to the database, thereby relieving the application of this responsibility.&lt;/p&gt;
    
    </summary>
    
      <category term="system design" scheme="https://bowenli86.github.io/categories/system-design/"/>
    
      <category term="cache" scheme="https://bowenli86.github.io/categories/system-design/cache/"/>
    
    
      <category term="cache design patterns" scheme="https://bowenli86.github.io/tags/cache-design-patterns/"/>
    
      <category term="read through" scheme="https://bowenli86.github.io/tags/read-through/"/>
    
      <category term="write through" scheme="https://bowenli86.github.io/tags/write-through/"/>
    
      <category term="cache aside" scheme="https://bowenli86.github.io/tags/cache-aside/"/>
    
      <category term="write back" scheme="https://bowenli86.github.io/tags/write-back/"/>
    
      <category term="write behind" scheme="https://bowenli86.github.io/tags/write-behind/"/>
    
  </entry>
  
  <entry>
    <title>Bloom Filter - Introduction</title>
    <link href="https://bowenli86.github.io/2016/10/07/data%20structures%20and%20algorithms/bloom%20filter/Bloom-Filter-Introduction/"/>
    <id>https://bowenli86.github.io/2016/10/07/data structures and algorithms/bloom filter/Bloom-Filter-Introduction/</id>
    <published>2016-10-07T18:50:33.000Z</published>
    <updated>2018-04-10T04:29:25.517Z</updated>
    
    <content type="html"><![CDATA[<p>Bloom Filter has become a hot topic in interviewing.</p><blockquote><p>A bloom filter is a space-efficient probabilistic data structure designed to tell you an element either <code>possibly in set</code> or <code>definitely not in set</code>. That’s saying, <code>false positive matches are possible</code>, but <code>false negatives are not</code>. Elements can be added to the set, but not removed (though this can be addressed with a <code>counting filter</code>). The more elements that are added to the set, the larger the probability of false positives.</p></blockquote><h2 id="How-to-store-a-value"><a href="#How-to-store-a-value" class="headerlink" title="How to store a value"></a>How to store a value</h2><blockquote><p>An empty <code>Bloom Filter</code> is <code>an bit array of m bits</code>, all set to 0. </p></blockquote><p>To store a value, there must also be either </p><ul><li><p><code>k different hash functions</code> defined, each of which maps or hashes some set element to one of the m array positions with a uniform random distribution.</p><p>  Typically, <code>k</code> is a constant, much smaller than <code>m</code> (<code>k</code> &lt;&lt; <code>m</code>), which is proportional to the number of elements to be added; the precise choice of <code>k</code> and the constant of proportionality of m are determined by the intended false positive rate of the filter. Feed the value to each of the <code>k</code> hash functions to get <code>k</code> array positions.</p></li><li><p>or one hash function which hashes bits of the value and returns <code>k</code> array positions.</p><ul><li>there’re several ways to achieve this, we’ll talk about this later</li></ul></li></ul><p>To add an element,  Set the bits at all these positions to <code>1</code>.</p><h2 id="How-to-query-a-value"><a href="#How-to-query-a-value" class="headerlink" title="How to query a value"></a>How to query a value</h2><p>To query for an element (test whether it is in the set), get the <code>k</code> array positions by either</p><ul><li>feeding it to each of the <code>k</code> hash functions </li><li>or that single hash function which returns <code>k</code> positions.</li></ul><p>If any of the bits at these positions is <code>0</code>, the element is definitely not in the set – if it were, then all the bits would have been set to <code>1</code> when it was inserted.</p><p>If all are <code>1</code>, then either the element is in the set, or the bits have by chance been set to <code>1</code> during the insertion of other elements, resulting in a false positive. In a simple Bloom Filter, there is no way to distinguish between the two cases, but more advanced techniques can address this problem.</p><a id="more"></a><h2 id="Space-and-Time-Advantage"><a href="#Space-and-Time-Advantage" class="headerlink" title="Space and Time Advantage"></a>Space and Time Advantage</h2><h3 id="Space"><a href="#Space" class="headerlink" title="Space"></a>Space</h3><p>While risking false positives, <code>Bloom Filter</code> has a strong space advantage over other data structures for representing sets, such as self-balancing binary search trees, tries, hash tables, or simple arrays or linked lists of the entries. Most of those require storing at least the data items themselves, which can require anywhere from a small number of bits, for small integers, to an arbitrary number of bits, such as for strings (tries are an exception, since they can share storage between elements with equal prefixes). </p><p>However, Bloom Filter does not store the data items at all, and a separate solution must be provided for the actual storage. Linked structures incur an additional linear space overhead for pointers. A Bloom Filter with <code>1%</code> error and an optimal value of <code>k</code>, in contrast, requires only about <code>9.6</code> bits per element, regardless of the size of the elements. This advantage comes partly from its compactness, inherited from arrays, and partly from its probabilistic nature. The <code>1%</code> false-positive rate can be reduced by a factor of ten by adding only about <code>4.8</code> bits per element.</p><p>However, if the number of potential values is small and many of them can be in the set, the Bloom Filter is easily surpassed by the deterministic bit array, which requires only one bit for each potential element. Note also that hash tables gain a space and time advantage if they begin ignoring collisions and store only whether each bucket contains an entry; in this case, they have effectively become Bloom Filters with <code>k = 1</code></p><h3 id="Time"><a href="#Time" class="headerlink" title="Time"></a>Time</h3><p>Bloom Filters also have the unusual property that the time needed either to add items or to check whether an item is in the set is a fixed constant, <code>O(k)</code>, completely independent of the number of items already in the set. </p><hr><p>No other constant-space set data structure has this property, but the average access time of sparse hash tables can make them faster in practice than some Bloom Filters. In a hardware implementation, however, the Bloom Filter shines because its <code>k</code> lookups are independent and can be parallelized.</p><p>To understand its space efficiency, it is instructive to compare the general Bloom Filter with its special case when <code>k = 1</code>. If <code>k = 1</code>, then in order to keep the false positive rate sufficiently low, a small fraction of bits should be set, which means the array must be very large and contain long runs of zeros. The information content of the array relative to its size is low. The generalized Bloom filter (when <code>k &gt; 1</code>) allows many more bits to be set while still maintaining a low false positive rate; if the parameters (<code>k</code> and <code>m</code>) are chosen well, about half of the bits will be set, and these will be apparently random, minimizing redundancy and maximizing information content.</p><h2 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h2><p>I’ll discuss the implementation details of these Bloom Filters later.</p><h3 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h3><p><a href="https://github.com/apache/hadoop/tree/f67237cbe7bc48a1b9088e990800b37529f1db2a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/bloom" target="_blank" rel="noopener">Hadoop Github Mirror</a></p><ul><li><p><a href="https://github.com/apache/hadoop/blob/f67237cbe7bc48a1b9088e990800b37529f1db2a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/bloom/BloomFilter.java" target="_blank" rel="noopener">Bloom Filter</a></p></li><li><p><a href="https://github.com/apache/hadoop/blob/f67237cbe7bc48a1b9088e990800b37529f1db2a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java" target="_blank" rel="noopener">Counting Bloom Filter</a></p></li><li><p><a href="https://github.com/apache/hadoop/blob/f67237cbe7bc48a1b9088e990800b37529f1db2a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java" target="_blank" rel="noopener">Dynamic Bloom Filter</a></p></li><li><p><a href="https://github.com/apache/hadoop/blob/f67237cbe7bc48a1b9088e990800b37529f1db2a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java" target="_blank" rel="noopener">Retouched Bloom Filter</a></p></li><li><p><a href="https://github.com/apache/hadoop/blob/f67237cbe7bc48a1b9088e990800b37529f1db2a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/bloom/TestBloomFilters.java" target="_blank" rel="noopener">Test Bloom Filter</a></p></li><li><p><a href="https://github.com/apache/hadoop/blob/f67237cbe7bc48a1b9088e990800b37529f1db2a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/bloom/BloomFilterCommonTester.java" target="_blank" rel="noopener">BloomFilterCommonTester</a></p></li></ul><h3 id="Cassandra"><a href="#Cassandra" class="headerlink" title="Cassandra"></a>Cassandra</h3><ul><li><a href="https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/utils/BloomFilter.java" target="_blank" rel="noopener">https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/utils/BloomFilter.java</a></li></ul><hr><p>Reference:</p><ul><li><p><a href="https://en.wikipedia.org/wiki/Bloom_filter" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Bloom_filter</a></p></li><li><p><a href="http://stackoverflow.com/questions/6533582/advantage-of-using-0x01-instead-of-1-for-an-integer-variable" target="_blank" rel="noopener">http://stackoverflow.com/questions/6533582/advantage-of-using-0x01-instead-of-1-for-an-integer-variable</a></p></li><li><p><a href="http://billmill.org/bloomfilter-tutorial/" target="_blank" rel="noopener">http://billmill.org/bloomfilter-tutorial/</a></p></li><li><p><a href="http://www.javamex.com/tutorials/collections/bloom_filter_java.shtml" target="_blank" rel="noopener">http://www.javamex.com/tutorials/collections/bloom_filter_java.shtml</a></p></li><li><p><a href="http://www.sanfoundry.com/java-program-implement-bloom-filter/" target="_blank" rel="noopener">http://www.sanfoundry.com/java-program-implement-bloom-filter/</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Bloom Filter has become a hot topic in interviewing.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A bloom filter is a space-efficient probabilistic data structure designed to tell you an element either &lt;code&gt;possibly in set&lt;/code&gt; or &lt;code&gt;definitely not in set&lt;/code&gt;. That’s saying, &lt;code&gt;false positive matches are possible&lt;/code&gt;, but &lt;code&gt;false negatives are not&lt;/code&gt;. Elements can be added to the set, but not removed (though this can be addressed with a &lt;code&gt;counting filter&lt;/code&gt;). The more elements that are added to the set, the larger the probability of false positives.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;How-to-store-a-value&quot;&gt;&lt;a href=&quot;#How-to-store-a-value&quot; class=&quot;headerlink&quot; title=&quot;How to store a value&quot;&gt;&lt;/a&gt;How to store a value&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;An empty &lt;code&gt;Bloom Filter&lt;/code&gt; is &lt;code&gt;an bit array of m bits&lt;/code&gt;, all set to 0. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To store a value, there must also be either &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;k different hash functions&lt;/code&gt; defined, each of which maps or hashes some set element to one of the m array positions with a uniform random distribution.&lt;/p&gt;
&lt;p&gt;  Typically, &lt;code&gt;k&lt;/code&gt; is a constant, much smaller than &lt;code&gt;m&lt;/code&gt; (&lt;code&gt;k&lt;/code&gt; &amp;lt;&amp;lt; &lt;code&gt;m&lt;/code&gt;), which is proportional to the number of elements to be added; the precise choice of &lt;code&gt;k&lt;/code&gt; and the constant of proportionality of m are determined by the intended false positive rate of the filter. Feed the value to each of the &lt;code&gt;k&lt;/code&gt; hash functions to get &lt;code&gt;k&lt;/code&gt; array positions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;or one hash function which hashes bits of the value and returns &lt;code&gt;k&lt;/code&gt; array positions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;there’re several ways to achieve this, we’ll talk about this later&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To add an element,  Set the bits at all these positions to &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&quot;How-to-query-a-value&quot;&gt;&lt;a href=&quot;#How-to-query-a-value&quot; class=&quot;headerlink&quot; title=&quot;How to query a value&quot;&gt;&lt;/a&gt;How to query a value&lt;/h2&gt;&lt;p&gt;To query for an element (test whether it is in the set), get the &lt;code&gt;k&lt;/code&gt; array positions by either&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;feeding it to each of the &lt;code&gt;k&lt;/code&gt; hash functions &lt;/li&gt;
&lt;li&gt;or that single hash function which returns &lt;code&gt;k&lt;/code&gt; positions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If any of the bits at these positions is &lt;code&gt;0&lt;/code&gt;, the element is definitely not in the set – if it were, then all the bits would have been set to &lt;code&gt;1&lt;/code&gt; when it was inserted.&lt;/p&gt;
&lt;p&gt;If all are &lt;code&gt;1&lt;/code&gt;, then either the element is in the set, or the bits have by chance been set to &lt;code&gt;1&lt;/code&gt; during the insertion of other elements, resulting in a false positive. In a simple Bloom Filter, there is no way to distinguish between the two cases, but more advanced techniques can address this problem.&lt;/p&gt;
    
    </summary>
    
      <category term="data structures and algorithms" scheme="https://bowenli86.github.io/categories/data-structures-and-algorithms/"/>
    
      <category term="bloom filter" scheme="https://bowenli86.github.io/categories/data-structures-and-algorithms/bloom-filter/"/>
    
    
      <category term="bloom filter" scheme="https://bowenli86.github.io/tags/bloom-filter/"/>
    
  </entry>
  
  <entry>
    <title>Apache Kafka, Samza, and the Unix Philosopy of Distributed Data</title>
    <link href="https://bowenli86.github.io/2016/10/06/distributed%20system/kafka/Apache-Kafka-Samza-and-the-Unix-Philosopy-of-Distributed-Data/"/>
    <id>https://bowenli86.github.io/2016/10/06/distributed system/kafka/Apache-Kafka-Samza-and-the-Unix-Philosopy-of-Distributed-Data/</id>
    <published>2016-10-07T02:55:32.000Z</published>
    <updated>2018-04-10T04:29:25.522Z</updated>
    
    <content type="html"><![CDATA[<p>Here’s another blog from Confluent’s website. What’s interesting about this article is that it compares the design philosophy between Unix and monolith database, and refers to Kafka as a distributed version of Unix pipeline. The latter point is exceptionally interesting and brings a whole new view of Kafka to me.</p><p>After working fulltime in the IT industry for almost two years, I’ve seen repeatedly that design of modern system borrows ideas from the design of Linux/Unix. Besides Kafka and Unix pipeline, another example is the <a href="http://docs.oracle.com/cd/E13924_01/coh.340/e13819/readthrough.htm" target="_blank" rel="noopener">Write Behind Caching Pattern</a> in modern system actually refers to <a href="https://www.thomas-krenn.com/en/wiki/Linux_Page_Cache_Basics" target="_blank" rel="noopener">Linux’s Page Cache</a></p><hr><p><a href="http://www.confluent.io/blog/apache-kafka-samza-and-the-unix-philosophy-of-distributed-data/" target="_blank" rel="noopener">Original Post</a> by Martin Kleppmann. August 1, 2015.</p><hr><p><img src="http://www.confluent.io/wp-content/uploads/2016/08/unixphil-01.png" alt="unixphil-01"></p><p>One of the things I realised while doing research for my book is that contemporary software engineering still has a lot to learn from the 1970s. As we’re in such a fast-moving field, we often have a tendency of dismissing older ideas as irrelevant – and consequently, we end up having to learn the same lessons over and over again, the hard way. Although computers have got faster, data has got bigger and requirements have become more complex, many old ideas are actually still highly relevant today. In this blog post I’d like to highlight one particular set of old ideas that I think deserves more attention today: <code>the Unix philosophy</code>. I’ll show how this philosophy is very different from the design approach of mainstream databases, and explore what it would look like if modern distributed data systems learnt a thing or two from Unix.</p><p>In particular, I’m going to argue that there are a lot of similarities between <code>Unix pipes</code> and <code>Apache Kafka</code>, and that this similarity enables good architectural styles for large-scale applications. But before we get into that, let me remind you of the foundations of the <code>Unix philosophy</code>. You’ve probably seen the power of Unix tools before – but to get started, let me give you a concrete example that we can talk about. Say you have a web server that writes an entry to a log file every time it serves a request. For example, using the nginx default access log format, one line of the log might look like this:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">216.58.210.78 - - [27/Feb/2015:17:55:11 +0000] &quot;GET /css/typography.css HTTP/1.1&quot; 200 3377 &quot;http://martin.kleppmann.com/&quot; &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115 Safari/537.36&quot;</span><br></pre></td></tr></table></figure><p>(That is actually one line, it’s only broken up into multiple lines here for readability.) This line of the log indicates that on 27 February 2015 at 17:55:11 UTC, the server received a request for the file /css/typography.css from the client IP address 216.58.210.78. It then goes on to note various other details, including the browser’s user-agent string. Various tools can take these log files and produce pretty reports about your website traffic, but for the sake of the exercise, let’s build our own, using basic Unix tools. Let’s determine the 5 most popular URLs on our website. To start with, we need to extract the path of the URL that was requested, for which we can use <code>awk</code>. <code>awk</code> doesn’t know about the format of nginx logs – it just treats the log file as text. By default, <code>awk</code> takes one line of input at a time, splits it by whitespace, and makes the whitespace-separated components available as variables <code>$1</code>, <code>$2</code>, etc. In the nginx log example, the requested URL path is the seventh whitespace-separated component: </p><a id="more"></a><p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-03.png" alt=""></p><p>Now that you’ve extracted the path, you can determine the 5 most popular pages on your website as follows:</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">awk '&#123;<span class="built_in">print</span> $<span class="number">7</span>&#125;' access.log | # Split by whitespace, <span class="number">7</span>th field is request <span class="built_in">path</span></span><br><span class="line">    sort                    | # Make occurrences of the same URL appear consecutively <span class="keyword">in</span> file</span><br><span class="line">    uniq -c                 | # <span class="built_in">Replace</span> consecutive occurrences of the same URL with a count</span><br><span class="line">    sort -rn                | # Sort by number of occurrences, descending</span><br><span class="line">    head -n <span class="number">5</span>                 # Output top <span class="number">5</span> URLs</span><br></pre></td></tr></table></figure><p>The output of that series of commands looks something like this:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">4189 /favicon.ico</span><br><span class="line">3631 /2013/05/24/improving-security-of-ssh-private-keys.html</span><br><span class="line">2124 /2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html</span><br><span class="line">1369 /</span><br><span class="line">915 /css/typography.css</span><br></pre></td></tr></table></figure><p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-04.png" alt=""></p><p>Although the above command line looks a bit obscure if you’re unfamiliar with Unix tools, it is incredibly powerful. It will process gigabytes of log files in a matter of seconds, and you can easily modify the analysis to suit your needs. For example, if you want to count top client IP addresses instead of top pages, change the awk argument to <code>{print $1}</code>. Many data analyses can be done in a few minutes using some combination of <code>awk</code>, <code>sed</code>, <code>grep</code>, <code>sort</code>, <code>uniq</code> and <code>xargs</code>, and they perform surprisingly well. This is no coincidence: it is a direct result of the design philosophy of Unix. </p><p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-05.png" alt=""></p><p>The Unix philosophy is a set of principles that emerged gradually during the design and implementation of Unix systems during the late 1960s and ‘70s. There are various interpretations of the Unix philosophy, but two points that particularly stand out were described by Doug McIlroy, Elliot Pinson and Berk Tague as follows in 1978:</p><ul><li>Make each program do one thing well. To do a new job, build afresh rather than complicate old programs by adding new <code>features.</code></li><li>Expect the output of every program to become the input to another, as yet unknown, program.</li></ul><p>These principles are the foundation for chaining together programs into pipelines that can accomplish complex processing tasks. The key idea here is that a program does not know or care where its input is coming from, or where its output is going to: it may be a file, or another program that’s part of the operating system, or another program written by someone else entirely.</p><p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-06.png" alt=""></p><p>The tools that come with the operating system are generic, but they are designed such that they can be <strong>composed</strong> together into larger programs that can perform application-specific tasks. The benefits that the designers of Unix derived from this design approach sound quite like the ideas of the Agile and DevOps movements that appeared decades later: scripting and automation, rapid prototyping, incremental iteration, being friendly to experimentation, and breaking down large projects into manageable chunks. Plus ça change.</p><p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-08.png" alt=""></p><p>When you join two commands with the pipe character in your shell, the shell starts both programs at the same time, and attaches the output of the first process to the second process’ input. This attachment mechanism uses the <code>pipe</code> syscall provided by the operating system. Note that this wiring is not done by the programs themselves, but by the shell – this allows them to be <a href="https://en.wikipedia.org/wiki/Loose_coupling" target="_blank" rel="noopener">loosely coupled</a>, and not worry about where their input is coming from, or where their output is going.</p><p>The pipe had been invented in 1964 by Doug McIlroy, who first described it like this in an internal Bell Labs memo: “We should have some ways of connecting programs like [a] garden hose – screw in another segment when it becomes necessary to massage data in another way.” Dennis Richie later wrote up his perspective on the memo.</p><p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-10.png" alt=""></p><p>They also realised early that the inter-process communication mechanism (pipes) can look very similar to the mechanism for reading and writing files. We now call this input redirection (using the contents of a file as input to a process) and output redirection (writing the output of a process to a file). The reason that Unix programs can be composed so flexibly is that they all conform to the same interface: most programs have one stream for input data (stdin) and two output streams (stdout for regular output data, and stderr for errors and diagnostic messages).</p><p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-12.png" alt=""></p><p>Programs may also do other things besides reading <code>stdin</code> and writing <code>stdout</code>, such as reading and writing files, communicating over the network, or drawing a graphical user interface. However, the <code>stdin</code>/<code>stdout</code> communication is considered to be the main way how data flows from one Unix tool to another. And the great thing about the <code>stdin</code>/<code>stdout</code> interface is that anyone can implement it easily, in any programming language. You can develop your own tool that conforms to this interface, and it will play nicely with all the standard tools that ship as part of the operating system.</p><p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-13.png" alt=""></p><p>For example, when analysing a web server log file, perhaps you want to find out how many visitors you have from each country. The log doesn’t tell you the country, but it does tell you the IP address, which you can translate into a country using an IP geolocation database. Such a database isn’t included with your operating system by default, but you can write your own tool that takes IP addresses on <code>stdin</code> and outputs country codes on <code>stdout</code>. Once you’ve written that tool, you can include it in the data processing pipeline we discussed previously, and it will work just fine. This may seem painfully obvious if you’ve been working with Unix for a while, but I’d like to emphasise how remarkable this is: your own code runs on equal terms with the tools provided by the operating system. Apps with graphical user interfaces or web apps cannot simply be extended and wired together like this. You can’t just pipe Gmail into a separate search engine app, and post results to a wiki. Today it’s an exception, not the norm, to have programs that work together as smoothly as Unix tools do.</p><p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-14.png" alt=""></p><p>Change of scene. Around the same time as Unix was being developed, the <code>relational data model</code> was proposed, which in time became SQL, and was implemented in many popular databases. Many databases actually run on Unix systems. Does that mean they also follow the Unix philosophy?</p><p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-15.png" alt=""></p><p>The dataflow in most database systems is very different from Unix tools. Rather than using <code>stdin</code> and <code>stdout</code> as communication channels, there is a <strong>database server</strong>, and several <strong>clients</strong>. The clients send queries to read or write data on the server, the server handles the queries and sends responses to the clients. This relationship is fundamentally asymmetric: <code>clients and servers are distinct roles</code>.</p><p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-16.png" alt=""></p><p>What about the <code>composability</code> and <code>extensibility</code> that we find in Unix systems? Clients can do anything they like (since they are application code), but database servers are mostly in the business of storing and retrieving your data. Letting you run arbitrary code is not their top priority. That said, many databases do provide some ways of extending the database server with your own code. For example, many relational databases let you write stored procedures in their own, rudimentary procedural language such as <code>PL/SQL</code> (and some let you run code in a general-purpose programming language such as JavaScript). However, the things you can do in stored procedures are limited. Other extension points in some databases are support for custom data types (this was one of the early design goals of Postgres), or pluggable storage engines. Essentially, these are plugin APIs: you can run your code in the database server, provided that your module adheres to a plugin API exposed by the database server for a particular purpose. This kind of extensibility is not the same as the arbitrary composability we saw with Unix tools. The plugin API is totally controlled by the database server, and subordinate to it. Your extension code is a guest in the database server’s home, not an equal partner.</p><p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-17.png" alt=""></p><p>A consequence of this design is that you can’t just pipe one database into another, even if they have the same data model. Nor can you insert your own code into the database’s internal processing pipelines (unless the server has specifically provided an extension point for you, such as triggers). I feel the design of databases is very self-centered. A database seems to assume that it’s the centre of your universe: the only place where you might want to store and query your data, the source of truth, and the destination for all queries. The closest you can get to piping data in and out of it is through bulk-loading and bulk-dumping (backup) operations, but those operations don’t really use any of the database’s features, such as query planning and indexes. If a database was designed according to the Unix philosophy, it would be based on a small number of core primitives that you could easily combine, extend and replace at will. Instead, databases are tremendously complicated, monolithic beasts. While Unix acknowledges that the operating system will never do everything you might want, and thus encourages you to extend it, databases try to implement all the features you may need in a single program.</p><p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-18.png" alt=""></p><p>Perhaps that design is fine in simple applications where a single database is indeed sufficient. However, many complex applications find that they have to use their data in various different ways: they need fast random access for OLTP, big sequential scans for analytics, inverted indexes for full-text search, graph indexes for connected data, machine learning systems for recommendation engines, a push mechanism for notifications, various different cached representations of the data for fast reads, and so on. A general-purpose database may try to do all of those things in one product (“one size fits all”), but in all likelihood it will not perform as well as a tool that is specialized for one particular purpose. In practice, you can often get the best results by combining various different data storage and indexing systems: for example, you may take the same data and store it in a relational database for random access, in Elasticsearch for full-text search, in a columnar format in Hadoop for analytics, and cached in a denormalized form in memcached. When you need to integrate different databases, the lack of Unix-style composability is a severe limitation. (I’ve done some work on piping data out of Postgres into other applications, but there’s still a long way to go before we can simply pipe any database into any other database.)</p><p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-19.png" alt=""></p><p>We said that Unix tools are <code>composable</code> because they all implement the same interface of <code>stdin</code>, <code>stdout</code> and <code>stderr</code> – and each of these is a file descriptor, i.e. a stream of bytes that you can read or write like a file. This interface is simple enough that anyone can easily implement it, but it is also powerful enough that you can use it for anything. Because all Unix tools implement the same interface, we call it a uniform interface. That’s why you can pipe the output of <code>gunzip</code> to <code>wc</code> without a second thought, even though the authors of those two tools probably never spoke to each other. It’s like lego bricks, which all implement the same pattern of knobbly bits and grooves, allowing you to stack any lego brick on any other, regardless of their shape, size or colour.</p><p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-20.png" alt=""></p><p>The uniform interface of file descriptors in Unix doesn’t just apply to the input and output of processes, but it’s a very broadly applied pattern. If you open a file on the filesystem, you get a file descriptor. Pipes and unix sockets provide file descriptors that are a communication channel to another process on the same machine. On Linux, the virtual files in <code>/dev</code> are the interfaces of device drivers, so you might be talking to a USB port or even a GPU. The virtual files in <code>/proc</code> are an API for the kernel, but since they’re exposed as files, you can access them with the same tools as regular files. Even a TCP connection to a process on another machine is a file descriptor, although the BSD sockets API (which is most commonly used to establish TCP connections) is arguably not as Unixy as it could be. Plan 9 shows that even the network could have been cleanly integrated into the same uniform interface. To a first approximation, everything on Unix is a file. This uniformity means the logic of Unix tools is separated from the wiring, making it more composable. sed doesn’t need to care whether it’s talking to a pipe to another process, or a socket, or a device driver, or a real file on the filesystem. It’s all the same.</p><p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-21.png" alt=""></p><p>A file is <strong>a stream of bytes</strong>, perhaps with an end-of-file (EOF) marker at some point, indicating that the stream has ended (a stream can be of arbitrary length, and a process may not know in advance how long its input is going to be). A few tools (e.g. <code>gzip</code>) operate purely on byte streams, and don’t care about the structure of the data. But most tools need to parse their input in order to do anything useful with it. For this, most Unix tools use ASCII, with each record on one line, and fields separated by tabs or spaces, or maybe commas. Files are totally obvious to us today, which shows that a byte stream turned out to be a good uniform interface. However, the implementors of Unix could have decided to do it very differently. For example, it could have been a function callback interface, using a schema to pass records from process to process. Or it could have been shared memory (like System V IPC or mmap, which came along later). Or it could have been a bit stream rather than a byte stream. In a sense, a byte stream is a lowest common denominator – the simplest possible interface. Everything can be expressed in terms of a stream of bytes, and it’s fairly agnostic to the transport medium (pipe from another process, file on disk, TCP connection, tape, etc). But this is also a disadvantage, as we shall discuss later.</p><p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-22.png" alt=""></p><p>We’ve seen that Unix developed some very good design principles for software development, and that databases have taken a very different route. I would love to see a future in which we can learn from both paths of development, and combine the best ideas from each. How can we make 21st-century data systems better by learning from the Unix philosophy? In the rest of this post I’d like to explore what it might look like if we bring the Unix philosophy to the world of databases.</p><p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-23.png" alt=""></p><p>First, let’s acknowledge that Unix is not perfect. Although I think the simple, uniform interface of byte streams was very successful at enabling an ecosystem of flexible, composable, powerful tools, Unix has some limitations:</p><ul><li><p>It’s designed for use on a single machine. As our applications get every more data and traffic, and have higher uptime requirements, moving to distributed systems is becoming increasingly inevitable. Although a TCP connection can be made to look somewhat like a file, I don’t think that’s the right answer: it only works if both sides of the connection are up, and it has somewhat messy edge case semantics. TCP is good, but by itself it’s too low-level to serve as a distributed pipe implementation.</p></li><li><p>A Unix pipe is designed to have a single sender process, and a single recipient. You can’t use pipes to send output to several processes, or to collect input from several processes. (You can branch a pipeline with <code>tee</code>, but a pipe itself is always one-to-one.)</p></li><li><p>ASCII text (or rather, UTF-8) is great for making data easily explorable, but it quickly gets messy. Every process needs to be set up with its own input parsing: first breaking the byte stream into records (usually separated by newline, though some advocate <code>0x1e</code>, the ASCII record separator). Then a record needs to be broken up into fields, like the <code>$7</code> in the <code>awk</code> example at the beginning. Separator characters that appear in the data need to be escaped somehow. Even a fairly simple tool like xargs has about half a dozen command-line options to specify how its input should be parsed. Text-based interfaces work tolerably well, but in retrospect, I am pretty sure that a richer data model with explicit schemas would have worked better.</p></li><li><p>Unix processes are generally assumed to be fairly short-running. For example, if a process in the middle of a pipeline crashes, there is no way for it to resume processing from its input pipe – the entire pipeline fails and must be re-run from scratch. That’s no problem if the commands run only for a few seconds, but if an application is expected to run continuously for years, better fault tolerance is needed.</p></li></ul><p>I think we can find a solution that overcomes these downsides, while retaining the Unix philosophy’s benefits. </p><p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-24.png" alt=""></p><p>The cool thing is that this solution already exists, and is implemented in Kafka and Samza, two open source projects that work together to provide distributed stream processing. As you probably already know from other posts on this blog, Kafka is a scalable distributed message broker, and Samza is a framework that lets you write code to consume and produce data streams.</p><p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-25.png" alt="streams"></p><p>In fact, when you look at it through the Unix lens, Kafka looks quite like the pipe that connects the output of one process to the input of another. And Samza looks quite like a standard library that helps you read <code>stdin</code> and write <code>stdout</code> (and a few helpful additions, such as a deployment mechanism, state management, metrics, and monitoring). The style of stream processing jobs that you can write with Kafka and Samza closely follows the Unix tradition of small, composable tools:</p><ul><li>In Unix, the operating system kernel provides the pipe, a transport mechanism for getting a stream of bytes from one process to another.</li><li>In stream processing, Kafka provides publish-subscribe streams, a transport mechanism for getting messages from one stream processing job to another.</li></ul><p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-26.png" alt=""></p><p>Kafka addresses the downsides of Unix pipes that we discussed previously:</p><ul><li><p>The single-machine limitation is lifted: Kafka itself is distributed by default, and any stream processors that use it can also be distributed across multiple machines.</p></li><li><p>A Unix pipe connects exactly one process output with exactly one process input, whereas a stream in Kafka can have many producers and many consumers. Many inputs is important for services that are distributed across multiple machines, and many outputs makes Kafka more like a broadcast channel. This is very useful, since it allows the same data stream to be consumed independently for several different purposes (including monitoring and audit purposes, which are often outside of the application itself). Kafka consumers can come and go without affecting other consumers.</p></li><li><p>Kafka also provides good fault tolerance: data is replicated across multiple Kafka nodes, so if one node fails, another node can automatically take over. If a stream processor node fails and is restarted, it can resume processing at its last checkpoint.</p></li><li><p>Rather than a stream of bytes, Kafka provides a stream of messages, which saves the first step of input parsing (breaking the stream of bytes into a sequence of records). Each message is just an array of bytes, so you can use your favourite serialisation format for individual messages: JSON, XML, Avro, Thrift or Protocol Buffers are all reasonable choices. It’s well worth standardising on one encoding, and Confluent provides particularly good schema management support for Avro. This allows applications to work with objects that have meaningful field names, and not have to worry about input parsing or output escaping. It also provides good support for schema evolution without breaking compatibility.</p></li></ul><p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-27.png" alt=""></p><p>There are a few more things that Kafka does differently from Unix pipes, which are worth calling out briefly:</p><ul><li><p>As mentioned, Unix pipes provide a byte stream, whereas Kafka provides a stream of messages. This is especially important if several processes are concurrently writing to the same stream: in a byte stream, the bytes from different writers can be interleaved, leading to an unparseable mess. Since messages are coarser-grained and self-contained, they can be safely interleaved, making it safe for multiple processes to concurrently write to the same stream.</p></li><li><p>Unix pipes are just a small in-memory buffer, whereas Kafka durably writes all messages to disk. In this regard, Kafka is less like a pipe, and more like one process writing to a temporary file, while several other processes continuously read that file using tail -f (each consumer tails the file independently). Kafka’s approach provides better fault tolerance, since it allows a consumer to fail and restart without skipping messages. Kafka automatically splits those ‘temporary’ files into segments and garbage-collects old segments on a configurable schedule.</p></li><li><p>In Unix, if the consuming process of a pipe is slow to read the data, the buffer fills up and the sending process is blocked from writing to the pipe. This is a kind of backpressure. In Kafka, the producer and consumer are more decoupled: a slow consumer has its input buffered, so it doesn’t slow down the producer or other consumers. As long as the buffer fits within Kafka’s available disk space, the slow consumer can catch up later. This makes the system less sensitive to individual slow components, and more robust overall.</p></li><li><p>A data stream in Kafka is called a <code>topic</code>, and you can refer to it by name (which makes it more like a Unix named pipe. A pipeline of Unix programs is usually started all at once, so the pipes normally don’t need explicit names. On the other hand, a long-running application usually has bits added, removed or replaced gradually over time, so you need names in order to tell the system what you want to connect to. Naming also helps with discovery and management.</p></li></ul><p>Despite those differences, I still think it makes sense to think of Kafka as Unix pipes for distributed data. For example, one thing they have in common is that Kafka keeps messages in a fixed order (like Unix pipes, which keep the byte stream in a fixed order). This is a very useful property for event log data: the order in which things happened is often meaningful and needs to be preserved. Other types of message broker, like AMQP and JMS, do not have this ordering property.</p><p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-28.png" alt=""></p><p>So we’ve got Unix tools and stream processors that look quite similar. Both read some input stream, modify or transform it in some way, and produce an output stream that is somehow derived from the input. Importantly, the processing does not modify the input itself: it remains immutable. If you run <code>awk</code> on some input file, the file remains unmodified (unless you explicitly choose to overwrite it). Also, most Unix tools are deterministic, i.e. if you give them the same input, they always produce the same output. This means you can re-run the same command as many times as you want, and gradually iterate your way towards a working program. It’s great for experimentation, because you can always go back to your original data if you mess up the processing. This deterministic and side-effect-free processing looks a lot like functional programming. That doesn’t mean you have to use a functional programming language like Haskell (although you’re welcome to do so if you want), but you still get many of the benefits of functional code.</p><p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-29.png" alt=""></p><p>The Unix-like design principles of Kafka enable building composable systems at a large scale. In a large organisation, different teams can each publish their data to Kafka. Each team can independently develop and maintain stream processing jobs that consume streams and produce new streams. Since a stream can have any number of independent consumers, no coordination is required to set up a new consumer. We’ve been calling this idea a stream data platform. In this kind of architecture, the data streams in Kafka act as the communication channel between different teams’ systems. Each team focusses on making their particular part of the system do one thing well. While Unix tools can be composed to accomplish a data processing task, distributed streaming systems can be composed to comprise the entire operation of a large organisation. A Unixy approach manages the complexity of a large system by encouraging loose coupling: thanks to the uniform interface of streams, different components can be developed and deployed independently. Thanks to the fault tolerance and buffering of the pipe (Kafka), when a problem occurs in one part of the system, it remains localised. And schema management allows changes to data structures to be made safely, so that each team can move fast without breaking things for other teams.</p><p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-30.png" alt=""></p><p>To wrap up this post, let’s consider a real-life example of how this works at LinkedIn. As you may know, companies can post their job openings on LinkedIn, and jobseekers can browse and apply for those jobs. What happens if a LinkedIn member (user) views one of those job postings? It’s very useful to know who has looked at which jobs, so the service that handles job views publishes an event to Kafka, saying something like “member 123 viewed job 456 at time 789”. Now that this information is in Kafka, it can be used for many good purposes:</p><ul><li><p>Monitoring systems: Companies pay LinkedIn to post their job openings, so it’s important that the site is working correctly. If the rate of job views drops unexpectedly, alarms should go off, because it indicates a problem that needs to be investigated.</p></li><li><p>Relevance and recommendations: It’s annoying for users to see the same thing over and over again, so it’s good to track how many times the users has seen a job posting, and feed that into the scoring process. Keeping track of who viewed what also allows for collaborative filtering recommendations (people who viewed X also viewed Y).</p></li><li><p>Preventing abuse: LinkedIn doesn’t want people to be able to scrape all the jobs, submit spam, or otherwise violate the terms of service. Knowing who is doing what is the first step towards detecting and blocking abuse.<br>Job poster analytics: The companies who post their job openings want to see stats (in the style of Google Analytics) about who is viewing their postings, for example so that they can test which wording attracts the best candidates.</p></li><li><p>Import into Hadoop and Data Warehouse: For LinkedIn’s internal business analytics, for senior management’s dashboards, for crunching numbers that are reported to Wall Street, for evaluating A/B tests, and so on.</p></li></ul><p>All of those systems are complex in their own right, and are maintained by different teams. Kafka provides a fault-tolerant, scalable implementation of a pipe. A stream data platform based on Kafka allows all of these various systems to be developed independently, and to be connected and composed in a robust way. If you enjoyed this post, you’ll love my book <a href="http://dataintensive.net/" target="_blank" rel="noopener">Designing Data-Intensive Applications</a>, published by O’Reilly. Thank you to Jay Kreps, Gwen Shapira, Michael Noll, Ewen Cheslack-Postava, Jason Gustafson, and Jeff Hartley for feedback on a draft of this post, and thanks also to Jay for providing the LinkedIn job view example.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Here’s another blog from Confluent’s website. What’s interesting about this article is that it compares the design philosophy between Unix and monolith database, and refers to Kafka as a distributed version of Unix pipeline. The latter point is exceptionally interesting and brings a whole new view of Kafka to me.&lt;/p&gt;
&lt;p&gt;After working fulltime in the IT industry for almost two years, I’ve seen repeatedly that design of modern system borrows ideas from the design of Linux/Unix. Besides Kafka and Unix pipeline, another example is the &lt;a href=&quot;http://docs.oracle.com/cd/E13924_01/coh.340/e13819/readthrough.htm&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Write Behind Caching Pattern&lt;/a&gt; in modern system actually refers to &lt;a href=&quot;https://www.thomas-krenn.com/en/wiki/Linux_Page_Cache_Basics&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Linux’s Page Cache&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href=&quot;http://www.confluent.io/blog/apache-kafka-samza-and-the-unix-philosophy-of-distributed-data/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Original Post&lt;/a&gt; by Martin Kleppmann. August 1, 2015.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&quot;http://www.confluent.io/wp-content/uploads/2016/08/unixphil-01.png&quot; alt=&quot;unixphil-01&quot;&gt;&lt;/p&gt;
&lt;p&gt;One of the things I realised while doing research for my book is that contemporary software engineering still has a lot to learn from the 1970s. As we’re in such a fast-moving field, we often have a tendency of dismissing older ideas as irrelevant – and consequently, we end up having to learn the same lessons over and over again, the hard way. Although computers have got faster, data has got bigger and requirements have become more complex, many old ideas are actually still highly relevant today. In this blog post I’d like to highlight one particular set of old ideas that I think deserves more attention today: &lt;code&gt;the Unix philosophy&lt;/code&gt;. I’ll show how this philosophy is very different from the design approach of mainstream databases, and explore what it would look like if modern distributed data systems learnt a thing or two from Unix.&lt;/p&gt;
&lt;p&gt;In particular, I’m going to argue that there are a lot of similarities between &lt;code&gt;Unix pipes&lt;/code&gt; and &lt;code&gt;Apache Kafka&lt;/code&gt;, and that this similarity enables good architectural styles for large-scale applications. But before we get into that, let me remind you of the foundations of the &lt;code&gt;Unix philosophy&lt;/code&gt;. You’ve probably seen the power of Unix tools before – but to get started, let me give you a concrete example that we can talk about. Say you have a web server that writes an entry to a log file every time it serves a request. For example, using the nginx default access log format, one line of the log might look like this:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;216.58.210.78 - - [27/Feb/2015:17:55:11 +0000] &amp;quot;GET /css/typography.css HTTP/1.1&amp;quot; 200 3377 &amp;quot;http://martin.kleppmann.com/&amp;quot; &amp;quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115 Safari/537.36&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;(That is actually one line, it’s only broken up into multiple lines here for readability.) This line of the log indicates that on 27 February 2015 at 17:55:11 UTC, the server received a request for the file /css/typography.css from the client IP address 216.58.210.78. It then goes on to note various other details, including the browser’s user-agent string. Various tools can take these log files and produce pretty reports about your website traffic, but for the sake of the exercise, let’s build our own, using basic Unix tools. Let’s determine the 5 most popular URLs on our website. To start with, we need to extract the path of the URL that was requested, for which we can use &lt;code&gt;awk&lt;/code&gt;. &lt;code&gt;awk&lt;/code&gt; doesn’t know about the format of nginx logs – it just treats the log file as text. By default, &lt;code&gt;awk&lt;/code&gt; takes one line of input at a time, splits it by whitespace, and makes the whitespace-separated components available as variables &lt;code&gt;$1&lt;/code&gt;, &lt;code&gt;$2&lt;/code&gt;, etc. In the nginx log example, the requested URL path is the seventh whitespace-separated component: &lt;/p&gt;
    
    </summary>
    
      <category term="distributed system" scheme="https://bowenli86.github.io/categories/distributed-system/"/>
    
      <category term="kafka" scheme="https://bowenli86.github.io/categories/distributed-system/kafka/"/>
    
    
      <category term="kafka" scheme="https://bowenli86.github.io/tags/kafka/"/>
    
      <category term="unix pipeline" scheme="https://bowenli86.github.io/tags/unix-pipeline/"/>
    
  </entry>
  
  <entry>
    <title>Throttling and Traffic Shaping</title>
    <link href="https://bowenli86.github.io/2016/10/05/system%20design/Throttling-and-Traffic-Shaping/"/>
    <id>https://bowenli86.github.io/2016/10/05/system design/Throttling-and-Traffic-Shaping/</id>
    <published>2016-10-06T06:49:42.000Z</published>
    <updated>2018-04-10T04:29:25.545Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Throttling-and-Traffic-Shaping"><a href="#Throttling-and-Traffic-Shaping" class="headerlink" title="Throttling and Traffic Shaping"></a>Throttling and Traffic Shaping</h2><blockquote><p>Throttling is, to control the consumption of resources used by an instance of an application, an individual tenant, or an entire service. </p></blockquote><p>Throttling can allow the system to continue to function and meet service level agreements, even when an increase in demand places an extreme load on resources.</p><p>The system could implement several throttling strategies, including:</p><ul><li><p><code>Service Metering</code> - that is, rejecting requests from an individual user who has already accessed system APIs more than n times per second over a given period of time. This requires that the system meters the use of resources for each tenant or user running an application.</p></li><li><p><code>Leaky bucket as a queue</code> - that is, using load leveling to smooth the volume of activity (also called <code>Queue-based Load Leveling pattern</code>). </p></li><li><p><code>Leaky bucket as a priority queue</code> - in a multitenant environment, <code>Leaky bucket as a queue</code> will reduce the performance for every tenant. </p><ul><li><p>If the system must support a mix of tenants with different SLAs, the work for high-value tenants might be performed immediately. </p></li><li><p>Requests for other tenants can be held back, and handled when the backlog has eased. Deferring operations being performed on behalf of lower priority applications or tenants. These operations can be suspended or curtailed, with an exception generated to inform the tenant that the system is busy and that the operation should be retried later.</p></li></ul></li><li><p><code>Token bucket</code> - that is, using tokens to limit the volume of activity</p></li><li><p><code>Disable non-critical services</code> Disabling or degrading the functionality of selected nonessential services so that essential services can run unimpeded with sufficient resources. For example, if the application is streaming video output, it could switch to a lower resolution.</p></li></ul><p>There are two most commonly used algorithms:</p><ol><li>Leaky Bucket for rate controll</li><li>Token Bucket for concurrency control</li></ol><a id="more"></a><h2 id="Leaky-Bucket-Algorithm"><a href="#Leaky-Bucket-Algorithm" class="headerlink" title="Leaky Bucket Algorithm"></a>Leaky Bucket Algorithm</h2><blockquote><p>Leaky Bucket Algorithm as a Queue, a.k.a Queue-Based Load Leveling Pattern</p></blockquote><h3 id="Statistics"><a href="#Statistics" class="headerlink" title="Statistics"></a>Statistics</h3><ul><li>What’s the max latency time a request can experience:<ul><li>V/(T - O) : V - the bucket’s volume, T - the request input rate, O - the output rate to process requests </li></ul></li></ul><h3 id="Pros"><a href="#Pros" class="headerlink" title="Pros:"></a>Pros:</h3><ul><li>Easy to implement</li><li>Best used to control processing rate</li></ul><h3 id="Cons"><a href="#Cons" class="headerlink" title="Cons:"></a>Cons:</h3><ul><li>Leaky bucket algorithms cannot effectively take advantages of resources. The leaking rate is fixed, so it cannot process a bulky traffic that exceeds its threshold even though there are plenty of resources. (Token Bucket Algorithm can do that)</li></ul><h3 id="Use-Case"><a href="#Use-Case" class="headerlink" title="Use Case:"></a>Use Case:</h3><ul><li>To address system callback flood</li></ul><h2 id="Token-Bucket-Algorithm"><a href="#Token-Bucket-Algorithm" class="headerlink" title="Token Bucket Algorithm"></a>Token Bucket Algorithm</h2><blockquote><p>What a token bucket limits is the traffic within a predefined time window. From the API level, the traffic that we always talk about is <code>QPS (Queries per sec)</code> and <code>TPS (Transactions per sec)</code>, and they are just the traffic in a 1-sec time window.</p></blockquote><p>The token bucket algorithm can be conceptually understood as follows:</p><ul><li>A token is added to the bucket every <code>1/r</code>  seconds.</li><li>The current number of tokens in the bucket is <code>c</code>. <ul><li>Strategy 1: The bucket can hold at the most <code>b</code> tokens. <code>c</code> &lt;= <code>b</code>. If a token arrives when the bucket is full, it is discarded.</li><li>Strategy 2: The bucket can hold unlimited tokens</li></ul></li><li>When n bytes/requests arrive, n tokens will be removed from the bucket, and the bytes/requests are sent to the network.</li></ul><p>There are four strategies for situations when <code>n</code> &gt; <code>c</code>.</p><ol><li>(simple and rudimentary) Consider the packet to be non-conformant, and discard the packet for now (availiable for request resubmit)</li><li>(naive and impractical) Have the packet wait until enough tokens accumulated<ul><li>simple waiting may block the workflow</li><li>maybe lower the priority of the packet, but it still possibily will not be processed forever</li></ul></li><li>(good for batch processing) Break the packet into smaller ones</li><li>(google guava supports) Grant the packet <code>n</code> tokens for now, but will have to delay its further requests in order to make it up. This requires a service metering to keep track of hosts’ token usage. Google Guava <code>RateLimiter</code> supports this feature.</li></ol><h3 id="Statistics-1"><a href="#Statistics-1" class="headerlink" title="Statistics"></a>Statistics</h3><ul><li>What’s the average allowed hit rate<ul><li>the rate to issue tokens <code>r</code></li></ul></li><li>How much is the max tolerable flood peak<ul><li>if the flood peak comes when the token buckets are full, the volume of max tolerable flood peak = V + r. V - token bucket size, r - the rate to issue tokens</li></ul></li></ul><h3 id="Pros-1"><a href="#Pros-1" class="headerlink" title="Pros:"></a>Pros:</h3><ul><li>Best used to control max concurrency</li></ul><h3 id="Cons-1"><a href="#Cons-1" class="headerlink" title="Cons:"></a>Cons:</h3><ul><li>It really depends on which above strategy you choose</li></ul><h3 id="Use-Case-1"><a href="#Use-Case-1" class="headerlink" title="Use Case:"></a>Use Case:</h3><ul><li>To address user requests flood</li></ul><h2 id="Difference-between-Leaky-Bucket-and-Token-Bucket"><a href="#Difference-between-Leaky-Bucket-and-Token-Bucket" class="headerlink" title="Difference between Leaky Bucket and Token Bucket"></a>Difference between Leaky Bucket and Token Bucket</h2><blockquote><p>Leaky bucket strictly forces a fixed maximum rate of processing. In some circumstances, leaky bucket cannot effectively use the internet resources, because it only grants fixed rate of processing and thus cannot handle traffic floods.</p></blockquote><blockquote><p>While token bucket not only limit the average rate of processing, it also allows systems to handle sudden flood peaks.</p></blockquote><p>Pratically, leaky bucket and token bucket algorithms are put together to provide a more powerful yet more flexible control over web traffic.</p><hr><p>References:</p><ul><li><p><a href="http://jm.taobao.org/2016/05/19/how-to-withstand-the-world-s-largest-traffic/" target="_blank" rel="noopener">http://jm.taobao.org/2016/05/19/how-to-withstand-the-world-s-largest-traffic/</a></p></li><li><p><a href="https://msdn.microsoft.com/en-us/library/dn589798.aspx" target="_blank" rel="noopener">https://msdn.microsoft.com/en-us/library/dn589798.aspx</a></p></li><li><p><a href="http://www.cnblogs.com/LBSer/p/4083131.html" target="_blank" rel="noopener">http://www.cnblogs.com/LBSer/p/4083131.html</a></p></li></ul><ul><li><a href="https://blog.jamespan.me/2015/10/19/traffic-shaping-with-token-bucket/" target="_blank" rel="noopener">https://blog.jamespan.me/2015/10/19/traffic-shaping-with-token-bucket/</a></li></ul><ul><li><a href="https://msdn.microsoft.com/en-us/library/dn589783.aspx" target="_blank" rel="noopener">https://msdn.microsoft.com/en-us/library/dn589783.aspx</a></li></ul><ul><li><a href="https://github.com/google/guava/blob/v18.0/guava/src/com/google/common/util/concurrent/RateLimiter.java" target="_blank" rel="noopener">https://github.com/google/guava/blob/v18.0/guava/src/com/google/common/util/concurrent/RateLimiter.java</a></li><li><a href="https://github.com/google/guava/blob/v18.0/guava/src/com/google/common/util/concurrent/SmoothRateLimiter.java" target="_blank" rel="noopener">https://github.com/google/guava/blob/v18.0/guava/src/com/google/common/util/concurrent/SmoothRateLimiter.java</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Throttling-and-Traffic-Shaping&quot;&gt;&lt;a href=&quot;#Throttling-and-Traffic-Shaping&quot; class=&quot;headerlink&quot; title=&quot;Throttling and Traffic Shaping&quot;&gt;&lt;/a&gt;Throttling and Traffic Shaping&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;Throttling is, to control the consumption of resources used by an instance of an application, an individual tenant, or an entire service. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Throttling can allow the system to continue to function and meet service level agreements, even when an increase in demand places an extreme load on resources.&lt;/p&gt;
&lt;p&gt;The system could implement several throttling strategies, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;Service Metering&lt;/code&gt; - that is, rejecting requests from an individual user who has already accessed system APIs more than n times per second over a given period of time. This requires that the system meters the use of resources for each tenant or user running an application.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;Leaky bucket as a queue&lt;/code&gt; - that is, using load leveling to smooth the volume of activity (also called &lt;code&gt;Queue-based Load Leveling pattern&lt;/code&gt;). &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;Leaky bucket as a priority queue&lt;/code&gt; - in a multitenant environment, &lt;code&gt;Leaky bucket as a queue&lt;/code&gt; will reduce the performance for every tenant. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If the system must support a mix of tenants with different SLAs, the work for high-value tenants might be performed immediately. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Requests for other tenants can be held back, and handled when the backlog has eased. Deferring operations being performed on behalf of lower priority applications or tenants. These operations can be suspended or curtailed, with an exception generated to inform the tenant that the system is busy and that the operation should be retried later.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;Token bucket&lt;/code&gt; - that is, using tokens to limit the volume of activity&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;Disable non-critical services&lt;/code&gt; Disabling or degrading the functionality of selected nonessential services so that essential services can run unimpeded with sufficient resources. For example, if the application is streaming video output, it could switch to a lower resolution.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are two most commonly used algorithms:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Leaky Bucket for rate controll&lt;/li&gt;
&lt;li&gt;Token Bucket for concurrency control&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="system design" scheme="https://bowenli86.github.io/categories/system-design/"/>
    
    
      <category term="throttling" scheme="https://bowenli86.github.io/tags/throttling/"/>
    
      <category term="traffic shaping" scheme="https://bowenli86.github.io/tags/traffic-shaping/"/>
    
      <category term="leaky bucket" scheme="https://bowenli86.github.io/tags/leaky-bucket/"/>
    
      <category term="token bucket" scheme="https://bowenli86.github.io/tags/token-bucket/"/>
    
  </entry>
  
  <entry>
    <title>Putting Apache Kafka To Use: A Practical Guide to Building a Stream Data Platform (Part 2)</title>
    <link href="https://bowenli86.github.io/2016/10/03/distributed%20system/kafka/Putting-Apache-Kafka-To-Use-A-Practical-Guide-to-Building-a-Stream-Data-Platform-Part-2/"/>
    <id>https://bowenli86.github.io/2016/10/03/distributed system/kafka/Putting-Apache-Kafka-To-Use-A-Practical-Guide-to-Building-a-Stream-Data-Platform-Part-2/</id>
    <published>2016-10-04T06:21:05.000Z</published>
    <updated>2018-04-10T04:29:25.523Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3></blockquote><blockquote><p>I’m preparing to start a new journey, which I’ll annouce soon. The new opportunity is about data.</p></blockquote><blockquote><p><code>Apache Kafka</code> has been a hot topic in the data field for a while, and, of course, I cannot taking on data problems without it. While learning and reading more about Kafka, I found <a href="http://www.confluent.io/blog/" target="_blank" rel="noopener">Conluent’s official tech blog</a> has been an amazingly useful place to find out materials I need - because the company is started by founders of Kafka in Linkedin.</p></blockquote><blockquote><p>Thus, I decided to repost a few great blogs from Confluent, add my own notes, and publish them here.</p></blockquote><blockquote><p>All posts are <a href="https://phoenixjiangnan.github.io/categories/distributed-system/kafka/" target="_blank" rel="noopener">here</a></p></blockquote><hr><blockquote><p>This post is Part II of a couple blogs from Concluent’s CEO Jay Kreps. Part II has been really amazing to present you an overall view of all major technical details of Kafka.</p></blockquote><blockquote><p><em><a href="http://www.confluent.io/blog/stream-data-platform-2/" target="_blank" rel="noopener">Original Post</a> from Jay Kreps. February 25, 2015.</em></p></blockquote><hr><p><img src="http://cdn2.hubspot.net/hub/540072/file-3062870613-png/blog-files/data-systems-sdp.png" alt=""></p><p>This is the second part of our guide on streaming data and Apache Kafka. In part one I talked about the uses for real-time data streams and explained our idea of a stream data platform. The remainder of this guide will contain specific advice on how to go about building a stream data platform in your organization.</p><p>This advice is drawn from our experience building and implementing Kafka at LinkedIn and rolling it out across all the data types and systems there. It also comes from four years working with tech companies in Silicon Valley to build Kafka-based stream data platforms in their organizations.</p><p>This is meant to be a living document. As we learn new techniques, or new tools become available, I’ll update it.</p><h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><p>Much of the advice in this guide covers techniques that will scale to hundreds or thousands of well formed data streams. No one starts with that, of course. Usually you start with one or two trial applications, often ones that have scalability requirements that make other systems less suitable. Even in this kind of limited deployment, though, the techniques described in this guide will help you to start off with good practices, which is critical as your usage expands.</p><p>Starting with something more limited is good, it let’s you get a hands on feel for what works and what doesn’t, so that, when broader adoption comes, you are well prepared for it.</p><h2 id="Recommendations"><a href="#Recommendations" class="headerlink" title="Recommendations"></a>Recommendations</h2><p>I’ll give a set of general recommendations for streaming data and Kafka and then discuss some specifics of different types of data.</p><h3 id="1-Limit-The-Number-of-Clusters"><a href="#1-Limit-The-Number-of-Clusters" class="headerlink" title="1. Limit The Number of Clusters"></a>1. Limit The Number of Clusters</h3><p>In early experimentation phases it is normal to end up with a few different Kafka clusters as adoption occurs organically in different parts of the organization.</p><a id="more"></a><blockquote><p>However part of the promise of this approach to data management is having <code>a central repository</code> with the full set of data streams your organization generates. This works best when data is all in the same place.</p></blockquote><p>This is similar to the recommendations given in data warehousing where the goal is to concentrate data in a central warehouse for simplicity and to enable uses that join together multiple data sources.</p><p>Likewise we have seen that storing stream data in the fewest number of Kafka clusters feasible has a great deal of value in simplifying system architecture. This means fewer integration points for data consumers, fewer things to operate, lower incremental cost for adding new applications, and makes it easier to reason about data flow.</p><p><code>The fewest number of clusters may not be one cluster.</code> There are several reasons to end up with multiple clusters:</p><blockquote><ul><li>To keep activity local to a datacenter. As described later we recommend that all applications connect to a cluster in their local datacenter with mirroring between data centers done between these local data centers.</li><li>For security reasons. Kafka does not yet have security controls which often means implementing network level security and physically segregating data types.</li><li>For SLA control. Kafka has some multi-tenancy features but this story is not complete.</li></ul></blockquote><p>Our job as Kafka engineers is to remove the restrictions that force new cluster creation, but until we’ve done that beware of the above limitations.</p><h3 id="2-Pick-A-Single-Data-Format"><a href="#2-Pick-A-Single-Data-Format" class="headerlink" title="2. Pick A Single Data Format"></a>2. Pick A Single Data Format</h3><p>Apache Kafka does not enforce any particular format for event data beyond a simple key/value model. It will work equally well with <code>XML</code>, <code>JSON</code>, or <code>Avro</code>. Our general philosophy is that it is not the role of data infrastructure systems to enforce this kind of policy, that is really an organizational choice.</p><blockquote><p>However, though your infrastructure shouldn’t make this choice for you, you should make a choice! Mandating a single, company-wide data format for events is critical. The overall simplicity of integration comes not only from having stream data in a single system—Kafka—but also by making all data look similar and follow similar conventions. If each individual or application chooses a representation of their own preference—say some use JSON, others XML, and others CSV—the result is that any system or process which uses multiple data streams has to munge and understand each of these. Local optimization—choosing your favorite format for data you produce—leads to huge global sub-optimization since now each system needs to write N adaptors, one for each format it wants to ingest.</p></blockquote><p>An analogy borrowed from a friend can help to explain why such a mundane thing as data format is worth fussing about. One of the few great successes in the integration of applications is the Unix command line tools. The Unix toolset all works together reasonably well despite the fact that the individual commands were written by different people over a long period of time. The standard for integrating these tools is newline delimited ASCII text, these can be strung together with a <code>|</code> which transmits a record stream using standard input and standard output. The stream data platform is actually not that far removed from this itself. It is a kind of modern Unix pipe implemented at the data center level and designed to support our new world of distributed, continually running programs.</p><p>Though surely newline delimited text is an inadequate format to standardize on these days, imagine how useless the Unix toolchain would be if each tool invented its own format: you would have to translate between formats every time you wanted to pipe one command to another.</p><p>Picking a single format, making sure that all tools and integrations use it, and holding firm on the use of this format across the board, is likely the single most important thing to do in the early implementation of your stream data platform. This stuff is fairly new, so if you are adopting it now sticking to the simplicity of a uniform data format should be easy.</p><h3 id="3-The-Mathematics-of-Simplicity"><a href="#3-The-Mathematics-of-Simplicity" class="headerlink" title="3. The Mathematics of Simplicity"></a>3. The Mathematics of Simplicity</h3><p>Together these two recommendations—<code>limiting the number of clusters</code> and <code>standardizing on a single data format</code>—bring a very real kind of simplicity to data flow in an organization.</p><p>By centralizing on a single infrastructure platform for data exchange which provides a single abstraction—the real-time stream—we dramatically simplify the data flow picture. Connecting all systems directly would look something like this:</p><p><img src="http://cdn2.hubspot.net/hub/540072/file-3062870603-png/blog-files/data-systems-point-to-point.png" alt="data-systems-point-to-point"></p><p>Whereas having this central stream data platform looks something like this:</p><p><img src="http://cdn2.hubspot.net/hub/540072/file-3062870613-png/blog-files/data-systems-sdp.png" alt="data-systems-sdp"></p><p>This doesn’t just look simpler. In the first picture we are on a path to build two pipelines for data for each pair of systems, whereas in the second we are just building an input and output connector for each system to the stream data pipeline. If we have 10 systems to fully integrate this is the difference between 200 pipelines and 20 (if each system did both input and output).</p><p>But this is not just about systems and pipelines. Data also has to be adapted between systems. Relational databases have one data model, Hadoop another, and things like document stores still others. Providing a pipeline for raw bytes between systems would not really reduce complexity if each system produced and consumed in its own format. We would be left with a Tower of Babel where the RDBMS needs a different format plug-in for each possible source system. Instead, by having a single data format in our stream data platform we need only adapt each system to this data format and we limit the format conversions in the same way we did the number of systems.</p><p>This is not to imply that we will never want to process or transform data as it flows between systems—that, after all, is exactly what stream processing is all about—but we want to eliminate low-value syntactic conversions. Semantic changes, enrichment, and filtering, to produce derived data streams will still be quite important.</p><h3 id="4-Use-Avro-as-Your-Data-Format"><a href="#4-Use-Avro-as-Your-Data-Format" class="headerlink" title="4. Use Avro as Your Data Format"></a>4. Use Avro as Your Data Format</h3><p>Any format, be it XML, JSON, or ASN.1, provided it is used consistently across the board, is better than a mishmash of ad hoc choices.</p><blockquote><p>But if you are starting fresh with Kafka, you should pick the best format to standardize on. There are many criteria here: <code>efficiency</code>, <code>ease of use</code>, <code>support in different programming languages</code>, and so on. In our own use, and in working with a few dozen companies, we have found <code>Apache Avro</code> to be easily the most successful format for stream data.</p></blockquote><p><code>Avro</code> has a <code>JSON</code> like data model, but can be represented as either <code>JSON</code> or in a compact binary form. It comes with a very sophisticated schema description language that describes data.</p><p>We think <code>Avro</code> is the best choice for a number of reasons:</p><blockquote><ul><li>It has a direct mapping to and from JSON</li><li>It has a very compact format. The bulk of JSON, repeating every field name with every single record, is what makes JSON inefficient for high-volume usage.</li><li>It is very fast.</li><li>It has great bindings for a wide variety of programming languages so you can generate Java objects that make working with event data easier, but it does not require code generation so tools can be written generically for any data stream.</li><li>It has a rich, extensible schema language defined in pure JSON</li><li>It has the best notion of compatibility for evolving your data over time.</li></ul></blockquote><p>Though it may seem like a minor thing handling this kind of metadata turns out to be one of the most critical and least appreciated aspects in keeping data high quality and easily useable at organizational scale.</p><p>One of the critical features of <code>Avro</code> is the ability to define a schema for your data. For example an event that represents the sale of a product might look like this:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;time&quot;: 1424849130111,</span><br><span class="line">  &quot;customer_id&quot;: 1234,</span><br><span class="line">  &quot;product_id&quot;: 5678,</span><br><span class="line">  &quot;quantity&quot;:3,</span><br><span class="line">  &quot;payment_type&quot;: &quot;mastercard&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>It might have a schema like this that defines these five fields:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;type&quot;: &quot;record&quot;,</span><br><span class="line">  &quot;doc&quot;:&quot;This event records the sale of a product&quot;,</span><br><span class="line">  &quot;name&quot;: &quot;ProductSaleEvent&quot;,</span><br><span class="line">  &quot;fields&quot; : [</span><br><span class="line">    &#123;&quot;name&quot;:&quot;time&quot;, &quot;type&quot;:&quot;long&quot;, &quot;doc&quot;:&quot;The time of the purchase&quot;&#125;,</span><br><span class="line">    &#123;&quot;name&quot;:&quot;customer_id&quot;, &quot;type&quot;:&quot;long&quot;, &quot;doc&quot;:&quot;The customer&quot;&#125;,</span><br><span class="line">    &#123;&quot;name&quot;:&quot;product_id&quot;, &quot;type&quot;:&quot;long&quot;, &quot;doc&quot;:&quot;The product&quot;&#125;,</span><br><span class="line">    &#123;&quot;name&quot;:&quot;quantity&quot;, &quot;type&quot;:&quot;int&quot;&#125;,</span><br><span class="line">    &#123;&quot;name&quot;:&quot;payment&quot;,</span><br><span class="line">     &quot;type&quot;:&#123;&quot;type&quot;:&quot;enum&quot;,</span><br><span class="line">     &quot;name&quot;:&quot;payment_types&quot;,</span><br><span class="line">             &quot;symbols&quot;:[&quot;cash&quot;,&quot;mastercard&quot;,&quot;visa&quot;]&#125;,</span><br><span class="line">     &quot;doc&quot;:&quot;The method of payment&quot;&#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>A real event, of course, would probably have more fields and hopefully better doc strings, but this gives their flavor.</p><blockquote><p>Here is how these schemas will be put to use. You will associate a schema like this with each Kafka topic. You can think of the schema much like the schema of a relational database table, giving the requirements for data that is produced into the topic as well as giving instructions on how to interpret data read from the topic.</p></blockquote><p>The schemas end up serving a number of critical purposes:</p><ol><li>They let the producers or consumers of data streams know the right fields are need in an event and what type each field is.</li><li>They document the usage of the event and the meaning of each field in the “doc” fields.</li><li>They protect downstream data consumers from malformed data, as only valid data will be permitted in the topic.</li></ol><p>The value of schemas is something that doesn’t become obvious when there is only one topic of data and perhaps a single writer and maybe a proof-of-concept reader. However when critical data streams are flowing through the pipeline and dozens or hundreds of systems depend on this, simple tools for reasoning about data have enormous impact.</p><p>But first, you may be asking why we need schemas at all? Isn’t the modern world of big data all about unstructured data, dumped in whatever form is convenient, and parsed later when it is queried?</p><h4 id="The-Need-For-Schemas"><a href="#The-Need-For-Schemas" class="headerlink" title="The Need For Schemas"></a>The Need For Schemas</h4><p>I will argue that schemas—when done right—can be a huge boon, keep your data clean, and make everyone more agile. Much of the reaction to schemas comes from two factors</p><blockquote><ul><li>historical limitations in relational databases that make schema changes difficult</li><li>the immaturity of much of the modern distributed infrastructure which simply hasn’t had the time yet to get to the semantic layer of modeling done.</li></ul></blockquote><p>Here is the case for schemas, point-by-point.</p><h5 id="1-Robustness"><a href="#1-Robustness" class="headerlink" title="1. Robustness"></a>1. Robustness</h5><p>One of the primary advantages of this type of architecture where data is modeled as streams is that applications are decoupled. Applications produce a stream of events capturing what occurred without knowledge of which things subscribe to these streams.</p><p>But in such a world, how can you reason about the correctness of the data? It isn’t feasible to test each application that produces a type of data against each thing that uses that data, many of these things may be off in Hadoop or in other teams with little communication. Testing all combinations is infeasible. In the absence of any real schema, new producers to a data stream will do their best to imitate existing data but jarring inconsistencies arise—certain magical string constants aren’t copied consistently, important fields are omitted, and so on.</p><h5 id="2-Clarity-and-Semantics"><a href="#2-Clarity-and-Semantics" class="headerlink" title="2. Clarity and Semantics"></a>2. Clarity and Semantics</h5><p>Worse, the actual meaning of the data becomes obscure and often misunderstood by different applications because there is no real canonical documentation for the meaning of the fields. One person interprets a field one way and populates it accordingly and another interprets it differently.</p><p>Invariably you end up with a sort of informal plain english “schema” passed around between users of the data via wiki or over email which is then promptly lost or obsoleted by changes that don’t update this informal definition. We found this lack of documentation lead to people guessing as to the meaning of fields, which inevitably leads to bugs and incorrect data analysis when these guesses are wrong.</p><p>Keeping an up-to-date doc string for each field means there is always a canonical definition of what that value means.</p><h5 id="3-Compatibility"><a href="#3-Compatibility" class="headerlink" title="3. Compatibility"></a>3. Compatibility</h5><p>Schemas also help solve one of the hardest problems in organization-<code>wide data flow: modeling and handling change in data format</code>. Schema definitions just capture a point in time, but your data needs to evolve with your business and with your code. There will always be new fields, changes in how data is represented, or new data streams.</p><p>This is a problem that databases mostly ignore. A database table has a single schema for all it’s rows. But this kind of rigid definition won’t work if you are writing many applications that all change at different times and evolve the schema of shared data streams. If you have dozens of applications all using a central data stream they simply cannot all update at once.</p><p>And managing these changes gets more complicated as more people use the data and the number of different data streams grows. Surely adding a new field is a safe change, but is removing a field? What about renaming an existing field? What about changing a field from a string to a number?</p><p>These problems become particularly serious because of Hadoop or any other system that stores the events. Hadoop has the ability to load data “as is” either with <code>Avro</code> or in a columnar file format like Parquet or ORC. Thus the loading of data from data streams can be made quite automatic, but what happens when there is a format change? Do you need to re-process all your historical data to convert it to the new format? That can be quite a large effort when hundreds of TBs of data are involved. How do you know if a given change will require this? Do you guess and wait to see what will break when the change goes to production?</p><p>Schemas make it possible for systems with flexible data format like Hadoop or Cassandra to track upstream data changes and simply propagate these changes into their own storage without expensive reprocessing. Schemas give a mechanism for reasoning about which format changes will be compatible and (hence won’t require reprocessing) and which won’t.</p><h5 id="4-Schemas-are-a-Conversation"><a href="#4-Schemas-are-a-Conversation" class="headerlink" title="4. Schemas are a Conversation"></a>4. Schemas are a Conversation</h5><p>I actually buy many arguments for flexible types. Dynamically typed languages have an important role to play. And arguably databases, when used by a single application in a service-oriented fashion, don’t need to enforce a schema, since, after all, the service that owns the data is the real “schema” enforcer to the rest of the organization.</p><p><code>However data streams are different; they are a broadcast channel.</code> Unlike an application’s database, the writer of the data is, almost by definition, not the reader. And worse, there are many readers, often in different parts of the organization. These two groups of people, the writers and the readers, need a concrete way to describe the data that will be exchanged between them and schemas provide exactly this.</p><h5 id="5-Schemas-Eliminate-The-Manual-Labor-of-Data-Science"><a href="#5-Schemas-Eliminate-The-Manual-Labor-of-Data-Science" class="headerlink" title="5. Schemas Eliminate The Manual Labor of Data Science"></a>5. Schemas Eliminate The Manual Labor of Data Science</h5><p>It is almost a truism that data science, which I am using as a short-hand here for “putting data to effective use”, is 80% parsing, validation, and low-level data munging. Data scientists complain that their training spent too much time on statistics and algorithms and too little on regular expressions, xml parsing, and practical data munging skills. This is quite true in most organizations, but it is somewhat disappointing that there are people with PhDs in Physics spending their time trying to regular-expression date fields out of mis-formatted CSV data (that inevitably has commas inside the fields themselves).</p><p>This problem is particularly silly because the nonsense data isn’t forced upon us by some law of physics, this data doesn’t just arise out of nature. Whenever you have one team whose job is to parse out garbage data formats and try to munge together inconsistent inputs into something that can be analyzed, there is another corresponding team whose job is to generate that garbage data. And once a few people have built complex processes to parse the garbage, that garbage format will be enshrined forever and never changed. Had these two teams talked about what data was needed for analysis and what data was available for capture, the entire problem could have been prevented.</p><p>The advantage isn’t limited to parsing. Much of what is done in this kind of data wrangling is munging disparate representations of data from various systems to look the same. It will turn out that similar business activities are captured in dramatically different ways in different parts of the same business. Building post hoc transformations can attempt to coerce these to look similar enough to perform analysis. However the same thing is possible at data capture time by just defining an enterprise-wide schema for common activities. If sales occur in 14 different business units it is worth figuring out if there is some commonality among these that can be enforced so that analysis can be done over all sales without post-processing. Schemas won’t automatically enforce this kind of thoughtful data modeling but they do give a tool by which you can enforce a standard like this.</p><h4 id="At-LinkedIn"><a href="#At-LinkedIn" class="headerlink" title="At LinkedIn"></a>At LinkedIn</h4><p>We put this idea of schemafied event data into practice at large scale at LinkedIn. User activity events, metrics data, stream processing output, data computed in Hadoop, and database changes were all represented as streams of <code>Avro</code> events.</p><p>These events were automatically loaded into <code>Hadoop</code>. When a new Kafka topic was added that data would automatically flow into <code>Hadoop</code> and a corresponding <code>Hive</code> table would be created using the event schema. When the schema evolved that metadata was propagated into <code>Hadoop</code>. When someone wanted to create a new data stream, or evolve the schema for an existing one, the schema for that stream would undergo a quick review by a group of people who cared about data quality. This review would ensure this stream didn’t duplicate an existing event and that things like dates and field names followed the same conventions, and so on. Once the schema change was reviewed it would automatically flow throughout the system. This leads to a much more consistent, structured representation of data throughout the organization.</p><p>Other companies we have worked with have largely come to the same conclusion. Many started with loosely structured <code>JSON</code> data streams with no schemas or contracts as these were the easiest to implement. But over time almost all have realized that this loose definition simply doesn’t scale beyond a dozen people and that some kind of stronger metadata is needed to preserve data quality.</p><h4 id="Back-to-Avro"><a href="#Back-to-Avro" class="headerlink" title="Back to Avro"></a>Back to Avro</h4><blockquote><p>Okay that concludes the case for schemas. We chose <code>Avro</code> as a schema representation language after evaluating all the common options—<code>JSON</code>, <code>XML</code>, <code>Thrift</code>, <code>protocol buffers</code>, etc. We recommend it because it is the best thought-out of these for this purpose. It has a pure JSON representation for readability but also a binary representation for efficient storage. It has an exact compatibility model that enables the kind of compatibility checks described above. It’s data model maps well to <code>Hadoop</code> data formats and <code>Hive</code> as well as to other data systems. It also has bindings to all the common programming languages which makes it convenient to use programmatically.</p></blockquote><p>Good overviews of Avro can be found <a href="http://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html" target="_blank" rel="noopener">here</a> and <a href="http://radar.oreilly.com/2014/11/the-problem-of-managing-schemas.html" target="_blank" rel="noopener">here</a>.</p><p>We have built tools for implementing <code>Avro</code> with Kafka or other systems as part of the <a href="http://confluent.io/product" target="_blank" rel="noopener">Confluent Platform</a>, you can read more about this schema support <a href="http://confluent.io/docs/current/schema-registry/docs/index.html" target="_blank" rel="noopener">here</a>.</p><h4 id="Effective-Avro"><a href="#Effective-Avro" class="headerlink" title="Effective Avro"></a>Effective Avro</h4><p>Here are some recommendations specific to <code>Avro</code>:</p><blockquote><ul><li>Use enumerated values whenever possible instead of magic strings. Avro allows specifying the set of values that can be used in the schema as an enumeration. This avoids typos in data producer code making its way into the production data set that will be recorded for all time.</li><li>Require documentation for all fields. Even seemingly obvious fields often have non-obvious details. Try to get them all written down in the schema so that anyone who needs to really understand the meaning of the field need not go any further.</li><li>Avoid non-trivial union types and recursive types. These are Avro features that map poorly to most other systems. Since our goal is an intermediate format that maps well to other systems we want to avoid any overly advanced features.</li><li>Enforce reasonable schema and field naming conventions. Since these schemas will map into Hadoop having common fields like customer_id named the same across events will be very helpful in making sure that joins between these are easy to do. A reasonable scheme might be something like PageViewEvent, OrderEvent, ApplicationBounceEvent, etc.</li></ul></blockquote><h3 id="5-Share-Event-Schemas"><a href="#5-Share-Event-Schemas" class="headerlink" title="5. Share Event Schemas"></a>5. Share Event Schemas</h3><p>Whenever you see a common activity across multiple systems, try to use a common schema for this activity. Doing so often requires a small amount of thought, but it saves a lot of work in using the data.</p><blockquote><p>An example of this that is common to all businesses is application errors. Application errors can generally be modeled in a fairly general way (say an error has a stack trace, an application name, an error message, and so on) and doing so lets the ErrorEvent stream capture the full stream of errors across the company. This means tools that process, alert, analyze, or report on errors will automatically extend to each new system that emits data to this stream. Had each application derived it’s own data format for errors than each error consumer would need to somehow munge all the disparate error streams into a common format for processing or analytics.</p></blockquote><p>This experience is common. Any time you can make similar things look similar by data modeling it is almost free to do so—you just need a schema—but every time you do this in post processing you need to maintain code to do this post-processing indefinitely.</p><blockquote><p>A corollary to this is to avoid system or application names in event names. When adding event capture to a system, named, say, “CRS”, there is a tendency to name every event with CRS as part of the name (“CRSOrderEvent”, “CRSResendEvent”, etc). However our experience was that systems tend to get replaced, while many many applications will end up feeding off the event stream. If you put the system name in the event stream name the source system can never change, or the new replacement system will have to produce data with the old name. Instead, name events in a system and application agnostic way—just use the high-level business activity they represent. So if CRS is an order management system then just <code>OrderEvent</code> is sufficient.</p></blockquote><h2 id="Pure-Event-Streams"><a href="#Pure-Event-Streams" class="headerlink" title="Pure Event Streams"></a>Pure Event Streams</h2><blockquote><p>Kafka’s data model is built to represent event streams.</p></blockquote><blockquote><p>A stream in Kafka is modeled by a <code>topic</code>, which is the logical name given to that data. Each message has a key, which is used to partition data over the cluster as well as a body which would contain the <code>Avro</code> record data (or whichever format you have chosen).</p></blockquote><p>Kafka maintains a configurable history of the stream. This can be managed with an SLA (e.g. retain 7 days) or by size (e.g retain 100 GB) or by key (e.g. retain at least that last update for each key).</p><p>Let’s begin with pure event data—the activities taking place inside the company. In a web company these might be clicks, impression, and various user actions. FedEx might have package deliveries, package pick ups, driver positions, notifications, transfers and so on.</p><p>These type of events can be represented with a single logical stream per action type. <code>For simplicity I recommend naming the Avro schema and the topic the same thing, e.g. PageViewEvent.</code></p><blockquote><p>If the event has a natural primary key you can use that to partition data in Kafka, otherwise the Kafka client will automatically load balance data for you.</p></blockquote><blockquote><p>Pure event streams will always be retained by size or time. You can choose to keep a month or 100GB per stream or whatever policy you define.</p></blockquote><p>We experimented at various times with mixing multiple events in a single topic and found this generally lead to undue complexity. <strong>Instead, give each event it’s own topic and consumers can always subscribe to multiple such topics to get a mixed feed when they want that.</strong></p><p>By having a single schema for each topic you will have a much easier time mapping a topic to a <code>Hive</code> table in <code>Hadoop</code>, a database table in a relational DB or other structured stores.</p><h2 id="Application-Logs"><a href="#Application-Logs" class="headerlink" title="Application Logs"></a>Application Logs</h2><p>The term “logs” is somewhat undefined. It sometimes means error messages, stack traces, and warnings in semi-formated english such as a server might record in the course of request processing. It sometimes means fairly structured request logs like might come out of Apache HTTPD. It sometimes means event data which might be dumped to a log file.</p><blockquote><p>For this section I will use “logs” to refer to the semi-structured application logs. Structured logs like request logs and other activity or event data should just be treated like any other event as described and should have a schema per activity that capture exactly the fields that make up that event.</p></blockquote><p>However there can be some value in capturing application logs in Kafka as well.</p><blockquote><p>At LinkedIn, all application logs were published to Kafka via a custom log4j appender for Java. These were loaded into Hadoop for batch analysis as well as being delivered to real-time tools that would subscribe to the stream of application logs for reporting on sudden error spikes or changes after new code was pushed. These errors were also joined back to the stream of service requests in a stream processing system so we could get a wholistic picture of utilization, latency, errors, and the call patterns amongst our services.</p></blockquote><h2 id="System-Metrics"><a href="#System-Metrics" class="headerlink" title="System Metrics"></a>System Metrics</h2><p>We also published a stream of statistics about applications and servers. These had a common format across all applications. They captured things like unix performance statistics (the kind of I/O and CPU load you would get out of iostat or top) as well as application defined gauges and counters captured using things like JMX.</p><p>This all went into a central feed of monitoring statistics that fed the company wide monitoring platform. Any new system could integrate by publishing its statistics, and all statistics were available in a company-wide monitoring store.</p><h2 id="Derived-Streams"><a href="#Derived-Streams" class="headerlink" title="Derived Streams"></a>Derived Streams</h2><p>Mostly so far we have talked about producing streams of events into Kafka. These events are things happening in applications or data systems. I’ll call these “primary” data streams.</p><p>However there is another type of data stream, a <code>derived</code> stream. These are streams that were computed off other data streams. This computation could be done in real-time as events occurred, either in an application or in a stream processing system, or it could be done periodically in Hadoop. These derived streams often do some kind of enrichment, say adding on new attributes not present in the original event.</p><p>Derived streams require no particular handling. They can be computed using simple programs that directly consume from Kafka and write back derived results or they can be computed using a stream processing system. Regardless which route is taken the output stream is just another Kafka topic so the consumer of the data need not be concerned with the mechanism used to produce it. A batch computed stream from Hadoop will look no different from a stream coming from a stream processing system, except that it will be higher latency.</p><h2 id="Hadoop-Data-Loads"><a href="#Hadoop-Data-Loads" class="headerlink" title="Hadoop Data Loads"></a>Hadoop Data Loads</h2><p>There are many ways to load data from Kafka into Hadoop and there are many aspects of doing this well.</p><blockquote><p>One of the most critical is doing it in a fully automated way. Since Hadoop will likely want to load data from all the data streams, you don’t want to be doing any custom set-up or mappings between your Kafka topics and your Hadoop data sets and Hive tables.</p></blockquote><p>We have packaged a simple system for doing this called <code>Camus</code> that came out of LinkedIn. It is described in more detail <a href="http://confluent.io/docs/current/camus/docs/index.html" target="_blank" rel="noopener">here</a>.</p><h2 id="Hadoop-Data-Publishing"><a href="#Hadoop-Data-Publishing" class="headerlink" title="Hadoop Data Publishing"></a>Hadoop Data Publishing</h2><p>The opposite of loading data into Hadoop is just as common. After all, the purpose of Hadoop is to act as a computational engine, and whatever it computes must go somewhere for serving. Often this piping can be quite complex as the Hadoop cluster may not be physically co-located with the serving system, and even if it is you often don’t want Hadoop writing directly to a database used for serving live requests as it will easily overwhelm such a system.</p><p>So the stream data platform is a great place to publish these derived streams from Hadoop. The stream data platform can handle the distribution of data across data centers. As far as the recipient is concerned this is just another stream which happens to receive updates only periodically.</p><p>This allows the same plugins that load data from a stream processor to also be used for loading Hadoop data. So an analytical job can begin its life in Hive and later migrate to a lower latency stream processing platform without needing to rewrite the serving layer.</p><h2 id="Database-Changes"><a href="#Database-Changes" class="headerlink" title="Database Changes"></a>Database Changes</h2><p>Database changes require some particular discussion. Database data is somewhat different from pure event streams in that it models updates—that is, rows that change.</p><p>The first and arguably most important issue is how changes are captured from the database. There are two common methods for doing this:</p><ol><li>Polling for changes</li><li>Direct log integration with the database</li></ol><p>Polling for changes requires little integration with the database so it is the easiest to implement. Polling requires some kind of last modified timestamp that can be used to detect new values so it requires some co-operation from the schema. There are also a number of gotchas in implementing correct change capture by polling. First, long running transactions can lead to rows that commit out of timestamp order when using simple time; this means that rows can appear in the near past. Many databases support some kind of logical change number that can help alleviate this problem. This method also doesn’t guarantee that every change is captured, when multiple updates occur on a single row in between polling intervals only the last of these is delivered. It also doesn’t capture deleted rows.</p><p>All the limitations of polling are fixed by direct integration with the database log, but the mechanism for integration is very database specific. MySQL has a binlog, Postgres has logical replication, Oracle has a number of products including Change Capture, Streams, XStreams, and Golden Gate, MongoDB has the oplog. These features range from deeply internal features like the MySQL binlog to full productized apis like XStreams. These log mechanisms will capture each change and have lower overhead than polling.</p><p>This is an area Confluent will be doing more work in the future.</p><h2 id="Retaining-Database-Changes"><a href="#Retaining-Database-Changes" class="headerlink" title="Retaining Database Changes"></a>Retaining Database Changes</h2><blockquote><p>For pure event data, Kafka often retains just a short window of events, say a week of data. However for database change streams, systems will want to do full restores off of this Kafka changelog. Kafka does have a relevant feature that can help with this called <code>Log Compaction</code>. </p></blockquote><blockquote><p><code>Log compaction ensures</code> that rather than discarding data by time, Kafka will retain at least the final update for each key. This means that any client reading the full log from Kafka will get a full copy of the data and not need to disturb the database. This is useful for cases where there are many subscribers that may need to restore a full copy of data to prevent them from overwhelming the source database.</p></blockquote><h2 id="Extract-Database-Data-As-is-Then-Transform"><a href="#Extract-Database-Data-As-is-Then-Transform" class="headerlink" title="Extract Database Data As-is, Then Transform"></a>Extract Database Data As-is, Then Transform</h2><p>Often databases have odd schemas specific to idiosyncrasies of their query pattern or internal implementation. Perhaps it stores data in odd key-value blobs. We would generally like to clean up this type of data for usage.</p><p>There are three ways we could do this clean-up:</p><ol><li>As part of the extraction process</li><li>As a stream processor that reads the original data stream and produces a “cleaned” stream with a more sane schema</li><li>In one of the destination system</li></ol><p>Pushing the clean-up to the consumer is not ideal as there can be many consumers so the work ends up being done over and over.</p><p>Clean up as part of the extraction is tempting, but often leads to problems. One person’s clean-up is another business logic and not all clean-ups are reversible so important aspects of the source data may be lost in the cleaning process.</p><p><strong>Our finding was that publishing the original data stream, what actually happened, had value; any additional clean-up could then be layered on top of that as a new stream of its own. This seems wasteful at first, but the reality is that this kind of storage is so cheap that it is often not a significant cost.</strong></p><h2 id="Stream-Processing"><a href="#Stream-Processing" class="headerlink" title="Stream Processing"></a>Stream Processing</h2><p>One of the goals of the stream data platform is being able to stream data between data systems. The other goal is to enable processing of data streams as data arrives.</p><p>Stream processing is easily modeled in the stream data platform as just a transformation between streams. A stream processing job continually reads from one or more data streams and outputs one or more data streams of output. These kind of processors can be strung together into a graph of flowing data:</p><p><img src="http://cdn2.hubspot.net/hub/540072/file-3062870623-png/blog-files/dag.png" alt="dag"></p><p>The particular method used to implement the processes that do the transformation is actually something of an implementation detail to the users of the output, though obviously it is an important detail to the implementor of the process.</p><p>Publishing data back into Kafka like this provides a number of benefits. First it decouples parts of the processing graph. One set of processing jobs may be written by one team and another by another. They may be built using different technologies. Most importantly we don’t want a slow downstream processor to be able to cause back-pressure to seize up anything that feeds data to it. Kafka acts as this buffer between the processors that can let an organization happily share data.</p><p>The most basic approach is to directly use the Kafka APIs to read input data streams, process that input and produce output streams. This can be done in a simple program in any programming language. Kafka allows you to scale these out by running multiple instances of these programs, it will spread the load across these instances. Kafka guarantees at-least once delivery of data and these programs will inherit that guarantee.</p><p>The advantage of the simple, framework free approach is that it is simple to operate and reason about and available in any language that has good Kafka clients.</p><p>However there are several stream processing systems that can potentially provide additional features. Used in this fashion as processing between Kafka topics they generally can’t give stronger guarantees or improve performance beyond what Kafka itself provides (though they can certainly make both worse). However building complex real-time processing can often be made simpler with a processing framework.</p><p>There are three common frameworks for stream processing:</p><ul><li><a href="http://storm.apache.org/" target="_blank" rel="noopener">Storm</a></li><li><a href="http://samza.apache.org/" target="_blank" rel="noopener">Samza</a></li><li><a href="https://spark.apache.org/streaming/" target="_blank" rel="noopener">Spark Streaming</a></li></ul><p>Coincidentally all are Apache projects beginning with the letter “S”! Of the two Storm and Samza are somewhat comparable, being message at a time stream processing systems, while Spark is more of a mini-batch framework that applies the (very nice) Spark abstraction to smaller batches of data. There are comparisons between these systems <a href="http://samza.apache.org/learn/documentation/0.7.0/comparisons/storm.html" target="_blank" rel="noopener">here</a> as well as <a href="http://samza.apache.org/learn/documentation/0.7.0/comparisons/spark-streaming.html" target="_blank" rel="noopener">here</a> and <a href="http://www.javacodegeeks.com/2015/02/streaming-big-data-storm-spark-samza.html" target="_blank" rel="noopener">here</a>.</p><p>So when should you use one of these stream processing frameworks?</p><p>Where these frameworks really shine is in areas where there will be lots of complex transformations. If there will be only a small number of processes doing transformations the cost of adopting a complex framework may not pay off, and the framework may come with operational and performance costs of their own. However if there will be a large number of transformations, making these easier to write should justify the additional operational burden.</p><p>Over time we think these frameworks will get more mature and more code will move into this stream processing domain, so the future of stream processing frameworks is quite bright.</p><h2 id="Have-Any-Streaming-Experiences-to-Share"><a href="#Have-Any-Streaming-Experiences-to-Share" class="headerlink" title="Have Any Streaming Experiences to Share?"></a>Have Any Streaming Experiences to Share?</h2><p>That is it for my current list of data stream do’s and don’ts. If you have additional recommendations to add to this, pass them on.</p><p>Meanwhile we’re working on trying to put a lot of these best practices into software as part of the Confluent Platform which you can find out more about here.</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;h3 id=&quot;Background&quot;&gt;&lt;a href=&quot;#Background&quot; class=&quot;headerlink&quot; title=&quot;Background&quot;&gt;&lt;/a&gt;Background&lt;/h3&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;I’m preparing to start a new journey, which I’ll annouce soon. The new opportunity is about data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;Apache Kafka&lt;/code&gt; has been a hot topic in the data field for a while, and, of course, I cannot taking on data problems without it. While learning and reading more about Kafka, I found &lt;a href=&quot;http://www.confluent.io/blog/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Conluent’s official tech blog&lt;/a&gt; has been an amazingly useful place to find out materials I need - because the company is started by founders of Kafka in Linkedin.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Thus, I decided to repost a few great blogs from Confluent, add my own notes, and publish them here.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;All posts are &lt;a href=&quot;https://phoenixjiangnan.github.io/categories/distributed-system/kafka/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;This post is Part II of a couple blogs from Concluent’s CEO Jay Kreps. Part II has been really amazing to present you an overall view of all major technical details of Kafka.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&quot;http://www.confluent.io/blog/stream-data-platform-2/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Original Post&lt;/a&gt; from Jay Kreps. February 25, 2015.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&quot;http://cdn2.hubspot.net/hub/540072/file-3062870613-png/blog-files/data-systems-sdp.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;This is the second part of our guide on streaming data and Apache Kafka. In part one I talked about the uses for real-time data streams and explained our idea of a stream data platform. The remainder of this guide will contain specific advice on how to go about building a stream data platform in your organization.&lt;/p&gt;
&lt;p&gt;This advice is drawn from our experience building and implementing Kafka at LinkedIn and rolling it out across all the data types and systems there. It also comes from four years working with tech companies in Silicon Valley to build Kafka-based stream data platforms in their organizations.&lt;/p&gt;
&lt;p&gt;This is meant to be a living document. As we learn new techniques, or new tools become available, I’ll update it.&lt;/p&gt;
&lt;h2 id=&quot;Getting-Started&quot;&gt;&lt;a href=&quot;#Getting-Started&quot; class=&quot;headerlink&quot; title=&quot;Getting Started&quot;&gt;&lt;/a&gt;Getting Started&lt;/h2&gt;&lt;p&gt;Much of the advice in this guide covers techniques that will scale to hundreds or thousands of well formed data streams. No one starts with that, of course. Usually you start with one or two trial applications, often ones that have scalability requirements that make other systems less suitable. Even in this kind of limited deployment, though, the techniques described in this guide will help you to start off with good practices, which is critical as your usage expands.&lt;/p&gt;
&lt;p&gt;Starting with something more limited is good, it let’s you get a hands on feel for what works and what doesn’t, so that, when broader adoption comes, you are well prepared for it.&lt;/p&gt;
&lt;h2 id=&quot;Recommendations&quot;&gt;&lt;a href=&quot;#Recommendations&quot; class=&quot;headerlink&quot; title=&quot;Recommendations&quot;&gt;&lt;/a&gt;Recommendations&lt;/h2&gt;&lt;p&gt;I’ll give a set of general recommendations for streaming data and Kafka and then discuss some specifics of different types of data.&lt;/p&gt;
&lt;h3 id=&quot;1-Limit-The-Number-of-Clusters&quot;&gt;&lt;a href=&quot;#1-Limit-The-Number-of-Clusters&quot; class=&quot;headerlink&quot; title=&quot;1. Limit The Number of Clusters&quot;&gt;&lt;/a&gt;1. Limit The Number of Clusters&lt;/h3&gt;&lt;p&gt;In early experimentation phases it is normal to end up with a few different Kafka clusters as adoption occurs organically in different parts of the organization.&lt;/p&gt;
    
    </summary>
    
      <category term="distributed system" scheme="https://bowenli86.github.io/categories/distributed-system/"/>
    
      <category term="kafka" scheme="https://bowenli86.github.io/categories/distributed-system/kafka/"/>
    
    
      <category term="kafka" scheme="https://bowenli86.github.io/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Putting Apache Kafka To Use - A Practical Guide to Building a Stream Data Platform (Part 1)</title>
    <link href="https://bowenli86.github.io/2016/10/03/distributed%20system/kafka/Putting-Apache-Kafka-To-Use-A-Practical-Guide-to-Building-a-Stream-Data-Platform-Part-1/"/>
    <id>https://bowenli86.github.io/2016/10/03/distributed system/kafka/Putting-Apache-Kafka-To-Use-A-Practical-Guide-to-Building-a-Stream-Data-Platform-Part-1/</id>
    <published>2016-10-04T01:47:07.000Z</published>
    <updated>2018-04-10T04:29:25.523Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3></blockquote><blockquote><p>I’m preparing to start a new journey, which I’ll annouce soon. The new opportunity is about data.</p></blockquote><blockquote><p><code>Apache Kafka</code> has been a hot topic in the data field for a while, and, of course, I cannot taking on data problems without it. While learning and reading more about Kafka, I found <a href="http://www.confluent.io/blog/" target="_blank" rel="noopener">Conluent’s official tech blog</a> has been an amazingly useful place to find out materials I need - because the company is started by founders of Kafka in Linkedin.</p></blockquote><blockquote><p>Thus, I decided to repost a few great blogs from Confluent, add my own notes, and publish them here.</p></blockquote><blockquote><p>All posts are <a href="https://phoenixjiangnan.github.io/categories/distributed-system/kafka/" target="_blank" rel="noopener">here</a></p></blockquote><hr><blockquote><p>This post is Part I of a couple blogs from Concluent’s CEO Jay Kreps. In this blog, he discussed the reason he led to develop Kafka in Linkedin, the problems Kafka can solve, and Kafka’s role in modern enterprise data system.</p></blockquote><blockquote><p>From my perspective, Part I is not as helpful as Part II w.r.t. technical details. But it certainly presents the fundamental background of how Kafka came to the world.</p></blockquote><blockquote><p><a href="http://www.confluent.io/blog/stream-data-platform-1/" target="_blank" rel="noopener">Original Post</a> from Jay Kreps. February 25, 2015.</p></blockquote><hr><p>These days you hear a lot about “stream processing”, “event data”, and “real-time”, often related to technologies like Kafka, Storm, Samza, or Spark’s Streaming module. Though there is a lot of excitement, not everyone knows how to fit these technologies into their technology stack or how to put it to use in practical applications.</p><p>This guide is going to discuss our experience with real-time data streams: how to build a home for real-time data within your company, and how to build applications that make use of that data. All of this is based on real experience: we spent the last five years building Apache Kafka, transitioning LinkedIn to a fully stream-based architecture, and helping a number of Silicon Valley tech companies do the same thing.</p><p>The first part of the guide will give a high-level overview of what we came to call a <code>stream data platform</code>: a central hub for real-time streams of data. It will cover the what and why of this idea.</p><p>The second part will dive into a lot of specifics and give advice on how to put this into practice effectively.</p><p>But first, what is a stream data platform?</p><h2 id="The-Stream-Data-Platform-A-Clean-Well-lighted-Place-For-Events"><a href="#The-Stream-Data-Platform-A-Clean-Well-lighted-Place-For-Events" class="headerlink" title="The Stream Data Platform: A Clean, Well-lighted Place For Events"></a>The Stream Data Platform: A Clean, Well-lighted Place For Events</h2><p>We built Apache Kafka at LinkedIn with a specific purpose in mind: to serve as a central repository of data streams. But why do this? There were two motivations.</p><p>The first problem was how to transport data between systems. We had lots of data systems: relational OLTP databases, Hadoop, Teradata, a search system, monitoring systems, OLAP stores, and derived key-value stores. Each of these needed reliable feeds of data in a geographically distributed environment. I’ll call this problem <code>data integration</code>, though we could also call it <code>ETL</code>.</p><p>The second part of this problem was the need to do richer analytical data processing—the kind of thing that would normally happen in a data warehouse or Hadoop cluster—but with very low latency. I call this “stream processing” though others might call it “messaging” or CEP or something similar.</p><a id="more"></a><p>I’ll talk a little about how these ideas developed at LinkedIn. At first we didn’t realize that these problems were connected at all. Our approach was very ad hoc: we built jerry-rigged piping between systems and applications on an as needed basis and shoe-horned any asynchronous processing into request-response web services. Over time this set-up got more and more complex as we ended up building pipelines between all kinds of different systems:</p><p><img src="http://cdn2.hubspot.net/hub/540072/file-3062870508-png/blog-files/data-flow-ugly.png" alt="data-flow-ugly"></p><p>Each of the pipelines was problematic in different ways. Our pipeline for log data was scalable but lossy and could only deliver data with high latency. Our pipeline between Oracle instances was fast, exact, and real-time, but not available to any other systems. Our pipeline of Oracle data for Hadoop was periodic CSV dumps—high throughput, but batch. Our pipeline of data to our search system was low latency, but unscalable and tied directly to the database. Our messaging systems were low latency but unreliable and unscalable.</p><p>As we added data centers geographically distributed around the world we had to build out geographical replication for each of these data flows. As each of these systems scaled, the supporting pipelines had to scale with them. Building simple duct tape pipelines had been easy enough but scaling these and operationalizing them was an enormous effort. I felt that my team, which was supposed to be made up of distributed systems engineers, was really acting more as distributed system plumbers.</p><p>Worse, the complexity meant that the data was always unreliable. Our reports were untrustworthy, derived indexes and stores were questionable, and everyone spent a lot of time battling data quality issues of all kinds. I remember an incident where we checked two systems that had similar data and found a discrepancy; we checked a third to try to determine which of these was correct and found that it matched neither.</p><p>At the same time we weren’t just shipping data from place to place; we also wanted to do things with it. Hadoop had given us a platform for batch processing, data archival, and ad hoc processing, and this had been enormously successful, but we lacked an analogous platform for low-latency processing. Many applications— especially our monitoring systems, search indexing pipeline, analytics, and security and fraud analysis—required latency of no more than a few seconds. These types of applications had no natural home in our infrastructure stack.</p><p>So in 2010 we decided to build a system that would focus on capturing data as streams and use this as both the integration mechanism between systems and also allow real-time processing of these same data streams. This was the origin of Apache Kafka.</p><p>We imagined something like this:</p><p><img src="http://cdn2.hubspot.net/hub/540072/file-3062870518-png/blog-files/stream_data_platform.png" alt=""></p><p>For a long time we didn’t really have a name for what we were doing (we just called it “Kafka stuff” or “the global commit log thingy”) but over time we came to call this kind of data “stream data”, and the concept of managing this centrally a “stream data platform”.</p><p>Our resulting system architecture went from the ugly spaghetti of pipelines I described before to a much cleaner stream-centric system:</p><p>A modern stream-centric data architecture built around Apache Kafka A modern stream-centric data architecture built around Apache Kafka</p><p>In this setup Kafka acts as a kind of universal pipeline for data. Each system can feed into this central pipeline or be fed by it; applications or stream processors can tap into it to create new, derived streams, which in turn can be fed back into the various systems for serving. Continuous feeds of well-formed data act as a kind of lingua franca across systems, applications, and data centers.</p><p>For example if a user updates their profile that update might flow into our stream processing layer where it would be processed to standardize their company information, geography, and other attributes. From there that stream might flow into search indexes and our social graph for querying, into a recommendation system for job matching; all of this would happen in milliseconds. This same flow would load into Hadoop to provide that data to the warehouse environment.</p><p>This usage at LinkedIn grew to phenomenal scale. Today at LinkedIn Kafka handles over 500 billion events per day spread over a number of data centers. It became the backbone for data flow between systems of all kinds, the core pipeline for Hadoop data, and the hub for stream processing.</p><p>Since Kafka was open source this usage spread beyond LinkedIn into companies of all kinds doing similar things.</p><p>In the rest of this article I’m going to outline a few details about this stream-centric world view, how it works, and what problems it solves.</p><h2 id="Streaming-Data"><a href="#Streaming-Data" class="headerlink" title="Streaming Data"></a>Streaming Data</h2><p>Most of what a business does can be thought of as streams of events. Sometimes this is obvious. Retail has streams of orders, sales, shipments, price adjustments, returns, and so on. Finance has orders, stock prices, and other financial time series. Web sites have streams of clicks, impressions, searches, and so on. Big software systems have streams of requests, errors, machine metrics, and logs. Indeed one view of a business is as a kind of data processing system that takes various input streams and produces corresponding output streams (and maybe some physical goods along the way).</p><p>This view of data can seem a little foreign to people who are more accustomed to thinking of data as rows in databases rather than as events, so let’s look at a few practical aspects of event data.</p><h2 id="The-Rise-of-Events-and-Event-Streams"><a href="#The-Rise-of-Events-and-Event-Streams" class="headerlink" title="The Rise of Events and Event Streams"></a>The Rise of Events and Event Streams</h2><p>Your database stores the current state of your data. But the current state is always caused by some actions that took place in the past. The actions are the events. Your inventory table is the state that results from the purchase and sale events that have been made, bank balances are the result of credits and debits, and the latency graph for your web server is an aggregation of the stream of HTTP request times.</p><p>Much of what people refer to when they talk about “big data” is really the act of capturing these events that previously weren’t recorded anywhere and putting them to use for analysis, optimization, and decision making. In some sense these events are the other half of the story the database tables don’t tell: they are the story of what the business did.</p><p>Event data has always been present in finance, where stock ticks, market indicators, trades, and other time series data are naturally thought of as event streams.</p><p>But the tech industry popularized the most modern incarnation of technology for capture and use of this data. Google transformed the stream of ad clicks and ad impressions into a multi-billion dollar business. In the web space event data is often called “log data”, because, lacking any proper infrastructure for their events, log files are often where the events are put. Systems like Hadoop are often described as being for <code>log processing</code>, but that usage might be better described as batch event storage and processing.</p><p>Web companies were probably the earliest to do this because the process of capturing event data in a web site is very easy: a few lines of code can add tracking that records what users on a website did. As a result a single page load or mobile screen on a popular website is likely recording dozens or even hundreds of these events for analysis and monitoring.</p><p>You will sometimes hear about “machine generated data”, but this is just event data by another name. In some sense virtually all data is machine generated, since it is made by computer systems.</p><p>Likewise there is a lot of talk about device data and the “internet of things”. This is a phrase that means a lot of things to different people, but a large part of the promise has to do with applying the same data collection and analytics of big web systems to industrial devices and consumer goods. In other words, more event streams.</p><h2 id="Databases-Are-Event-Streams"><a href="#Databases-Are-Event-Streams" class="headerlink" title="Databases Are Event Streams"></a>Databases Are Event Streams</h2><p>Event streams are an obvious fit for log data or things like “orders”, “sales”, “clicks” or “trades” that are obviously event-like. But, like most people, you probably keep much of your data in databases, whether relational databases like Oracle, MySQL, and Postgres, or newer distributed databases like MongoDB, Cassandra, and Couchbase. These would seem at first to be far removed from the world of events or streams.</p><p>But, in fact, data in databases can also be thought of as an event stream. The easiest way to understand the event stream representation of a database is to think about the process of creating a backup or standby copy of a database. A naive approach to doing this might be to dump out the contents of your database periodically, and load this up into the standby database. If we do this only infrequently, and our data isn’t too large, than taking a full dump of all the data may be quite feasible. In fact many backup and ETL procedures do exactly this. However this approach won’t scale as we increase the frequency of the data capture: if we do a full dump of data twice a day, it will take twice the system resources, and if we do it hourly, 24 times as much. The obvious approach to make this more efficient is to take a “diff” of what has changed and just fetch rows that have been newly created, updated, or deleted since our last diff was taken. Using this method, if we take our diffs twice as often, the diffs themselves will get (roughly) half as big, and the system resources will remain more or less the same as we increase the frequency of our data capture.</p><p>Why not take this process to the limit and take our diffs more and more frequently? If we do this what we will be left with is a continuous sequence of single row changes. This kind of event stream is called change capture, and is a common part of many databases systems (Oracle has XStreams and GoldenGate, MySQL has binlog replication, and Postgres has Logical Log Streaming Replication).</p><p>By publishing the database changes into the stream data platform you add this to the other set of event streams. You can use these streams to synchronize other systems like a Hadoop cluster, a replica database, or a search index, or you can feed these changes into applications or stream processors to directly compute new things off the changes. These changes are in turn published back as streams that are available to all the integrated systems.</p><h2 id="What-Is-a-Stream-Data-Platform-For"><a href="#What-Is-a-Stream-Data-Platform-For" class="headerlink" title="What Is a Stream Data Platform For?"></a>What Is a Stream Data Platform For?</h2><blockquote><p>A stream data platform has two primary uses:</p></blockquote><blockquote><ol><li>Data Integration: The stream data platform captures streams of events or data changes and feeds these to other data systems such as relational databases, key-value stores, Hadoop, or the data warehouse.</li></ol></blockquote><blockquote><ol start="2"><li>Stream processing: It enables continuous, real-time processing and transformation of these streams and makes the results available system-wide.</li></ol></blockquote><p>In its first role, the stream data platform is a central hub for data streams. Applications that integrate don’t need to be concerned with the details of the original data source, all streams look the same. It also acts as a buffer between these systems—the publisher of data doesn’t need to be concerned with the various systems that will eventually consume and load the data. This means consumers of data can come and go and are fully decoupled from the source.</p><p>If you adopt a new system you can do this by tapping into your existing data streams rather than instrumenting each individual source system and application for each possible destination. The streams all look the same whether they originated in log files, a database, Hadoop, a stream processing system, or wherever else. This makes adding a new data system a much cheaper proposition—it need only integrate with the stream data platform not with every possible data source and sink directly.</p><p>A similar story is important for Hadoop which wants to be able to maintain a full copy of all the data in your organization and act as a “data lake” or “enterprise data hub”. Directly integrating each data source with HDFS is a hugely time consuming proposition, and the end result only makes that data available to Hadoop. This type of data capture isn’t suitable for real-time processing or syncing other real-time applications. Likewise this same pipeline can run in reverse: Hadoop and the data warehouse environment can publish out results that need to flow into appropriate systems for serving in customer-facing applications.</p><p>The stream processing use case plays off the data integration use case. All the streams that are captured for loading into Hadoop for archival are equally available for continuous “stream processing” as data is captured in the stream. The results of the stream processing are just a new, derived stream. This stream looks just like any other stream and is available for loading in all the data systems that have integrated with the stream data platform.</p><p>This stream processing can be done using simple application code that taps into the stream of events and publishes out a new stream of events. But this type of application code can be made easier with the help of a stream processing framework—such as Storm, Samza, or Spark Streaming—that helps provide richer processing primitives. These frameworks are just gaining prominence now, but each integrates well with Apache Kafka.</p><p>Stream processing acts as both a way to develop applications that need low-latency transformations but it is also directly part of the data integration usage as well: integrating systems often requires some munging of data streams in between.</p><h2 id="What-Does-a-Stream-Data-Platform-Need-To-Do"><a href="#What-Does-a-Stream-Data-Platform-Need-To-Do" class="headerlink" title="What Does a Stream Data Platform Need To Do?"></a>What Does a Stream Data Platform Need To Do?</h2><p>I’ve discussed a number of different use cases. Each of these use cases has a corresponding event stream, but each stream has slightly different requirements—some need to be fast, some high-throughput, some need to scale out, etc. If we want to make a single platform that can handle all of these uses what will it need to do?</p><p><strong>I think the following are the key requirements for a stream data platform:</strong></p><blockquote><ul><li>It must be reliable enough to handle critical updates such as replicating the changelog of a database to a replica store like a search index, delivering this data in order and without loss.</li><li>It must support throughput high enough to handle large volume log or event data streams.</li><li>It must be able to buffer or persist data for long periods of time to support integration with batch systems such as Hadoop that may only perform their loads and processing periodically.</li><li>It must provide data with latency low enough for real-time applications.</li><li>It must be possible to operate it as a central system that can scale to carry the full load of the organization and operate with hundreds of applications built by disparate teams all plugged into the same central nervous system.</li><li>It has to support close integration with stream processing systems.</li></ul></blockquote><p>These requirements are necessary for this system to truly bring simplicity to data flow. The goal of the stream data platform is to sit at the heart of the company and manage these data streams. If the system cannot provide sufficient reliability guarantees or scale to large volume data then data will again end up fragmented over multiple systems. If the system cannot support both batch and real-time consumption, then again data will be fragmented. And if the system does not support operations at company-wide scale then silos will arise.</p><h2 id="What-is-Apache-Kafka"><a href="#What-is-Apache-Kafka" class="headerlink" title="What is Apache Kafka?"></a>What is Apache Kafka?</h2><p>Apache Kafka is a distributed system designed for streams. It is built to be fault-tolerant, high-throughput, horizontally scalable, and allows geographically distributing data streams and processing.</p><p>Kafka is often categorized as a messaging system, and it serves a similar role, but provides a fundamentally different abstraction. The key abstraction in Kafka is a structured commit log of updates:</p><p><img src="http://cdn2.hubspot.net/hub/540072/file-3062870538-png/blog-files/commit_log-copy.png" alt="commit_log"></p><p>A producer of data sends a stream of records which are appended to this log, and any number of consumers can continually stream these updates off the tail of the log with millisecond latency. Each of these data consumers has its own position in the log and advances independently. This allows a reliable, ordered stream of updates to be distributed to each consumer.</p><p>The log can be sharded and spread over a cluster of machines, and each shard is replicated for fault-tolerance. This gives a model for parallel, ordered consumption which is key to Kafka’s use as a change capture system for database updates (which must be delivered in order).</p><p>Kafka is built as a modern distributed system. Data is replicated and partitioned over a cluster of machines that can grow and shrink transparently to the applications using the cluster. Consumers of data can be scaled out over a pool of machines as well and automatically adapt to failures in the consuming processes.</p><p>A key aspect of Kafka’s design is that it handles persistence well. A Kafka broker can store many TBs of data. This allows usage patterns that would be impossible in a traditional database:</p><ul><li>A Hadoop cluster or other offline system that is fed off Kafka can go down for maintenance and come back hours or days later confident that all changes have been safely persisted in the up-stream Kafka cluster.</li><li>When synchronizing from database tables it is possible to initialize a “full dump” of the database so that downstream consumers of data have access to the full data set.</li></ul><p>These features make Kafka applicable well beyond the uses of traditional enterprise messaging systems.</p><h2 id="Event-driven-Applications"><a href="#Event-driven-Applications" class="headerlink" title="Event-driven Applications"></a>Event-driven Applications</h2><p>Since we built Kafka as an open source project we have had the opportunity to work closely with companies who put it to use and to see the general pattern of Kafka adoption: how it first is adopted and how its role evolves over time in their architecture.</p><p>The initial adoption is usually for a single particularly large-scale use case: Log data, feeds into Hadoop, or other data streams beyond the capabilities of their existing messaging systems or infrastructure.</p><p>From there, though, the usage spreads. Though the initial use case may have been feeding a Hadoop cluster, once there is a continual feed of events available, the use cases for processing these events in real-time quickly emerge. Existing applications will end up tapping into the event streams to react to what is happening more intelligently, and new applications will be built to harness intelligence derived off these streams.</p><p>For example at LinkedIn we originally began capturing a stream of views to jobs displayed on the website as one of many feeds to deliver to Hadoop and our relational data warehouse. However this ETL-centric use case soon became one of many and the stream of job views over time began to be used by a variety of systems:</p><p><img src="http://cdn2.hubspot.net/hub/540072/file-3062870548-png/blog-files/job-view.png" alt="job-view"></p><p>Note that the application that showed jobs didn’t need any particular modification to integrate with these other uses. It just produced the stream of jobs that were viewed. The other applications tapped into this stream to add their own processing. Likewise when job views began happening in other applications—mobile applications—these are just added to the global feed of events, the downstream processors don’t need to integrate with new upstream sources.</p><h2 id="How-Does-a-Stream-Data-Platform-Relate-To-Existing-Things"><a href="#How-Does-a-Stream-Data-Platform-Relate-To-Existing-Things" class="headerlink" title="How Does a Stream Data Platform Relate To Existing Things"></a>How Does a Stream Data Platform Relate To Existing Things</h2><p>Let’s talk briefly about the relationship this stream data platform concept has with other things in the world.</p><h3 id="1-Messaging"><a href="#1-Messaging" class="headerlink" title="1. Messaging"></a>1. Messaging</h3><p>A stream data platform is similar to an enterprise messaging system—it receives messages and distributes them to interested subscribers. There are three important differences:</p><ul><li><p>Messaging systems are typically run in one-off deployments for different applications. The purpose of the stream data platform is very much as a central data hub.</p></li><li><p>Messaging systems do a poor job of supporting integration with batch systems, such as a data warehouse or a Hadoop cluster, as they have limited data storage capacity.</p></li><li><p>Messaging systems do not provide semantics that are easily compatible with rich stream processing.</p></li></ul><p>In other words a data stream data platform is a messaging system whose role has been rethought at a company-wide scale.</p><h3 id="2-Data-Integration-Tools"><a href="#2-Data-Integration-Tools" class="headerlink" title="2. Data Integration Tools"></a>2. Data Integration Tools</h3><p>A data stream data platform does a lot to make integration between systems easier. However its role is different from a tool like Informatica. A stream data platform is a true platform that any other system can choose to tap into and many applications can build around.</p><p>One practical area of overlap is that by making data available in a uniform format in a single place with a common stream abstraction, many of the routine data clean-up tasks can be avoided entirely. I’ll dive into this more in the second part of this article.</p><h3 id="3-Enterprise-Service-Buses"><a href="#3-Enterprise-Service-Buses" class="headerlink" title="3. Enterprise Service Buses"></a>3. Enterprise Service Buses</h3><p>I think a data stream data platform embodies many of the ideas of an enterprise service bus, but with better implementation. The challenges of Enterprise Service Bus adoption has been the coupling of transformations of data with the bus itself. Some of the challenges of Enterprise Service Bus adoption are that much of the logic required for transformation are baked into the message bus itself without a good model for multi-tenant cloud like deployment and operation of this logic.</p><p>The advantage of a stream data platform is that transformation is fundamentally decoupled from the stream itself. This code can live in applications or stream processing tasks, allowing teams to iterate at their own pace without a central bottleneck for application development.</p><h3 id="4-Change-Capture-Systems"><a href="#4-Change-Capture-Systems" class="headerlink" title="4. Change Capture Systems"></a>4. Change Capture Systems</h3><p>Databases have long had similar log mechanisms such as Golden Gate. However these mechanisms are limited to database changes only and are not a general purpose event capture platform. They tend to focus primarily on the replication between databases, often between instances of the same database system (e.g. Oracle-to-Oracle).</p><h3 id="5-Data-Warehouses-and-Hadoop"><a href="#5-Data-Warehouses-and-Hadoop" class="headerlink" title="5. Data Warehouses and Hadoop"></a>5. Data Warehouses and Hadoop</h3><p>A stream data platform doesn’t replace your data warehouse; in fact, quite the opposite: it feeds it data. It acts as a conduit for data to quickly flow into the warehouse environment for long-term retention, ad hoc analysis, and batch processing. That same pipeline can run in reverse to publish out derived results from nightly or hourly batch processing.</p><h3 id="6-Stream-Processing-Systems"><a href="#6-Stream-Processing-Systems" class="headerlink" title="6. Stream Processing Systems"></a>6. Stream Processing Systems</h3><p>Stream processing frameworks such as Storm, Samza, or Spark Streaming can be an excellent addition to the data stream data platform. They attempt to add richer processing semantics to subscribers and can make implementing data transformation easier.</p><p>Of course data transformation doesn’t require a specialized system. Normal application code can subscribe to streams, process them, and write back derived streams, just as one does in one of these fancier systems. However these frameworks can potentially make this kind of processing easier.</p><h2 id="What-Does-This-Look-Like-In-Practice"><a href="#What-Does-This-Look-Like-In-Practice" class="headerlink" title="What Does This Look Like In Practice?"></a>What Does This Look Like In Practice?</h2><p>One of the interesting things about this concept is that it isn’t just an idea, we have actually had the opportunity to “do the experiment”. We spent the last five years building Kafka and helping companies put streaming data to use. At a number of Silicon Valley companies today you can see this concept in action—everything from user activity to database changes to administrative actions like restarting a process are captured in real-time streams that are subscribed to and processed in real-time.</p><p>What is interesting about this is that what begins as simple plumbing quickly evolves into something much more. These data streams begin to act as a kind of central nervous system that applications organize themselves around.</p><h2 id="Next-Steps"><a href="#Next-Steps" class="headerlink" title="Next Steps"></a>Next Steps</h2><p>We think this technology is changing how data is put to use in companies. We are building the Confluent Platform, a set of tools aimed at helping companies adopt and use Apache Kafka in this way. We think the Confluent Platform represents the best place to get started if you are thinking about putting streaming data to use in your organization.</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;h3 id=&quot;Background&quot;&gt;&lt;a href=&quot;#Background&quot; class=&quot;headerlink&quot; title=&quot;Background&quot;&gt;&lt;/a&gt;Background&lt;/h3&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;I’m preparing to start a new journey, which I’ll annouce soon. The new opportunity is about data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;Apache Kafka&lt;/code&gt; has been a hot topic in the data field for a while, and, of course, I cannot taking on data problems without it. While learning and reading more about Kafka, I found &lt;a href=&quot;http://www.confluent.io/blog/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Conluent’s official tech blog&lt;/a&gt; has been an amazingly useful place to find out materials I need - because the company is started by founders of Kafka in Linkedin.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Thus, I decided to repost a few great blogs from Confluent, add my own notes, and publish them here.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;All posts are &lt;a href=&quot;https://phoenixjiangnan.github.io/categories/distributed-system/kafka/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;This post is Part I of a couple blogs from Concluent’s CEO Jay Kreps. In this blog, he discussed the reason he led to develop Kafka in Linkedin, the problems Kafka can solve, and Kafka’s role in modern enterprise data system.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;From my perspective, Part I is not as helpful as Part II w.r.t. technical details. But it certainly presents the fundamental background of how Kafka came to the world.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;http://www.confluent.io/blog/stream-data-platform-1/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Original Post&lt;/a&gt; from Jay Kreps. February 25, 2015.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;These days you hear a lot about “stream processing”, “event data”, and “real-time”, often related to technologies like Kafka, Storm, Samza, or Spark’s Streaming module. Though there is a lot of excitement, not everyone knows how to fit these technologies into their technology stack or how to put it to use in practical applications.&lt;/p&gt;
&lt;p&gt;This guide is going to discuss our experience with real-time data streams: how to build a home for real-time data within your company, and how to build applications that make use of that data. All of this is based on real experience: we spent the last five years building Apache Kafka, transitioning LinkedIn to a fully stream-based architecture, and helping a number of Silicon Valley tech companies do the same thing.&lt;/p&gt;
&lt;p&gt;The first part of the guide will give a high-level overview of what we came to call a &lt;code&gt;stream data platform&lt;/code&gt;: a central hub for real-time streams of data. It will cover the what and why of this idea.&lt;/p&gt;
&lt;p&gt;The second part will dive into a lot of specifics and give advice on how to put this into practice effectively.&lt;/p&gt;
&lt;p&gt;But first, what is a stream data platform?&lt;/p&gt;
&lt;h2 id=&quot;The-Stream-Data-Platform-A-Clean-Well-lighted-Place-For-Events&quot;&gt;&lt;a href=&quot;#The-Stream-Data-Platform-A-Clean-Well-lighted-Place-For-Events&quot; class=&quot;headerlink&quot; title=&quot;The Stream Data Platform: A Clean, Well-lighted Place For Events&quot;&gt;&lt;/a&gt;The Stream Data Platform: A Clean, Well-lighted Place For Events&lt;/h2&gt;&lt;p&gt;We built Apache Kafka at LinkedIn with a specific purpose in mind: to serve as a central repository of data streams. But why do this? There were two motivations.&lt;/p&gt;
&lt;p&gt;The first problem was how to transport data between systems. We had lots of data systems: relational OLTP databases, Hadoop, Teradata, a search system, monitoring systems, OLAP stores, and derived key-value stores. Each of these needed reliable feeds of data in a geographically distributed environment. I’ll call this problem &lt;code&gt;data integration&lt;/code&gt;, though we could also call it &lt;code&gt;ETL&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The second part of this problem was the need to do richer analytical data processing—the kind of thing that would normally happen in a data warehouse or Hadoop cluster—but with very low latency. I call this “stream processing” though others might call it “messaging” or CEP or something similar.&lt;/p&gt;
    
    </summary>
    
      <category term="distributed system" scheme="https://bowenli86.github.io/categories/distributed-system/"/>
    
      <category term="kafka" scheme="https://bowenli86.github.io/categories/distributed-system/kafka/"/>
    
    
      <category term="kafka" scheme="https://bowenli86.github.io/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Spring Boot - Introduction to Spring Boot</title>
    <link href="https://bowenli86.github.io/2016/09/30/spring/spring%20boot/Spring-Boot-Introduction-to-Spring-Boot/"/>
    <id>https://bowenli86.github.io/2016/09/30/spring/spring boot/Spring-Boot-Introduction-to-Spring-Boot/</id>
    <published>2016-10-01T06:18:56.000Z</published>
    <updated>2018-04-10T04:29:25.544Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Spring-io-Spring-Boot"><a href="#Spring-io-Spring-Boot" class="headerlink" title="Spring.io - Spring Boot"></a>Spring.io - Spring Boot</h2><p>I’m taking some important parts from it and summarizing them as my learning notes.</p><p>Spring has been notorious for its heaviness when starting a project. Lots of people avoid this framework because of that. Many lighter dependency injection frameworks emerge as a replacement for Spring, <a href="https://github.com/google/guice" target="_blank" rel="noopener">Google’s Guice</a> being one of the most representative ones. </p><p>I’ve talked to quite a few folks who touched Spring a few years back and then switched to some better solutions because of the pain of managing the sophisticated configs. They hadn’t heard of Spring Boot, and were surprised about this project when I introduced it to them.</p><p>Here, let’s discuss some of the very basic yet fundamental aspects of Spring Boot.</p><hr><p>The following content is from the official guidance of <a href="http://docs.spring.io/spring-boot/docs/current-SNAPSHOT/reference/htmlsingle/#getting-started" target="_blank" rel="noopener">Spring Boot</a></p><hr><h2 id="Introducing-Spring-Boot"><a href="#Introducing-Spring-Boot" class="headerlink" title="Introducing Spring Boot"></a>Introducing Spring Boot</h2><p>Spring Boot makes it easy to create stand-alone, production-grade Spring based Applications that you can <code>just run</code>. We take an opinionated view of the Spring platform and third-party libraries so you can get started with minimum fuss. </p><blockquote><p>Most Spring Boot applications need very little Spring configuration.</p></blockquote><p>You can use Spring Boot to create Java applications that can be started using <code>java -jar</code> or more traditional war deployments. We also provide a command line tool that runs <code>spring scripts</code>.</p><p>Some featured characteristics are:</p><blockquote><ul><li>Provide a radically faster and widely accessible getting started experience for all Spring development.<ul><li>Create stand-alone Spring applications</li><li>Embed Tomcat, Jetty or Undertow directly (no need to deploy WAR files)</li><li>Provide opinionated <code>starter</code> POMs (Project Object Model) to simplify your Maven configuration</li><li>Automatically configure Spring whenever possible</li><li>Provide production-ready features such as metrics, health checks and externalized configuration</li><li>Absolutely <code>no code generation</code> and <code>no requirement for XML configuration</code></li></ul></li></ul></blockquote><p>Spring Boot offers four main features that will change the way you develop Spring applications:</p><blockquote><ul><li><code>Spring Boot starters</code> — Spring Boot starters aggregate common groupings of dependencies into single dependencies that can be added to a project’s Maven or Gradle build.</li><li><code>Autoconfiguration</code> — Spring Boot’s autoconfiguration feature leverages Spring 4’s support for conditional configuration to make reasonable guesses about the beans your application needs and automatically configure them.</li><li><code>Command-line interface (CLI)</code> — Spring Boot’s CLI takes advantage of the Groovy programming language along with autoconfiguration to further simplify Spring application development.</li><li><code>Actuator</code> — The Spring Boot Actuator adds certain management features to a Spring Boot application.</li></ul></blockquote><a id="more"></a><h3 id="1-Adding-starter-dependencies"><a href="#1-Adding-starter-dependencies" class="headerlink" title="1 Adding starter dependencies"></a>1 Adding starter dependencies</h3><p>There are two ways to bake a cake. The ambitious baker will mix flour, eggs, sugar, baking powder, salt, butter, vanilla, and milk into a batter. Or you can buy a prepackaged box of cake mix that includes most of the ingredients you’ll need and only mix in a few wet ingredients like water, eggs, and vegetable oil.</p><p>Much as a prepackaged cake mix aggregates many of the ingredients of a cake recipe into a single ingredient, Spring Boot starters aggregate the various dependencies of an application into a single dependency.</p><p>If you’re building your project with Gradle, you’ll need (at least) the following dependencies in <code>build.gradle</code>:</p><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">dependencies</span> &#123;</span><br><span class="line">  <span class="keyword">compile</span>(<span class="string">"org.springframework:spring-web:4.0.6.RELEASE"</span>)</span><br><span class="line">  <span class="keyword">compile</span>(<span class="string">"org.springframework:spring-webmvc:4.0.6.RELEASE"</span>)</span><br><span class="line">  <span class="keyword">compile</span>(<span class="string">"com.fasterxml.jackson.core:jackson-databind:2.2.2"</span>)</span><br><span class="line">  <span class="keyword">compile</span>(<span class="string">"org.springframework:spring-jdbc:4.0.6.RELEASE"</span>)</span><br><span class="line">  <span class="keyword">compile</span>(<span class="string">"org.springframework:spring-tx:4.0.6.RELEASE"</span>)</span><br><span class="line">  <span class="keyword">compile</span>(<span class="string">"com.h2database:h2:1.3.174"</span>)</span><br><span class="line">  <span class="keyword">compile</span>(<span class="string">"org.thymeleaf:thymeleaf-spring4:2.1.2.RELEASE"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Fortunately, Gradle makes it possible to express dependencies succinctly. Even so, a lot of work went into creating this list, and more will go into maintaining it. How can you know if these dependencies will play well together? As the application grows and evolves, dependency management will become even more challenging.</p><p>But if you’re using the prepackaged dependencies from Spring Boot starters, the Gradle dependency list can be a little shorter:</p><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">dependencies</span> &#123;</span><br><span class="line">  <span class="keyword">compile</span>(<span class="string">"org.springframework.boot:spring-boot-starter-web:</span></span><br><span class="line"><span class="string">           1.1.4.RELEASE"</span>)</span><br><span class="line">  <span class="keyword">compile</span>(<span class="string">"org.springframework.boot:spring-boot-starter-jdbc:</span></span><br><span class="line"><span class="string">           1.1.4.RELEASE"</span>)</span><br><span class="line">  <span class="keyword">compile</span>(<span class="string">"com.h2database:h2:1.3.174"</span>)</span><br><span class="line">  <span class="keyword">compile</span>(<span class="string">"org.thymeleaf:thymeleaf-spring4:2.1.2.RELEASE"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>As you can see, Spring Boot’s web and JDBC starters replaced several of the finergrained dependencies. You still need to include the H2 and Thymeleaf dependencies, but the other dependencies are rolled up into the starter dependencies. Aside from making the dependency list shorter, you can feel confident that the versions of dependencies provided by the starters are compatible with each other.</p><p>E.g.</p><table><thead><tr><th>Starter</th><th>Provides</th></tr></thead><tbody><tr><td>spring-boot-starter-ac tuator</td><td>spring-boot-starter, spring-boot- actuator, spring-core</td></tr><tr><td></td></tr><tr><td>spring-boot-starter-amqp</td><td>spring-boot-starter, spring-boot-rabbit, spring-core, spring-tx</td></tr><tr><td>spring-boot-starter-aop</td><td>spring-boot-starter, spring-aop, AspectJ Runtime, AspectJ Weaver, spring-core</td></tr></tbody></table><p>Taking advantage of Maven’s and Gradle’s transitive dependency resolution, the starters declare several dependencies in their own pom.xml file. When you add one of these starter dependencies to your Maven or Gradle build, the starter’s dependencies are resolved transitively.</p><h3 id="2-Autoconfiguration"><a href="#2-Autoconfiguration" class="headerlink" title="2 Autoconfiguration"></a>2 Autoconfiguration</h3><p>Whereas <code>Spring Boot starters cut down the size of your build&#39;s dependency list</code>, <code>Spring Boot autoconfiguration cuts down on the amount of Spring configuration</code>. It does this by considering other factors in your application and making assumptions about what Spring configuration you’ll need.</p><p>Spring Boot starters can trigger autoconfiguration. For instance, all you need to do to use Spring MVC in your Spring Boot application is to add <code>the web starter</code> as a dependency in the build. When you add the web starter to your project’s build, it will transitively pull in Spring MVC dependencies. When Spring Boot’s web autoconfiguration detects Spring MVC in the classpath, it will automatically configure several beans to support Spring MVC, including view resolvers, resource handlers, and message converters (among others). All that’s left for you to do is write the controller classes to handle the requests.</p><h3 id="3-The-Spring-Boot-CLI"><a href="#3-The-Spring-Boot-CLI" class="headerlink" title="3 The Spring Boot CLI"></a>3 The Spring Boot CLI</h3><p>The Spring Boot CLI takes the magic provided by Spring Boot <code>starters</code> and <code>autoconfiguration</code> and spices it up a little with Groovy. It reduces the Spring development process to the point where you can run one or more Groovy scripts through a CLI and see it run. In the course of running the application, the CLI will also automatically import Spring types and resolve dependencies.</p><p>One of the most interesting examples used to illustrate Spring Boot CLI is contained in the following Groovy script:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Hi</span> </span>&#123;</span><br><span class="line">    <span class="meta">@RequestMapping</span>(<span class="string">"/"</span>)</span><br><span class="line">    <span class="function">String <span class="title">hi</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="string">"Hi!"</span> </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Believe it or not, that is a complete (albeit simple) Spring application that can be executed through the Spring Boot CLI. Including whitespace, it’s 82 characters in length. You can paste it into your Twitter client and tweet it to your friends.</p><p>Eliminate the unnecessary whitespace and you get this 64-character one-liner:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@RestController</span> <span class="class"><span class="keyword">class</span> <span class="title">Hi</span></span>&#123;<span class="meta">@RequestMapping</span>(<span class="string">"/"</span>)<span class="function">String <span class="title">hi</span><span class="params">()</span></span>&#123;<span class="string">"Hi!"</span>&#125;&#125;</span><br></pre></td></tr></table></figure><p>This version is so brief that you can paste it twice into a single tweet on Twitter. But it’s still a complete and runnable (if feature-poor) Spring application. If you have the Spring Boot CLI installed, you can run it with the following command line:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ spring run Hi.groovy</span><br></pre></td></tr></table></figure><h3 id="4-The-Actuator"><a href="#4-The-Actuator" class="headerlink" title="4 The Actuator"></a>4 The Actuator</h3><p>The Spring Boot Actuator brings a handful of useful features to a Spring Boot project, including:</p><blockquote><ul><li>Management endpoint</li><li>Sensible error handling and a default mapping for an <code>/error</code> endpoint</li><li>An <code>/info</code> endpoint that can communicate information about an application</li><li>An audit events framework when Spring Security is in play</li></ul></blockquote><p>All of these features are useful, but the management endpoints are the most immediately useful and interesting features of the Actuator.</p><p>Spring Boot includes a number of additional features to help you monitor and manage your application when it’s pushed to production. You can choose to manage and monitor your application using HTTP endpoints, with JMX or even by remote shell (SSH or Telnet). Auditing, health and metrics gathering can be automatically applied to your application.</p><p>Actuator HTTP endpoints are only available with a Spring MVC-based application. In particular, it will not work with Jersey unless you enable Spring MVC as well.</p><table><thead><tr><th>ID</th><th style="text-align:center">Description</th><th style="text-align:right">Sensitive Default</th></tr></thead><tbody><tr><td>actuator</td><td style="text-align:center">Provides a hypermedia-based “discovery page” for the other endpoints. Requires Spring HATEOAS to be on the classpath.</td><td style="text-align:right">true</td></tr><tr><td>autoconfig</td><td style="text-align:center">Displays an auto-configuration report showing all auto-configuration candidates and the reason why they ‘were’ or ‘were not’ applied.</td><td style="text-align:right">true</td></tr><tr><td>beans</td><td style="text-align:center">Displays a complete list of all the Spring beans in your application.</td><td style="text-align:right">true</td></tr><tr><td>configprops</td><td style="text-align:center">Displays a collated list of all @ConfigurationProperties.</td><td style="text-align:right">true</td></tr><tr><td>dump</td><td style="text-align:center">Performs a thread dump.</td><td style="text-align:right">true</td></tr><tr><td>env</td><td style="text-align:center">Exposes properties from Spring’s ConfigurableEnvironment.</td><td style="text-align:right">true</td></tr><tr><td>flyway</td><td style="text-align:center">Shows any Flyway database migrations that have been applied.</td><td style="text-align:right">true</td></tr><tr><td>health</td><td style="text-align:center">Shows application health information (when the application is secure, a simple ‘status’ when accessed over an unauthenticated connection or full message details when authenticated).</td><td style="text-align:right">false</td></tr><tr><td>info</td><td style="text-align:center">Displays arbitrary application info.</td><td style="text-align:right">false</td></tr><tr><td>liquibase</td><td style="text-align:center">Shows any Liquibase database migrations that have been applied.</td><td style="text-align:right">true</td></tr><tr><td>metrics</td><td style="text-align:center">Shows ‘metrics’ information for the current application.</td><td style="text-align:right">true</td></tr><tr><td>mappings</td><td style="text-align:center">Displays a collated list of all @RequestMapping paths.</td><td style="text-align:right">true</td></tr><tr><td>shutdown</td><td style="text-align:center">Allows the application to be gracefully shutdown (not enabled by default).</td><td style="text-align:right">true</td></tr><tr><td>trace</td><td style="text-align:center">Displays trace information (by default the last 100 HTTP requests).</td><td style="text-align:right">true</td></tr></tbody></table><p>If you are using Spring MVC, the following additional endpoints can also be used:</p><table><thead><tr><th>ID</th><th style="text-align:center">Description</th><th style="text-align:right">Sensitive Default</th></tr></thead><tbody><tr><td>docs</td><td style="text-align:center">Displays documentation, including example requests and responses, for the Actuator’s endpoints. Requires spring-boot-actuator-docs to be on the classpath.</td><td style="text-align:right">false</td></tr><tr><td>heapdump</td><td style="text-align:center">Returns a GZip compressed hprof heap dump file.</td><td style="text-align:right">true</td></tr><tr><td>jolokia</td><td style="text-align:center">Exposes JMX beans over HTTP (when Jolokia is on the classpath).</td><td style="text-align:right">true</td></tr><tr><td>logfile</td><td style="text-align:center">Returns the contents of the logfile (if logging.file or logging.path properties have been set). Supports the use of the HTTP Range header to retrieve part of the log file’s content.</td><td style="text-align:right">true</td></tr></tbody></table><blockquote><p>Depending on how an endpoint is exposed, the sensitive property may be used as a security hint. For example, sensitive endpoints will require a username/password when they are accessed over HTTP (or simply disabled if web security is not enabled).</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Spring-io-Spring-Boot&quot;&gt;&lt;a href=&quot;#Spring-io-Spring-Boot&quot; class=&quot;headerlink&quot; title=&quot;Spring.io - Spring Boot&quot;&gt;&lt;/a&gt;Spring.io - Spring Boot&lt;/h2&gt;&lt;p&gt;I’m taking some important parts from it and summarizing them as my learning notes.&lt;/p&gt;
&lt;p&gt;Spring has been notorious for its heaviness when starting a project. Lots of people avoid this framework because of that. Many lighter dependency injection frameworks emerge as a replacement for Spring, &lt;a href=&quot;https://github.com/google/guice&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Google’s Guice&lt;/a&gt; being one of the most representative ones. &lt;/p&gt;
&lt;p&gt;I’ve talked to quite a few folks who touched Spring a few years back and then switched to some better solutions because of the pain of managing the sophisticated configs. They hadn’t heard of Spring Boot, and were surprised about this project when I introduced it to them.&lt;/p&gt;
&lt;p&gt;Here, let’s discuss some of the very basic yet fundamental aspects of Spring Boot.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;The following content is from the official guidance of &lt;a href=&quot;http://docs.spring.io/spring-boot/docs/current-SNAPSHOT/reference/htmlsingle/#getting-started&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Spring Boot&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;Introducing-Spring-Boot&quot;&gt;&lt;a href=&quot;#Introducing-Spring-Boot&quot; class=&quot;headerlink&quot; title=&quot;Introducing Spring Boot&quot;&gt;&lt;/a&gt;Introducing Spring Boot&lt;/h2&gt;&lt;p&gt;Spring Boot makes it easy to create stand-alone, production-grade Spring based Applications that you can &lt;code&gt;just run&lt;/code&gt;. We take an opinionated view of the Spring platform and third-party libraries so you can get started with minimum fuss. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Most Spring Boot applications need very little Spring configuration.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You can use Spring Boot to create Java applications that can be started using &lt;code&gt;java -jar&lt;/code&gt; or more traditional war deployments. We also provide a command line tool that runs &lt;code&gt;spring scripts&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Some featured characteristics are:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Provide a radically faster and widely accessible getting started experience for all Spring development.&lt;ul&gt;
&lt;li&gt;Create stand-alone Spring applications&lt;/li&gt;
&lt;li&gt;Embed Tomcat, Jetty or Undertow directly (no need to deploy WAR files)&lt;/li&gt;
&lt;li&gt;Provide opinionated &lt;code&gt;starter&lt;/code&gt; POMs (Project Object Model) to simplify your Maven configuration&lt;/li&gt;
&lt;li&gt;Automatically configure Spring whenever possible&lt;/li&gt;
&lt;li&gt;Provide production-ready features such as metrics, health checks and externalized configuration&lt;/li&gt;
&lt;li&gt;Absolutely &lt;code&gt;no code generation&lt;/code&gt; and &lt;code&gt;no requirement for XML configuration&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Spring Boot offers four main features that will change the way you develop Spring applications:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Spring Boot starters&lt;/code&gt; — Spring Boot starters aggregate common groupings of dependencies into single dependencies that can be added to a project’s Maven or Gradle build.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Autoconfiguration&lt;/code&gt; — Spring Boot’s autoconfiguration feature leverages Spring 4’s support for conditional configuration to make reasonable guesses about the beans your application needs and automatically configure them.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Command-line interface (CLI)&lt;/code&gt; — Spring Boot’s CLI takes advantage of the Groovy programming language along with autoconfiguration to further simplify Spring application development.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Actuator&lt;/code&gt; — The Spring Boot Actuator adds certain management features to a Spring Boot application.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="spring" scheme="https://bowenli86.github.io/categories/spring/"/>
    
      <category term="spring boot" scheme="https://bowenli86.github.io/categories/spring/spring-boot/"/>
    
    
      <category term="spring boot" scheme="https://bowenli86.github.io/tags/spring-boot/"/>
    
  </entry>
  
  <entry>
    <title>Hexo - How to backup configs and blogs of Github Pages in a Github repository</title>
    <link href="https://bowenli86.github.io/2016/09/29/hexo/Hexo-How-to-backup-configs-and-blogs-of-Github-Pages-in-a-Github-repository/"/>
    <id>https://bowenli86.github.io/2016/09/29/hexo/Hexo-How-to-backup-configs-and-blogs-of-Github-Pages-in-a-Github-repository/</id>
    <published>2016-09-30T00:50:02.000Z</published>
    <updated>2018-04-10T05:08:19.859Z</updated>
    
    <content type="html"><![CDATA[<p>Github Pages is great - it allows you to set up static website on it and use it as a blog.</p><p>I’ve been using <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> as the blogging platform and static website generator, and <a href="http://theme-next.iissnan.com/" target="_blank" rel="noopener">NexT</a> as its theme. One thing I really need when using Hexo is to back up all my blogs which are in markdown format, as well as Hexo configuration file, a bunch website configuration files, and the theme configuration file. The reason being that 1) self-managed blog generator is very easy to be messed up, and 2) I need to re-setup everything when switching computers, both cases requiring reconfigure Hexo and restore all contents and configurations.</p><p>So wouldn’t it be nice that we can use your <strong><username>.github.io</username></strong> repository for Github Pages, and another repository, say <strong>Hexo</strong>, for all your blogs and configurations?</p><p>It’s a bit tricky, and I’m gonna show you how to do it.</p><h3 id="Backgrounds-What-files-should-be-backed-up"><a href="#Backgrounds-What-files-should-be-backed-up" class="headerlink" title="Backgrounds - What files should be backed up"></a>Backgrounds - What files should be backed up</h3><p>Let’s say Hexo lives in a dir called <strong>Hexo</strong>, and your theme is NexT, the file layout looks like this:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">- Hexo</span><br><span class="line">    - _config.yml (Hexo config file)</span><br><span class="line">    - node_modules (Node.js code of Hexo and all plugins)</span><br><span class="line">    - public (generated static website data)</span><br><span class="line">    - scaffolds (template files)</span><br><span class="line">    - source</span><br><span class="line">        - _posts (all your blogs)</span><br><span class="line">        - (other top-level sites you want to add, like **about**, **categories**, and **tags**)</span><br><span class="line">    - themes (Hexo themes)</span><br><span class="line">        - landscape (Hexo default theme)</span><br><span class="line">        - next (NexT theme)</span><br><span class="line">            - _config.yml (NexT&apos;s config file)</span><br></pre></td></tr></table></figure><h3 id="0-Make-sure-you-have-installed-hexo"><a href="#0-Make-sure-you-have-installed-hexo" class="headerlink" title="0. Make sure you have installed hexo"></a>0. Make sure you have installed hexo</h3><p>Check <a href="https://phoenixjiangnan.github.io/2016/01/23/hexo/Hexo-How-to-install-hexo-on-Mac-with-github-pages/" target="_blank" rel="noopener">this post</a></p><h3 id="1-Init-Hexo-in-a-temp-folder"><a href="#1-Init-Hexo-in-a-temp-folder" class="headerlink" title="1. Init Hexo in a temp folder"></a>1. Init Hexo in a temp folder</h3><p>Let’s init Hexo in a temp folder called <strong>hexocopy</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir hexocopy</span><br><span class="line">cd hexocopy</span><br><span class="line">hexo init</span><br></pre></td></tr></table></figure><h3 id="2-Init-your-official-Hexo-repository"><a href="#2-Init-your-official-Hexo-repository" class="headerlink" title="2. Init your official Hexo repository"></a>2. Init your official Hexo repository</h3><ul><li><p>Create a repository called <strong>Hexo</strong>, or clone your already existing ‘Hexo’ repository to your local machine with <strong>git clone</strong></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/bowenli86/Hexo</span><br></pre></td></tr></table></figure></li></ul><h3 id="3-Copy-files"><a href="#3-Copy-files" class="headerlink" title="3. Copy files"></a>3. Copy files</h3><p>Copy <strong>_config.yml</strong>, <strong>node_modules</strong>, and <strong>themes</strong> folders from <strong>hexocopy</strong> to <strong>Hexo</strong>.</p><p>The reason for not initing Hexo itself from dir <strong>Hexo</strong> is that, command <strong>init Hexo</strong> will clone bits from a git repository and it will overwrite <strong>.git</strong> of the <strong>Hexo</strong> repository.</p><p>Notes:</p><ul><li>for <strong>_config.yml</strong> files, carefully merge the old one with the new one since configs may be incompatible over time. Also <strong>make sure their versions are correct</strong>!! Otherwise, Hexo won’t be able to process template and generate sites.</li><li>several files should not be overriden, like <code>scaffolds/post.md</code>, etc</li></ul><h3 id="4-Continue"><a href="#4-Continue" class="headerlink" title="4. Continue"></a>4. Continue</h3><p>Then, refer to <a href="https://bowenli86.github.io/2016/01/23/hexo/Hexo-How-to-install-hexo-on-Mac-with-github-pages/">this post</a> to continue setting up plugins, themes, and configs.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Github Pages is great - it allows you to set up static website on it and use it as a blog.&lt;/p&gt;
&lt;p&gt;I’ve been using &lt;a href=&quot;https://hexo.i
      
    
    </summary>
    
      <category term="hexo" scheme="https://bowenli86.github.io/categories/hexo/"/>
    
    
      <category term="hexo backup" scheme="https://bowenli86.github.io/tags/hexo-backup/"/>
    
      <category term="hexo github integration" scheme="https://bowenli86.github.io/tags/hexo-github-integration/"/>
    
  </entry>
  
</feed>
